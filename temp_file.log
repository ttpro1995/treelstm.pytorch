2017-05-06 22:27:15,733 : INFO : LOG_FILE
2017-05-06 22:27:15,733 : INFO : _________________________________start___________________________________
2017-05-06 22:27:15,740 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-06 22:27:15,863 : INFO : ==> SST vocabulary size : 21705
2017-05-06 22:27:22,095 : INFO : _param count_
2017-05-06 22:27:22,095 : INFO : torch.Size([168, 300])
2017-05-06 22:27:22,096 : INFO : torch.Size([168])
2017-05-06 22:27:22,096 : INFO : torch.Size([168, 168])
2017-05-06 22:27:22,096 : INFO : torch.Size([168])
2017-05-06 22:27:22,096 : INFO : torch.Size([168, 300])
2017-05-06 22:27:22,096 : INFO : torch.Size([168])
2017-05-06 22:27:22,096 : INFO : torch.Size([168, 168])
2017-05-06 22:27:22,097 : INFO : torch.Size([168])
2017-05-06 22:27:22,097 : INFO : torch.Size([168, 300])
2017-05-06 22:27:22,097 : INFO : torch.Size([168])
2017-05-06 22:27:22,097 : INFO : torch.Size([168, 168])
2017-05-06 22:27:22,097 : INFO : torch.Size([168])
2017-05-06 22:27:22,097 : INFO : torch.Size([168, 300])
2017-05-06 22:27:22,098 : INFO : torch.Size([168])
2017-05-06 22:27:22,098 : INFO : torch.Size([168, 168])
2017-05-06 22:27:22,098 : INFO : torch.Size([168])
2017-05-06 22:27:22,098 : INFO : torch.Size([3, 168])
2017-05-06 22:27:22,098 : INFO : torch.Size([3])
2017-05-06 22:27:22,098 : INFO : sum
2017-05-06 22:27:22,098 : INFO : 316347
2017-05-06 22:27:22,098 : INFO : ____________
2017-05-06 22:27:50,894 : INFO : LOG_FILE
2017-05-06 22:27:50,894 : INFO : _________________________________start___________________________________
2017-05-06 22:27:50,901 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-06 22:27:51,023 : INFO : ==> SST vocabulary size : 21705
2017-05-06 22:27:57,176 : INFO : _param count_
2017-05-06 22:27:57,176 : INFO : torch.Size([168, 300])
2017-05-06 22:27:57,176 : INFO : torch.Size([168])
2017-05-06 22:27:57,176 : INFO : torch.Size([168, 168])
2017-05-06 22:27:57,177 : INFO : torch.Size([168])
2017-05-06 22:27:57,177 : INFO : torch.Size([168, 300])
2017-05-06 22:27:57,177 : INFO : torch.Size([168])
2017-05-06 22:27:57,177 : INFO : torch.Size([168, 168])
2017-05-06 22:27:57,177 : INFO : torch.Size([168])
2017-05-06 22:27:57,177 : INFO : torch.Size([168, 300])
2017-05-06 22:27:57,177 : INFO : torch.Size([168])
2017-05-06 22:27:57,177 : INFO : torch.Size([168, 168])
2017-05-06 22:27:57,178 : INFO : torch.Size([168])
2017-05-06 22:27:57,178 : INFO : torch.Size([168, 300])
2017-05-06 22:27:57,178 : INFO : torch.Size([168])
2017-05-06 22:27:57,178 : INFO : torch.Size([168, 168])
2017-05-06 22:27:57,178 : INFO : torch.Size([168])
2017-05-06 22:27:57,178 : INFO : torch.Size([3, 168])
2017-05-06 22:27:57,178 : INFO : torch.Size([3])
2017-05-06 22:27:57,179 : INFO : sum
2017-05-06 22:27:57,179 : INFO : 316347
2017-05-06 22:27:57,179 : INFO : ____________
2017-05-06 22:29:20,634 : INFO : LOG_FILE
2017-05-06 22:29:20,635 : INFO : _________________________________start___________________________________
2017-05-06 22:29:20,645 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-06 22:29:20,822 : INFO : ==> SST vocabulary size : 21705
2017-05-06 22:29:27,479 : INFO : _param count_
2017-05-06 22:29:27,479 : INFO : torch.Size([168, 300])
2017-05-06 22:29:27,479 : INFO : torch.Size([168])
2017-05-06 22:29:27,479 : INFO : torch.Size([168, 168])
2017-05-06 22:29:27,480 : INFO : torch.Size([168])
2017-05-06 22:29:27,480 : INFO : torch.Size([168, 300])
2017-05-06 22:29:27,480 : INFO : torch.Size([168])
2017-05-06 22:29:27,480 : INFO : torch.Size([168, 168])
2017-05-06 22:29:27,480 : INFO : torch.Size([168])
2017-05-06 22:29:27,480 : INFO : torch.Size([168, 300])
2017-05-06 22:29:27,481 : INFO : torch.Size([168])
2017-05-06 22:29:27,481 : INFO : torch.Size([168, 168])
2017-05-06 22:29:27,481 : INFO : torch.Size([168])
2017-05-06 22:29:27,481 : INFO : torch.Size([168, 300])
2017-05-06 22:29:27,481 : INFO : torch.Size([168])
2017-05-06 22:29:27,481 : INFO : torch.Size([168, 168])
2017-05-06 22:29:27,481 : INFO : torch.Size([168])
2017-05-06 22:29:27,482 : INFO : torch.Size([3, 168])
2017-05-06 22:29:27,482 : INFO : torch.Size([3])
2017-05-06 22:29:27,482 : INFO : sum
2017-05-06 22:29:27,482 : INFO : 316347
2017-05-06 22:29:27,482 : INFO : ____________
2017-05-06 22:32:10,111 : INFO : LOG_FILE
2017-05-06 22:32:10,111 : INFO : _________________________________start___________________________________
2017-05-06 22:32:10,117 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-06 22:32:10,227 : INFO : ==> SST vocabulary size : 21705
2017-05-06 22:32:16,770 : INFO : _param count_
2017-05-06 22:32:16,770 : INFO : torch.Size([168, 300])
2017-05-06 22:32:16,770 : INFO : torch.Size([168])
2017-05-06 22:32:16,770 : INFO : torch.Size([168, 168])
2017-05-06 22:32:16,770 : INFO : torch.Size([168])
2017-05-06 22:32:16,771 : INFO : torch.Size([168, 300])
2017-05-06 22:32:16,771 : INFO : torch.Size([168])
2017-05-06 22:32:16,771 : INFO : torch.Size([168, 168])
2017-05-06 22:32:16,771 : INFO : torch.Size([168])
2017-05-06 22:32:16,771 : INFO : torch.Size([168, 300])
2017-05-06 22:32:16,771 : INFO : torch.Size([168])
2017-05-06 22:32:16,771 : INFO : torch.Size([168, 168])
2017-05-06 22:32:16,772 : INFO : torch.Size([168])
2017-05-06 22:32:16,772 : INFO : torch.Size([168, 300])
2017-05-06 22:32:16,772 : INFO : torch.Size([168])
2017-05-06 22:32:16,772 : INFO : torch.Size([168, 168])
2017-05-06 22:32:16,772 : INFO : torch.Size([168])
2017-05-06 22:32:16,772 : INFO : torch.Size([3, 168])
2017-05-06 22:32:16,772 : INFO : torch.Size([3])
2017-05-06 22:32:16,773 : INFO : sum
2017-05-06 22:32:16,773 : INFO : 316347
2017-05-06 22:32:16,773 : INFO : ____________
2017-05-06 22:32:25,268 : INFO : LOG_FILE
2017-05-06 22:32:25,268 : INFO : _________________________________start___________________________________
2017-05-06 22:32:25,275 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-06 22:32:25,388 : INFO : ==> SST vocabulary size : 21705
2017-05-06 22:32:31,803 : INFO : _param count_
2017-05-06 22:32:31,804 : INFO : torch.Size([168, 300])
2017-05-06 22:32:31,804 : INFO : torch.Size([168])
2017-05-06 22:32:31,804 : INFO : torch.Size([168, 168])
2017-05-06 22:32:31,804 : INFO : torch.Size([168])
2017-05-06 22:32:31,805 : INFO : torch.Size([168, 300])
2017-05-06 22:32:31,805 : INFO : torch.Size([168])
2017-05-06 22:32:31,805 : INFO : torch.Size([168, 168])
2017-05-06 22:32:31,805 : INFO : torch.Size([168])
2017-05-06 22:32:31,805 : INFO : torch.Size([168, 300])
2017-05-06 22:32:31,805 : INFO : torch.Size([168])
2017-05-06 22:32:31,806 : INFO : torch.Size([168, 168])
2017-05-06 22:32:31,806 : INFO : torch.Size([168])
2017-05-06 22:32:31,806 : INFO : torch.Size([168, 300])
2017-05-06 22:32:31,806 : INFO : torch.Size([168])
2017-05-06 22:32:31,806 : INFO : torch.Size([168, 168])
2017-05-06 22:32:31,807 : INFO : torch.Size([168])
2017-05-06 22:32:31,807 : INFO : torch.Size([3, 168])
2017-05-06 22:32:31,807 : INFO : torch.Size([3])
2017-05-06 22:32:31,807 : INFO : sum
2017-05-06 22:32:31,807 : INFO : 316347
2017-05-06 22:32:31,807 : INFO : ____________
2017-05-06 22:33:04,151 : INFO : LOG_FILE
2017-05-06 22:33:04,151 : INFO : _________________________________start___________________________________
2017-05-06 22:33:04,164 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-06 22:33:04,528 : INFO : ==> SST vocabulary size : 21705
2017-05-06 22:33:11,615 : INFO : _param count_
2017-05-06 22:33:11,618 : INFO : torch.Size([168, 300])
2017-05-06 22:33:11,618 : INFO : torch.Size([168])
2017-05-06 22:33:11,619 : INFO : torch.Size([168, 168])
2017-05-06 22:33:11,619 : INFO : torch.Size([168])
2017-05-06 22:33:11,620 : INFO : torch.Size([168, 300])
2017-05-06 22:33:11,621 : INFO : torch.Size([168])
2017-05-06 22:33:11,622 : INFO : torch.Size([168, 168])
2017-05-06 22:33:11,622 : INFO : torch.Size([168])
2017-05-06 22:33:11,623 : INFO : torch.Size([168, 300])
2017-05-06 22:33:11,623 : INFO : torch.Size([168])
2017-05-06 22:33:11,624 : INFO : torch.Size([168, 168])
2017-05-06 22:33:11,624 : INFO : torch.Size([168])
2017-05-06 22:33:11,625 : INFO : torch.Size([168, 300])
2017-05-06 22:33:11,626 : INFO : torch.Size([168])
2017-05-06 22:33:11,626 : INFO : torch.Size([168, 168])
2017-05-06 22:33:11,627 : INFO : torch.Size([168])
2017-05-06 22:33:11,628 : INFO : torch.Size([3, 168])
2017-05-06 22:33:11,628 : INFO : torch.Size([3])
2017-05-06 22:33:11,629 : INFO : sum
2017-05-06 22:33:11,629 : INFO : 316347
2017-05-06 22:33:11,630 : INFO : ____________
2017-05-06 22:33:16,867 : INFO : LOG_FILE
2017-05-06 22:33:16,868 : INFO : _________________________________start___________________________________
2017-05-06 22:33:16,874 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-06 22:33:16,999 : INFO : ==> SST vocabulary size : 21705
2017-05-06 22:33:23,246 : INFO : _param count_
2017-05-06 22:33:23,246 : INFO : torch.Size([168, 300])
2017-05-06 22:33:23,246 : INFO : torch.Size([168])
2017-05-06 22:33:23,246 : INFO : torch.Size([168, 168])
2017-05-06 22:33:23,246 : INFO : torch.Size([168])
2017-05-06 22:33:23,247 : INFO : torch.Size([168, 300])
2017-05-06 22:33:23,247 : INFO : torch.Size([168])
2017-05-06 22:33:23,247 : INFO : torch.Size([168, 168])
2017-05-06 22:33:23,247 : INFO : torch.Size([168])
2017-05-06 22:33:23,247 : INFO : torch.Size([168, 300])
2017-05-06 22:33:23,247 : INFO : torch.Size([168])
2017-05-06 22:33:23,247 : INFO : torch.Size([168, 168])
2017-05-06 22:33:23,248 : INFO : torch.Size([168])
2017-05-06 22:33:23,248 : INFO : torch.Size([168, 300])
2017-05-06 22:33:23,248 : INFO : torch.Size([168])
2017-05-06 22:33:23,248 : INFO : torch.Size([168, 168])
2017-05-06 22:33:23,248 : INFO : torch.Size([168])
2017-05-06 22:33:23,249 : INFO : torch.Size([3, 168])
2017-05-06 22:33:23,249 : INFO : torch.Size([3])
2017-05-06 22:33:23,249 : INFO : sum
2017-05-06 22:33:23,249 : INFO : 316347
2017-05-06 22:33:23,249 : INFO : ____________
2017-05-06 22:34:48,683 : INFO : LOG_FILE
2017-05-06 22:34:48,683 : INFO : _________________________________start___________________________________
2017-05-06 22:34:48,692 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-06 22:34:48,813 : INFO : ==> SST vocabulary size : 21705
2017-05-06 22:34:55,184 : INFO : _param count_
2017-05-06 22:34:55,185 : INFO : torch.Size([168, 300])
2017-05-06 22:34:55,185 : INFO : torch.Size([168])
2017-05-06 22:34:55,185 : INFO : torch.Size([168, 168])
2017-05-06 22:34:55,185 : INFO : torch.Size([168])
2017-05-06 22:34:55,185 : INFO : torch.Size([168, 300])
2017-05-06 22:34:55,185 : INFO : torch.Size([168])
2017-05-06 22:34:55,185 : INFO : torch.Size([168, 168])
2017-05-06 22:34:55,186 : INFO : torch.Size([168])
2017-05-06 22:34:55,186 : INFO : torch.Size([168, 300])
2017-05-06 22:34:55,186 : INFO : torch.Size([168])
2017-05-06 22:34:55,186 : INFO : torch.Size([168, 168])
2017-05-06 22:34:55,186 : INFO : torch.Size([168])
2017-05-06 22:34:55,186 : INFO : torch.Size([168, 300])
2017-05-06 22:34:55,186 : INFO : torch.Size([168])
2017-05-06 22:34:55,187 : INFO : torch.Size([168, 168])
2017-05-06 22:34:55,187 : INFO : torch.Size([168])
2017-05-06 22:34:55,187 : INFO : torch.Size([3, 168])
2017-05-06 22:34:55,187 : INFO : torch.Size([3])
2017-05-06 22:34:55,187 : INFO : sum
2017-05-06 22:34:55,187 : INFO : 316347
2017-05-06 22:34:55,187 : INFO : ____________
2017-05-06 22:54:47,887 : INFO : ==> Train loss   : 0.524754
2017-05-06 22:54:47,887 : INFO : Epoch
2017-05-06 22:54:47,888 : INFO : 0
2017-05-06 22:54:47,888 : INFO : train percentage
2017-05-06 22:54:47,888 : INFO : 0.846098265896
2017-05-06 22:54:47,888 : INFO : Epoch
2017-05-06 22:54:47,888 : INFO : 0
2017-05-06 22:54:47,888 : INFO : dev percentage
2017-05-06 22:54:47,888 : INFO : 0.755733944954
2017-05-06 22:54:47,888 : INFO : Epoch
2017-05-06 22:54:47,888 : INFO : 0
2017-05-06 22:54:47,889 : INFO : test percentage
2017-05-06 22:54:47,889 : INFO : 0.727073036793
2017-05-06 23:14:41,053 : INFO : ==> Train loss   : 0.259927
2017-05-06 23:14:41,053 : INFO : Epoch
2017-05-06 23:14:41,053 : INFO : 1
2017-05-06 23:14:41,053 : INFO : train percentage
2017-05-06 23:14:41,053 : INFO : 0.929046242775
2017-05-06 23:14:41,053 : INFO : Epoch
2017-05-06 23:14:41,053 : INFO : 1
2017-05-06 23:14:41,054 : INFO : dev percentage
2017-05-06 23:14:41,054 : INFO : 0.784403669725
2017-05-06 23:14:41,054 : INFO : Epoch
2017-05-06 23:14:41,054 : INFO : 1
2017-05-06 23:14:41,054 : INFO : test percentage
2017-05-06 23:14:41,054 : INFO : 0.792970895113
2017-05-06 23:34:22,989 : INFO : ==> Train loss   : 0.218820
2017-05-06 23:34:22,989 : INFO : Epoch
2017-05-06 23:34:22,989 : INFO : 2
2017-05-06 23:34:22,989 : INFO : train percentage
2017-05-06 23:34:22,990 : INFO : 0.948843930636
2017-05-06 23:34:22,990 : INFO : Epoch
2017-05-06 23:34:22,990 : INFO : 2
2017-05-06 23:34:22,990 : INFO : dev percentage
2017-05-06 23:34:22,990 : INFO : 0.776376146789
2017-05-06 23:34:22,990 : INFO : Epoch
2017-05-06 23:34:22,990 : INFO : 2
2017-05-06 23:34:22,990 : INFO : test percentage
2017-05-06 23:34:22,990 : INFO : 0.79516749039
2017-05-06 23:53:50,093 : INFO : ==> Train loss   : 0.179927
2017-05-06 23:53:50,094 : INFO : Epoch
2017-05-06 23:53:50,094 : INFO : 3
2017-05-06 23:53:50,094 : INFO : train percentage
2017-05-06 23:53:50,094 : INFO : 0.957225433526
2017-05-06 23:53:50,094 : INFO : Epoch
2017-05-06 23:53:50,094 : INFO : 3
2017-05-06 23:53:50,094 : INFO : dev percentage
2017-05-06 23:53:50,095 : INFO : 0.793577981651
2017-05-06 23:53:50,095 : INFO : Epoch
2017-05-06 23:53:50,095 : INFO : 3
2017-05-06 23:53:50,095 : INFO : test percentage
2017-05-06 23:53:50,095 : INFO : 0.79516749039
2017-05-07 00:13:08,695 : INFO : ==> Train loss   : 0.145396
2017-05-07 00:13:08,695 : INFO : Epoch
2017-05-07 00:13:08,695 : INFO : 4
2017-05-07 00:13:08,695 : INFO : train percentage
2017-05-07 00:13:08,696 : INFO : 0.962572254335
2017-05-07 00:13:08,696 : INFO : Epoch
2017-05-07 00:13:08,696 : INFO : 4
2017-05-07 00:13:08,696 : INFO : dev percentage
2017-05-07 00:13:08,696 : INFO : 0.771788990826
2017-05-07 00:13:08,696 : INFO : Epoch
2017-05-07 00:13:08,696 : INFO : 4
2017-05-07 00:13:08,696 : INFO : test percentage
2017-05-07 00:13:08,697 : INFO : 0.788577704558
2017-05-07 00:33:00,981 : INFO : ==> Train loss   : 0.125242
2017-05-07 00:33:00,981 : INFO : Epoch
2017-05-07 00:33:00,981 : INFO : 5
2017-05-07 00:33:00,981 : INFO : train percentage
2017-05-07 00:33:00,982 : INFO : 0.971098265896
2017-05-07 00:33:00,982 : INFO : Epoch
2017-05-07 00:33:00,982 : INFO : 5
2017-05-07 00:33:00,982 : INFO : dev percentage
2017-05-07 00:33:00,982 : INFO : 0.770642201835
2017-05-07 00:33:00,982 : INFO : Epoch
2017-05-07 00:33:00,982 : INFO : 5
2017-05-07 00:33:00,982 : INFO : test percentage
2017-05-07 00:33:00,983 : INFO : 0.797364085667
2017-05-07 00:52:34,921 : INFO : ==> Train loss   : 0.120709
2017-05-07 00:52:34,921 : INFO : Epoch
2017-05-07 00:52:34,923 : INFO : 6
2017-05-07 00:52:34,923 : INFO : train percentage
2017-05-07 00:52:34,923 : INFO : 0.973554913295
2017-05-07 00:52:34,923 : INFO : Epoch
2017-05-07 00:52:34,924 : INFO : 6
2017-05-07 00:52:34,924 : INFO : dev percentage
2017-05-07 00:52:34,924 : INFO : 0.767201834862
2017-05-07 00:52:34,924 : INFO : Epoch
2017-05-07 00:52:34,924 : INFO : 6
2017-05-07 00:52:34,924 : INFO : test percentage
2017-05-07 00:52:34,924 : INFO : 0.794618341571
2017-05-07 01:11:45,253 : INFO : ==> Train loss   : 0.104618
2017-05-07 01:11:45,253 : INFO : Epoch
2017-05-07 01:11:45,254 : INFO : 7
2017-05-07 01:11:45,254 : INFO : train percentage
2017-05-07 01:11:45,254 : INFO : 0.979046242775
2017-05-07 01:11:45,254 : INFO : Epoch
2017-05-07 01:11:45,254 : INFO : 7
2017-05-07 01:11:45,255 : INFO : dev percentage
2017-05-07 01:11:45,255 : INFO : 0.775229357798
2017-05-07 01:11:45,255 : INFO : Epoch
2017-05-07 01:11:45,255 : INFO : 7
2017-05-07 01:11:45,255 : INFO : test percentage
2017-05-07 01:11:45,255 : INFO : 0.799560680945
2017-05-07 01:29:50,786 : INFO : ==> Train loss   : 0.093125
2017-05-07 01:29:50,786 : INFO : Epoch
2017-05-07 01:29:50,786 : INFO : 8
2017-05-07 01:29:50,786 : INFO : train percentage
2017-05-07 01:29:50,786 : INFO : 0.978179190751
2017-05-07 01:29:50,786 : INFO : Epoch
2017-05-07 01:29:50,787 : INFO : 8
2017-05-07 01:29:50,787 : INFO : dev percentage
2017-05-07 01:29:50,787 : INFO : 0.764908256881
2017-05-07 01:29:50,787 : INFO : Epoch
2017-05-07 01:29:50,787 : INFO : 8
2017-05-07 01:29:50,787 : INFO : test percentage
2017-05-07 01:29:50,787 : INFO : 0.786381109281
2017-05-07 01:48:04,906 : INFO : ==> Train loss   : 0.090114
2017-05-07 01:48:04,906 : INFO : Epoch
2017-05-07 01:48:04,906 : INFO : 9
2017-05-07 01:48:04,907 : INFO : train percentage
2017-05-07 01:48:04,907 : INFO : 0.980346820809
2017-05-07 01:48:04,907 : INFO : Epoch
2017-05-07 01:48:04,907 : INFO : 9
2017-05-07 01:48:04,907 : INFO : dev percentage
2017-05-07 01:48:04,907 : INFO : 0.771788990826
2017-05-07 01:48:04,907 : INFO : Epoch
2017-05-07 01:48:04,907 : INFO : 9
2017-05-07 01:48:04,907 : INFO : test percentage
2017-05-07 01:48:04,908 : INFO : 0.793520043932
2017-05-07 02:06:18,536 : INFO : ==> Train loss   : 0.082572
2017-05-07 02:06:18,536 : INFO : Epoch
2017-05-07 02:06:18,536 : INFO : 10
2017-05-07 02:06:18,536 : INFO : train percentage
2017-05-07 02:06:18,537 : INFO : 0.982369942197
2017-05-07 02:06:18,537 : INFO : Epoch
2017-05-07 02:06:18,537 : INFO : 10
2017-05-07 02:06:18,537 : INFO : dev percentage
2017-05-07 02:06:18,537 : INFO : 0.766055045872
2017-05-07 02:06:18,537 : INFO : Epoch
2017-05-07 02:06:18,537 : INFO : 10
2017-05-07 02:06:18,538 : INFO : test percentage
2017-05-07 02:06:18,538 : INFO : 0.796265788029
2017-05-07 02:24:41,285 : INFO : ==> Train loss   : 0.073851
2017-05-07 02:24:41,285 : INFO : Epoch
2017-05-07 02:24:41,286 : INFO : 11
2017-05-07 02:24:41,286 : INFO : train percentage
2017-05-07 02:24:41,286 : INFO : 0.983670520231
2017-05-07 02:24:41,286 : INFO : Epoch
2017-05-07 02:24:41,286 : INFO : 11
2017-05-07 02:24:41,286 : INFO : dev percentage
2017-05-07 02:24:41,286 : INFO : 0.768348623853
2017-05-07 02:24:41,287 : INFO : Epoch
2017-05-07 02:24:41,287 : INFO : 11
2017-05-07 02:24:41,287 : INFO : test percentage
2017-05-07 02:24:41,287 : INFO : 0.797364085667
2017-05-07 02:43:02,764 : INFO : ==> Train loss   : 0.070180
2017-05-07 02:43:02,764 : INFO : Epoch
2017-05-07 02:43:02,764 : INFO : 12
2017-05-07 02:43:02,764 : INFO : train percentage
2017-05-07 02:43:02,765 : INFO : 0.984393063584
2017-05-07 02:43:02,765 : INFO : Epoch
2017-05-07 02:43:02,765 : INFO : 12
2017-05-07 02:43:02,765 : INFO : dev percentage
2017-05-07 02:43:02,765 : INFO : 0.766055045872
2017-05-07 02:43:02,765 : INFO : Epoch
2017-05-07 02:43:02,765 : INFO : 12
2017-05-07 02:43:02,765 : INFO : test percentage
2017-05-07 02:43:02,766 : INFO : 0.789126853377
2017-05-07 03:01:29,421 : INFO : ==> Train loss   : 0.074552
2017-05-07 03:01:29,421 : INFO : Epoch
2017-05-07 03:01:29,421 : INFO : 13
2017-05-07 03:01:29,421 : INFO : train percentage
2017-05-07 03:01:29,421 : INFO : 0.985549132948
2017-05-07 03:01:29,421 : INFO : Epoch
2017-05-07 03:01:29,421 : INFO : 13
2017-05-07 03:01:29,422 : INFO : dev percentage
2017-05-07 03:01:29,422 : INFO : 0.767201834862
2017-05-07 03:01:29,422 : INFO : Epoch
2017-05-07 03:01:29,422 : INFO : 13
2017-05-07 03:01:29,422 : INFO : test percentage
2017-05-07 03:01:29,422 : INFO : 0.790225151016
2017-05-07 03:19:51,525 : INFO : ==> Train loss   : 0.061458
2017-05-07 03:19:51,526 : INFO : Epoch
2017-05-07 03:19:51,526 : INFO : 14
2017-05-07 03:19:51,526 : INFO : train percentage
2017-05-07 03:19:51,526 : INFO : 0.986271676301
2017-05-07 03:19:51,526 : INFO : Epoch
2017-05-07 03:19:51,526 : INFO : 14
2017-05-07 03:19:51,526 : INFO : dev percentage
2017-05-07 03:19:51,527 : INFO : 0.76376146789
2017-05-07 03:19:51,527 : INFO : Epoch
2017-05-07 03:19:51,527 : INFO : 14
2017-05-07 03:19:51,527 : INFO : test percentage
2017-05-07 03:19:51,527 : INFO : 0.779791323449
2017-05-07 03:38:11,161 : INFO : ==> Train loss   : 0.058806
2017-05-07 03:38:11,161 : INFO : Epoch
2017-05-07 03:38:11,161 : INFO : 15
2017-05-07 03:38:11,161 : INFO : train percentage
2017-05-07 03:38:11,161 : INFO : 0.988728323699
2017-05-07 03:38:11,161 : INFO : Epoch
2017-05-07 03:38:11,161 : INFO : 15
2017-05-07 03:38:11,162 : INFO : dev percentage
2017-05-07 03:38:11,162 : INFO : 0.754587155963
2017-05-07 03:38:11,162 : INFO : Epoch
2017-05-07 03:38:11,162 : INFO : 15
2017-05-07 03:38:11,162 : INFO : test percentage
2017-05-07 03:38:11,162 : INFO : 0.789126853377
2017-05-07 03:56:29,817 : INFO : ==> Train loss   : 0.057890
2017-05-07 03:56:29,817 : INFO : Epoch
2017-05-07 03:56:29,818 : INFO : 16
2017-05-07 03:56:29,818 : INFO : train percentage
2017-05-07 03:56:29,818 : INFO : 0.988150289017
2017-05-07 03:56:29,818 : INFO : Epoch
2017-05-07 03:56:29,818 : INFO : 16
2017-05-07 03:56:29,818 : INFO : dev percentage
2017-05-07 03:56:29,818 : INFO : 0.755733944954
2017-05-07 03:56:29,818 : INFO : Epoch
2017-05-07 03:56:29,819 : INFO : 16
2017-05-07 03:56:29,819 : INFO : test percentage
2017-05-07 03:56:29,819 : INFO : 0.783635365184
2017-05-07 04:14:57,881 : INFO : ==> Train loss   : 0.053610
2017-05-07 04:14:57,882 : INFO : Epoch
2017-05-07 04:14:57,882 : INFO : 17
2017-05-07 04:14:57,882 : INFO : train percentage
2017-05-07 04:14:57,882 : INFO : 0.990173410405
2017-05-07 04:14:57,882 : INFO : Epoch
2017-05-07 04:14:57,882 : INFO : 17
2017-05-07 04:14:57,882 : INFO : dev percentage
2017-05-07 04:14:57,883 : INFO : 0.762614678899
2017-05-07 04:14:57,883 : INFO : Epoch
2017-05-07 04:14:57,883 : INFO : 17
2017-05-07 04:14:57,883 : INFO : test percentage
2017-05-07 04:14:57,883 : INFO : 0.780889621087
2017-05-07 04:33:29,302 : INFO : ==> Train loss   : 0.057240
2017-05-07 04:33:29,303 : INFO : Epoch
2017-05-07 04:33:29,303 : INFO : 18
2017-05-07 04:33:29,303 : INFO : train percentage
2017-05-07 04:33:29,303 : INFO : 0.989739884393
2017-05-07 04:33:29,303 : INFO : Epoch
2017-05-07 04:33:29,303 : INFO : 18
2017-05-07 04:33:29,303 : INFO : dev percentage
2017-05-07 04:33:29,303 : INFO : 0.754587155963
2017-05-07 04:33:29,303 : INFO : Epoch
2017-05-07 04:33:29,304 : INFO : 18
2017-05-07 04:33:29,304 : INFO : test percentage
2017-05-07 04:33:29,304 : INFO : 0.779791323449
2017-05-07 04:52:03,476 : INFO : ==> Train loss   : 0.049628
2017-05-07 04:52:03,476 : INFO : Epoch
2017-05-07 04:52:03,476 : INFO : 19
2017-05-07 04:52:03,476 : INFO : train percentage
2017-05-07 04:52:03,476 : INFO : 0.99161849711
2017-05-07 04:52:03,476 : INFO : Epoch
2017-05-07 04:52:03,476 : INFO : 19
2017-05-07 04:52:03,477 : INFO : dev percentage
2017-05-07 04:52:03,477 : INFO : 0.754587155963
2017-05-07 04:52:03,477 : INFO : Epoch
2017-05-07 04:52:03,477 : INFO : 19
2017-05-07 04:52:03,477 : INFO : test percentage
2017-05-07 04:52:03,477 : INFO : 0.783086216365
2017-05-07 05:10:29,885 : INFO : ==> Train loss   : 0.046750
2017-05-07 05:10:29,885 : INFO : Epoch
2017-05-07 05:10:29,885 : INFO : 20
2017-05-07 05:10:29,885 : INFO : train percentage
2017-05-07 05:10:29,885 : INFO : 0.990751445087
2017-05-07 05:10:29,885 : INFO : Epoch
2017-05-07 05:10:29,886 : INFO : 20
2017-05-07 05:10:29,886 : INFO : dev percentage
2017-05-07 05:10:29,886 : INFO : 0.753440366972
2017-05-07 05:10:29,886 : INFO : Epoch
2017-05-07 05:10:29,886 : INFO : 20
2017-05-07 05:10:29,886 : INFO : test percentage
2017-05-07 05:10:29,886 : INFO : 0.788028555739
2017-05-07 05:28:47,199 : INFO : ==> Train loss   : 0.046476
2017-05-07 05:28:47,199 : INFO : Epoch
2017-05-07 05:28:47,199 : INFO : 21
2017-05-07 05:28:47,199 : INFO : train percentage
2017-05-07 05:28:47,199 : INFO : 0.99161849711
2017-05-07 05:28:47,200 : INFO : Epoch
2017-05-07 05:28:47,200 : INFO : 21
2017-05-07 05:28:47,200 : INFO : dev percentage
2017-05-07 05:28:47,200 : INFO : 0.752293577982
2017-05-07 05:28:47,200 : INFO : Epoch
2017-05-07 05:28:47,200 : INFO : 21
2017-05-07 05:28:47,200 : INFO : test percentage
2017-05-07 05:28:47,201 : INFO : 0.785282811642
2017-05-07 05:47:10,220 : INFO : ==> Train loss   : 0.044099
2017-05-07 05:47:10,220 : INFO : Epoch
2017-05-07 05:47:10,220 : INFO : 22
2017-05-07 05:47:10,220 : INFO : train percentage
2017-05-07 05:47:10,220 : INFO : 0.99161849711
2017-05-07 05:47:10,220 : INFO : Epoch
2017-05-07 05:47:10,220 : INFO : 22
2017-05-07 05:47:10,221 : INFO : dev percentage
2017-05-07 05:47:10,221 : INFO : 0.751146788991
2017-05-07 05:47:10,221 : INFO : Epoch
2017-05-07 05:47:10,221 : INFO : 22
2017-05-07 05:47:10,221 : INFO : test percentage
2017-05-07 05:47:10,221 : INFO : 0.784733662823
2017-05-07 06:05:37,492 : INFO : ==> Train loss   : 0.041457
2017-05-07 06:05:37,492 : INFO : Epoch
2017-05-07 06:05:37,492 : INFO : 23
2017-05-07 06:05:37,492 : INFO : train percentage
2017-05-07 06:05:37,492 : INFO : 0.992630057803
2017-05-07 06:05:37,492 : INFO : Epoch
2017-05-07 06:05:37,492 : INFO : 23
2017-05-07 06:05:37,493 : INFO : dev percentage
2017-05-07 06:05:37,493 : INFO : 0.754587155963
2017-05-07 06:05:37,493 : INFO : Epoch
2017-05-07 06:05:37,493 : INFO : 23
2017-05-07 06:05:37,493 : INFO : test percentage
2017-05-07 06:05:37,493 : INFO : 0.778143876991
2017-05-07 06:24:02,268 : INFO : ==> Train loss   : 0.042946
2017-05-07 06:24:02,268 : INFO : Epoch
2017-05-07 06:24:02,268 : INFO : 24
2017-05-07 06:24:02,268 : INFO : train percentage
2017-05-07 06:24:02,268 : INFO : 0.992630057803
2017-05-07 06:24:02,268 : INFO : Epoch
2017-05-07 06:24:02,269 : INFO : 24
2017-05-07 06:24:02,269 : INFO : dev percentage
2017-05-07 06:24:02,269 : INFO : 0.751146788991
2017-05-07 06:24:02,269 : INFO : Epoch
2017-05-07 06:24:02,269 : INFO : 24
2017-05-07 06:24:02,269 : INFO : test percentage
2017-05-07 06:24:02,269 : INFO : 0.77869302581
2017-05-07 06:42:15,869 : INFO : ==> Train loss   : 0.040278
2017-05-07 06:42:15,869 : INFO : Epoch
2017-05-07 06:42:15,870 : INFO : 25
2017-05-07 06:42:15,870 : INFO : train percentage
2017-05-07 06:42:15,870 : INFO : 0.993497109827
2017-05-07 06:42:15,870 : INFO : Epoch
2017-05-07 06:42:15,870 : INFO : 25
2017-05-07 06:42:15,870 : INFO : dev percentage
2017-05-07 06:42:15,870 : INFO : 0.746559633028
2017-05-07 06:42:15,870 : INFO : Epoch
2017-05-07 06:42:15,870 : INFO : 25
2017-05-07 06:42:15,871 : INFO : test percentage
2017-05-07 06:42:15,871 : INFO : 0.77869302581
2017-05-07 07:00:26,107 : INFO : ==> Train loss   : 0.038044
2017-05-07 07:00:26,107 : INFO : Epoch
2017-05-07 07:00:26,108 : INFO : 26
2017-05-07 07:00:26,108 : INFO : train percentage
2017-05-07 07:00:26,108 : INFO : 0.992630057803
2017-05-07 07:00:26,108 : INFO : Epoch
2017-05-07 07:00:26,108 : INFO : 26
2017-05-07 07:00:26,108 : INFO : dev percentage
2017-05-07 07:00:26,108 : INFO : 0.753440366972
2017-05-07 07:00:26,108 : INFO : Epoch
2017-05-07 07:00:26,108 : INFO : 26
2017-05-07 07:00:26,109 : INFO : test percentage
2017-05-07 07:00:26,109 : INFO : 0.774299835255
2017-05-07 07:18:48,747 : INFO : ==> Train loss   : 0.040583
2017-05-07 07:18:48,747 : INFO : Epoch
2017-05-07 07:18:48,748 : INFO : 27
2017-05-07 07:18:48,748 : INFO : train percentage
2017-05-07 07:18:48,748 : INFO : 0.992919075145
2017-05-07 07:18:48,748 : INFO : Epoch
2017-05-07 07:18:48,748 : INFO : 27
2017-05-07 07:18:48,748 : INFO : dev percentage
2017-05-07 07:18:48,748 : INFO : 0.758027522936
2017-05-07 07:18:48,748 : INFO : Epoch
2017-05-07 07:18:48,749 : INFO : 27
2017-05-07 07:18:48,749 : INFO : test percentage
2017-05-07 07:18:48,749 : INFO : 0.782537067545
2017-05-07 07:37:13,613 : INFO : ==> Train loss   : 0.035783
2017-05-07 07:37:13,613 : INFO : Epoch
2017-05-07 07:37:13,613 : INFO : 28
2017-05-07 07:37:13,613 : INFO : train percentage
2017-05-07 07:37:13,613 : INFO : 0.992774566474
2017-05-07 07:37:13,614 : INFO : Epoch
2017-05-07 07:37:13,614 : INFO : 28
2017-05-07 07:37:13,614 : INFO : dev percentage
2017-05-07 07:37:13,614 : INFO : 0.751146788991
2017-05-07 07:37:13,614 : INFO : Epoch
2017-05-07 07:37:13,614 : INFO : 28
2017-05-07 07:37:13,614 : INFO : test percentage
2017-05-07 07:37:13,614 : INFO : 0.777045579352
2017-05-07 07:55:32,572 : INFO : ==> Train loss   : 0.036345
2017-05-07 07:55:32,572 : INFO : Epoch
2017-05-07 07:55:32,572 : INFO : 29
2017-05-07 07:55:32,572 : INFO : train percentage
2017-05-07 07:55:32,572 : INFO : 0.994219653179
2017-05-07 07:55:32,572 : INFO : Epoch
2017-05-07 07:55:32,573 : INFO : 29
2017-05-07 07:55:32,573 : INFO : dev percentage
2017-05-07 07:55:32,573 : INFO : 0.756880733945
2017-05-07 07:55:32,573 : INFO : Epoch
2017-05-07 07:55:32,573 : INFO : 29
2017-05-07 07:55:32,573 : INFO : test percentage
2017-05-07 07:55:32,574 : INFO : 0.779791323449
2017-05-07 08:13:51,026 : INFO : ==> Train loss   : 0.036306
2017-05-07 08:13:51,026 : INFO : Epoch
2017-05-07 08:13:51,026 : INFO : 30
2017-05-07 08:13:51,027 : INFO : train percentage
2017-05-07 08:13:51,027 : INFO : 0.994219653179
2017-05-07 08:13:51,027 : INFO : Epoch
2017-05-07 08:13:51,027 : INFO : 30
2017-05-07 08:13:51,027 : INFO : dev percentage
2017-05-07 08:13:51,027 : INFO : 0.759174311927
2017-05-07 08:13:51,027 : INFO : Epoch
2017-05-07 08:13:51,027 : INFO : 30
2017-05-07 08:13:51,028 : INFO : test percentage
2017-05-07 08:13:51,028 : INFO : 0.77869302581
2017-05-07 08:32:09,566 : INFO : ==> Train loss   : 0.035594
2017-05-07 08:32:09,566 : INFO : Epoch
2017-05-07 08:32:09,566 : INFO : 31
2017-05-07 08:32:09,566 : INFO : train percentage
2017-05-07 08:32:09,566 : INFO : 0.99450867052
2017-05-07 08:32:09,566 : INFO : Epoch
2017-05-07 08:32:09,567 : INFO : 31
2017-05-07 08:32:09,567 : INFO : dev percentage
2017-05-07 08:32:09,567 : INFO : 0.758027522936
2017-05-07 08:32:09,567 : INFO : Epoch
2017-05-07 08:32:09,567 : INFO : 31
2017-05-07 08:32:09,567 : INFO : test percentage
2017-05-07 08:32:09,567 : INFO : 0.785831960461
2017-05-07 08:50:28,784 : INFO : ==> Train loss   : 0.034374
2017-05-07 08:50:28,784 : INFO : Epoch
2017-05-07 08:50:28,784 : INFO : 32
2017-05-07 08:50:28,784 : INFO : train percentage
2017-05-07 08:50:28,784 : INFO : 0.994797687861
2017-05-07 08:50:28,785 : INFO : Epoch
2017-05-07 08:50:28,785 : INFO : 32
2017-05-07 08:50:28,785 : INFO : dev percentage
2017-05-07 08:50:28,785 : INFO : 0.753440366972
2017-05-07 08:50:28,785 : INFO : Epoch
2017-05-07 08:50:28,785 : INFO : 32
2017-05-07 08:50:28,785 : INFO : test percentage
2017-05-07 08:50:28,785 : INFO : 0.785282811642
2017-05-07 09:08:50,020 : INFO : ==> Train loss   : 0.032505
2017-05-07 09:08:50,021 : INFO : Epoch
2017-05-07 09:08:50,021 : INFO : 33
2017-05-07 09:08:50,021 : INFO : train percentage
2017-05-07 09:08:50,021 : INFO : 0.995086705202
2017-05-07 09:08:50,021 : INFO : Epoch
2017-05-07 09:08:50,021 : INFO : 33
2017-05-07 09:08:50,022 : INFO : dev percentage
2017-05-07 09:08:50,022 : INFO : 0.753440366972
2017-05-07 09:08:50,022 : INFO : Epoch
2017-05-07 09:08:50,022 : INFO : 33
2017-05-07 09:08:50,022 : INFO : test percentage
2017-05-07 09:08:50,022 : INFO : 0.784184514003
2017-05-07 09:27:13,793 : INFO : ==> Train loss   : 0.033180
2017-05-07 09:27:13,793 : INFO : Epoch
2017-05-07 09:27:13,794 : INFO : 34
2017-05-07 09:27:13,794 : INFO : train percentage
2017-05-07 09:27:13,794 : INFO : 0.994653179191
2017-05-07 09:27:13,794 : INFO : Epoch
2017-05-07 09:27:13,794 : INFO : 34
2017-05-07 09:27:13,794 : INFO : dev percentage
2017-05-07 09:27:13,794 : INFO : 0.768348623853
2017-05-07 09:27:13,794 : INFO : Epoch
2017-05-07 09:27:13,795 : INFO : 34
2017-05-07 09:27:13,795 : INFO : test percentage
2017-05-07 09:27:13,795 : INFO : 0.781987918726
2017-05-07 09:45:51,835 : INFO : ==> Train loss   : 0.029905
2017-05-07 09:45:51,836 : INFO : Epoch
2017-05-07 09:45:51,836 : INFO : 35
2017-05-07 09:45:51,836 : INFO : train percentage
2017-05-07 09:45:51,836 : INFO : 0.995375722543
2017-05-07 09:45:51,836 : INFO : Epoch
2017-05-07 09:45:51,836 : INFO : 35
2017-05-07 09:45:51,836 : INFO : dev percentage
2017-05-07 09:45:51,837 : INFO : 0.756880733945
2017-05-07 09:45:51,837 : INFO : Epoch
2017-05-07 09:45:51,837 : INFO : 35
2017-05-07 09:45:51,837 : INFO : test percentage
2017-05-07 09:45:51,837 : INFO : 0.779242174629
2017-05-07 10:04:11,320 : INFO : ==> Train loss   : 0.031770
2017-05-07 10:04:11,321 : INFO : Epoch
2017-05-07 10:04:11,321 : INFO : 36
2017-05-07 10:04:11,321 : INFO : train percentage
2017-05-07 10:04:11,321 : INFO : 0.995231213873
2017-05-07 10:04:11,321 : INFO : Epoch
2017-05-07 10:04:11,321 : INFO : 36
2017-05-07 10:04:11,321 : INFO : dev percentage
2017-05-07 10:04:11,322 : INFO : 0.758027522936
2017-05-07 10:04:11,322 : INFO : Epoch
2017-05-07 10:04:11,322 : INFO : 36
2017-05-07 10:04:11,322 : INFO : test percentage
2017-05-07 10:04:11,322 : INFO : 0.779242174629
2017-05-07 10:22:37,235 : INFO : ==> Train loss   : 0.029919
2017-05-07 10:22:37,235 : INFO : Epoch
2017-05-07 10:22:37,235 : INFO : 37
2017-05-07 10:22:37,236 : INFO : train percentage
2017-05-07 10:22:37,236 : INFO : 0.996098265896
2017-05-07 10:22:37,236 : INFO : Epoch
2017-05-07 10:22:37,236 : INFO : 37
2017-05-07 10:22:37,236 : INFO : dev percentage
2017-05-07 10:22:37,236 : INFO : 0.759174311927
2017-05-07 10:22:37,236 : INFO : Epoch
2017-05-07 10:22:37,237 : INFO : 37
2017-05-07 10:22:37,237 : INFO : test percentage
2017-05-07 10:22:37,237 : INFO : 0.780889621087
2017-05-07 10:41:07,112 : INFO : ==> Train loss   : 0.028959
2017-05-07 10:41:07,112 : INFO : Epoch
2017-05-07 10:41:07,112 : INFO : 38
2017-05-07 10:41:07,112 : INFO : train percentage
2017-05-07 10:41:07,112 : INFO : 0.996098265896
2017-05-07 10:41:07,113 : INFO : Epoch
2017-05-07 10:41:07,113 : INFO : 38
2017-05-07 10:41:07,113 : INFO : dev percentage
2017-05-07 10:41:07,113 : INFO : 0.760321100917
2017-05-07 10:41:07,113 : INFO : Epoch
2017-05-07 10:41:07,113 : INFO : 38
2017-05-07 10:41:07,113 : INFO : test percentage
2017-05-07 10:41:07,113 : INFO : 0.780889621087
2017-05-07 10:59:39,734 : INFO : ==> Train loss   : 0.028536
2017-05-07 10:59:39,734 : INFO : Epoch
2017-05-07 10:59:39,734 : INFO : 39
2017-05-07 10:59:39,734 : INFO : train percentage
2017-05-07 10:59:39,735 : INFO : 0.995375722543
2017-05-07 10:59:39,735 : INFO : Epoch
2017-05-07 10:59:39,735 : INFO : 39
2017-05-07 10:59:39,735 : INFO : dev percentage
2017-05-07 10:59:39,735 : INFO : 0.756880733945
2017-05-07 10:59:39,735 : INFO : Epoch
2017-05-07 10:59:39,735 : INFO : 39
2017-05-07 10:59:39,735 : INFO : test percentage
2017-05-07 10:59:39,736 : INFO : 0.780340472268
2017-05-07 11:18:06,495 : INFO : ==> Train loss   : 0.029545
2017-05-07 11:18:06,495 : INFO : Epoch
2017-05-07 11:18:06,495 : INFO : 40
2017-05-07 11:18:06,495 : INFO : train percentage
2017-05-07 11:18:06,495 : INFO : 0.99450867052
2017-05-07 11:18:06,495 : INFO : Epoch
2017-05-07 11:18:06,495 : INFO : 40
2017-05-07 11:18:06,495 : INFO : dev percentage
2017-05-07 11:18:06,496 : INFO : 0.758027522936
2017-05-07 11:18:06,496 : INFO : Epoch
2017-05-07 11:18:06,496 : INFO : 40
2017-05-07 11:18:06,496 : INFO : test percentage
2017-05-07 11:18:06,496 : INFO : 0.780889621087
2017-05-07 11:36:33,225 : INFO : ==> Train loss   : 0.028769
2017-05-07 11:36:33,225 : INFO : Epoch
2017-05-07 11:36:33,225 : INFO : 41
2017-05-07 11:36:33,225 : INFO : train percentage
2017-05-07 11:36:33,226 : INFO : 0.995809248555
2017-05-07 11:36:33,226 : INFO : Epoch
2017-05-07 11:36:33,226 : INFO : 41
2017-05-07 11:36:33,226 : INFO : dev percentage
2017-05-07 11:36:33,226 : INFO : 0.766055045872
2017-05-07 11:36:33,226 : INFO : Epoch
2017-05-07 11:36:33,226 : INFO : 41
2017-05-07 11:36:33,226 : INFO : test percentage
2017-05-07 11:36:33,227 : INFO : 0.780889621087
2017-05-07 11:55:05,992 : INFO : ==> Train loss   : 0.027628
2017-05-07 11:55:05,993 : INFO : Epoch
2017-05-07 11:55:05,993 : INFO : 42
2017-05-07 11:55:05,993 : INFO : train percentage
2017-05-07 11:55:05,993 : INFO : 0.996098265896
2017-05-07 11:55:05,993 : INFO : Epoch
2017-05-07 11:55:05,993 : INFO : 42
2017-05-07 11:55:05,993 : INFO : dev percentage
2017-05-07 11:55:05,993 : INFO : 0.752293577982
2017-05-07 11:55:05,994 : INFO : Epoch
2017-05-07 11:55:05,994 : INFO : 42
2017-05-07 11:55:05,994 : INFO : test percentage
2017-05-07 11:55:05,994 : INFO : 0.780889621087
2017-05-07 12:13:27,351 : INFO : ==> Train loss   : 0.027335
2017-05-07 12:13:27,351 : INFO : Epoch
2017-05-07 12:13:27,351 : INFO : 43
2017-05-07 12:13:27,351 : INFO : train percentage
2017-05-07 12:13:27,352 : INFO : 0.995664739884
2017-05-07 12:13:27,352 : INFO : Epoch
2017-05-07 12:13:27,352 : INFO : 43
2017-05-07 12:13:27,352 : INFO : dev percentage
2017-05-07 12:13:27,352 : INFO : 0.756880733945
2017-05-07 12:13:27,352 : INFO : Epoch
2017-05-07 12:13:27,352 : INFO : 43
2017-05-07 12:13:27,352 : INFO : test percentage
2017-05-07 12:13:27,352 : INFO : 0.777594728171
2017-05-07 12:32:02,918 : INFO : ==> Train loss   : 0.028654
2017-05-07 12:32:02,918 : INFO : Epoch
2017-05-07 12:32:02,918 : INFO : 44
2017-05-07 12:32:02,919 : INFO : train percentage
2017-05-07 12:32:02,919 : INFO : 0.996387283237
2017-05-07 12:32:02,919 : INFO : Epoch
2017-05-07 12:32:02,919 : INFO : 44
2017-05-07 12:32:02,919 : INFO : dev percentage
2017-05-07 12:32:02,919 : INFO : 0.759174311927
2017-05-07 12:32:02,919 : INFO : Epoch
2017-05-07 12:32:02,920 : INFO : 44
2017-05-07 12:32:02,920 : INFO : test percentage
2017-05-07 12:32:02,920 : INFO : 0.777045579352
2017-05-07 12:50:27,258 : INFO : ==> Train loss   : 0.027044
2017-05-07 12:50:27,259 : INFO : Epoch
2017-05-07 12:50:27,259 : INFO : 45
2017-05-07 12:50:27,259 : INFO : train percentage
2017-05-07 12:50:27,259 : INFO : 0.995520231214
2017-05-07 12:50:27,259 : INFO : Epoch
2017-05-07 12:50:27,259 : INFO : 45
2017-05-07 12:50:27,259 : INFO : dev percentage
2017-05-07 12:50:27,260 : INFO : 0.759174311927
2017-05-07 12:50:27,260 : INFO : Epoch
2017-05-07 12:50:27,260 : INFO : 45
2017-05-07 12:50:27,260 : INFO : test percentage
2017-05-07 12:50:27,260 : INFO : 0.781987918726
2017-05-07 13:08:56,504 : INFO : ==> Train loss   : 0.024827
2017-05-07 13:08:56,504 : INFO : Epoch
2017-05-07 13:08:56,504 : INFO : 46
2017-05-07 13:08:56,504 : INFO : train percentage
2017-05-07 13:08:56,504 : INFO : 0.996098265896
2017-05-07 13:08:56,504 : INFO : Epoch
2017-05-07 13:08:56,504 : INFO : 46
2017-05-07 13:08:56,505 : INFO : dev percentage
2017-05-07 13:08:56,505 : INFO : 0.756880733945
2017-05-07 13:08:56,505 : INFO : Epoch
2017-05-07 13:08:56,505 : INFO : 46
2017-05-07 13:08:56,505 : INFO : test percentage
2017-05-07 13:08:56,505 : INFO : 0.779791323449
2017-05-07 13:27:29,149 : INFO : ==> Train loss   : 0.025454
2017-05-07 13:27:29,149 : INFO : Epoch
2017-05-07 13:27:29,149 : INFO : 47
2017-05-07 13:27:29,149 : INFO : train percentage
2017-05-07 13:27:29,150 : INFO : 0.996242774566
2017-05-07 13:27:29,150 : INFO : Epoch
2017-05-07 13:27:29,150 : INFO : 47
2017-05-07 13:27:29,150 : INFO : dev percentage
2017-05-07 13:27:29,150 : INFO : 0.755733944954
2017-05-07 13:27:29,150 : INFO : Epoch
2017-05-07 13:27:29,150 : INFO : 47
2017-05-07 13:27:29,150 : INFO : test percentage
2017-05-07 13:27:29,151 : INFO : 0.771554091159
2017-05-07 13:45:42,287 : INFO : ==> Train loss   : 0.025488
2017-05-07 13:45:42,287 : INFO : Epoch
2017-05-07 13:45:42,287 : INFO : 48
2017-05-07 13:45:42,287 : INFO : train percentage
2017-05-07 13:45:42,287 : INFO : 0.996242774566
2017-05-07 13:45:42,287 : INFO : Epoch
2017-05-07 13:45:42,287 : INFO : 48
2017-05-07 13:45:42,288 : INFO : dev percentage
2017-05-07 13:45:42,288 : INFO : 0.761467889908
2017-05-07 13:45:42,288 : INFO : Epoch
2017-05-07 13:45:42,288 : INFO : 48
2017-05-07 13:45:42,288 : INFO : test percentage
2017-05-07 13:45:42,288 : INFO : 0.781438769907
2017-05-07 14:04:10,977 : INFO : ==> Train loss   : 0.022955
2017-05-07 14:04:10,977 : INFO : Epoch
2017-05-07 14:04:10,977 : INFO : 49
2017-05-07 14:04:10,977 : INFO : train percentage
2017-05-07 14:04:10,977 : INFO : 0.996387283237
2017-05-07 14:04:10,977 : INFO : Epoch
2017-05-07 14:04:10,977 : INFO : 49
2017-05-07 14:04:10,977 : INFO : dev percentage
2017-05-07 14:04:10,978 : INFO : 0.759174311927
2017-05-07 14:04:10,978 : INFO : Epoch
2017-05-07 14:04:10,978 : INFO : 49
2017-05-07 14:04:10,978 : INFO : test percentage
2017-05-07 14:04:10,978 : INFO : 0.773750686436
2017-05-07 14:22:38,761 : INFO : ==> Train loss   : 0.023644
2017-05-07 14:22:38,762 : INFO : Epoch
2017-05-07 14:22:38,762 : INFO : 50
2017-05-07 14:22:38,762 : INFO : train percentage
2017-05-07 14:22:38,762 : INFO : 0.996531791908
2017-05-07 14:22:38,762 : INFO : Epoch
2017-05-07 14:22:38,762 : INFO : 50
2017-05-07 14:22:38,762 : INFO : dev percentage
2017-05-07 14:22:38,763 : INFO : 0.762614678899
2017-05-07 14:22:38,763 : INFO : Epoch
2017-05-07 14:22:38,763 : INFO : 50
2017-05-07 14:22:38,763 : INFO : test percentage
2017-05-07 14:22:38,763 : INFO : 0.777045579352
2017-05-07 14:41:15,904 : INFO : ==> Train loss   : 0.025586
2017-05-07 14:41:15,904 : INFO : Epoch
2017-05-07 14:41:15,905 : INFO : 51
2017-05-07 14:41:15,905 : INFO : train percentage
2017-05-07 14:41:15,905 : INFO : 0.995664739884
2017-05-07 14:41:15,905 : INFO : Epoch
2017-05-07 14:41:15,905 : INFO : 51
2017-05-07 14:41:15,905 : INFO : dev percentage
2017-05-07 14:41:15,905 : INFO : 0.756880733945
2017-05-07 14:41:15,905 : INFO : Epoch
2017-05-07 14:41:15,906 : INFO : 51
2017-05-07 14:41:15,906 : INFO : test percentage
2017-05-07 14:41:15,906 : INFO : 0.777045579352
2017-05-07 14:59:35,683 : INFO : ==> Train loss   : 0.023717
2017-05-07 14:59:35,684 : INFO : Epoch
2017-05-07 14:59:35,684 : INFO : 52
2017-05-07 14:59:35,684 : INFO : train percentage
2017-05-07 14:59:35,684 : INFO : 0.996242774566
2017-05-07 14:59:35,684 : INFO : Epoch
2017-05-07 14:59:35,684 : INFO : 52
2017-05-07 14:59:35,684 : INFO : dev percentage
2017-05-07 14:59:35,685 : INFO : 0.761467889908
2017-05-07 14:59:35,685 : INFO : Epoch
2017-05-07 14:59:35,685 : INFO : 52
2017-05-07 14:59:35,685 : INFO : test percentage
2017-05-07 14:59:35,685 : INFO : 0.778143876991
2017-05-07 15:17:54,420 : INFO : ==> Train loss   : 0.022934
2017-05-07 15:17:54,420 : INFO : Epoch
2017-05-07 15:17:54,420 : INFO : 53
2017-05-07 15:17:54,421 : INFO : train percentage
2017-05-07 15:17:54,421 : INFO : 0.995953757225
2017-05-07 15:17:54,421 : INFO : Epoch
2017-05-07 15:17:54,421 : INFO : 53
2017-05-07 15:17:54,421 : INFO : dev percentage
2017-05-07 15:17:54,421 : INFO : 0.762614678899
2017-05-07 15:17:54,421 : INFO : Epoch
2017-05-07 15:17:54,421 : INFO : 53
2017-05-07 15:17:54,422 : INFO : test percentage
2017-05-07 15:17:54,422 : INFO : 0.77045579352
2017-05-07 15:36:21,065 : INFO : ==> Train loss   : 0.022904
2017-05-07 15:36:21,065 : INFO : Epoch
2017-05-07 15:36:21,066 : INFO : 54
2017-05-07 15:36:21,066 : INFO : train percentage
2017-05-07 15:36:21,066 : INFO : 0.996242774566
2017-05-07 15:36:21,066 : INFO : Epoch
2017-05-07 15:36:21,066 : INFO : 54
2017-05-07 15:36:21,066 : INFO : dev percentage
2017-05-07 15:36:21,066 : INFO : 0.759174311927
2017-05-07 15:36:21,066 : INFO : Epoch
2017-05-07 15:36:21,067 : INFO : 54
2017-05-07 15:36:21,067 : INFO : test percentage
2017-05-07 15:36:21,067 : INFO : 0.779242174629
2017-05-07 15:54:54,246 : INFO : ==> Train loss   : 0.022857
2017-05-07 15:54:54,246 : INFO : Epoch
2017-05-07 15:54:54,246 : INFO : 55
2017-05-07 15:54:54,246 : INFO : train percentage
2017-05-07 15:54:54,246 : INFO : 0.996098265896
2017-05-07 15:54:54,246 : INFO : Epoch
2017-05-07 15:54:54,247 : INFO : 55
2017-05-07 15:54:54,247 : INFO : dev percentage
2017-05-07 15:54:54,247 : INFO : 0.75
2017-05-07 15:54:54,247 : INFO : Epoch
2017-05-07 15:54:54,247 : INFO : 55
2017-05-07 15:54:54,247 : INFO : test percentage
2017-05-07 15:54:54,247 : INFO : 0.774299835255
2017-05-07 16:13:43,602 : INFO : ==> Train loss   : 0.022285
2017-05-07 16:13:43,603 : INFO : Epoch
2017-05-07 16:13:43,603 : INFO : 56
2017-05-07 16:13:43,603 : INFO : train percentage
2017-05-07 16:13:43,603 : INFO : 0.996098265896
2017-05-07 16:13:43,603 : INFO : Epoch
2017-05-07 16:13:43,603 : INFO : 56
2017-05-07 16:13:43,603 : INFO : dev percentage
2017-05-07 16:13:43,604 : INFO : 0.754587155963
2017-05-07 16:13:43,604 : INFO : Epoch
2017-05-07 16:13:43,604 : INFO : 56
2017-05-07 16:13:43,604 : INFO : test percentage
2017-05-07 16:13:43,604 : INFO : 0.773750686436
2017-05-07 16:31:58,435 : INFO : ==> Train loss   : 0.021239
2017-05-07 16:31:58,435 : INFO : Epoch
2017-05-07 16:31:58,435 : INFO : 57
2017-05-07 16:31:58,436 : INFO : train percentage
2017-05-07 16:31:58,436 : INFO : 0.996242774566
2017-05-07 16:31:58,436 : INFO : Epoch
2017-05-07 16:31:58,436 : INFO : 57
2017-05-07 16:31:58,436 : INFO : dev percentage
2017-05-07 16:31:58,436 : INFO : 0.754587155963
2017-05-07 16:31:58,436 : INFO : Epoch
2017-05-07 16:31:58,436 : INFO : 57
2017-05-07 16:31:58,436 : INFO : test percentage
2017-05-07 16:31:58,437 : INFO : 0.776496430533
2017-05-07 16:50:17,051 : INFO : ==> Train loss   : 0.021405
2017-05-07 16:50:17,051 : INFO : Epoch
2017-05-07 16:50:17,051 : INFO : 58
2017-05-07 16:50:17,051 : INFO : train percentage
2017-05-07 16:50:17,052 : INFO : 0.996676300578
2017-05-07 16:50:17,052 : INFO : Epoch
2017-05-07 16:50:17,052 : INFO : 58
2017-05-07 16:50:17,052 : INFO : dev percentage
2017-05-07 16:50:17,052 : INFO : 0.747706422018
2017-05-07 16:50:17,052 : INFO : Epoch
2017-05-07 16:50:17,052 : INFO : 58
2017-05-07 16:50:17,052 : INFO : test percentage
2017-05-07 16:50:17,053 : INFO : 0.777045579352
2017-05-07 17:08:34,537 : INFO : ==> Train loss   : 0.021101
2017-05-07 17:08:34,537 : INFO : Epoch
2017-05-07 17:08:34,537 : INFO : 59
2017-05-07 17:08:34,537 : INFO : train percentage
2017-05-07 17:08:34,537 : INFO : 0.99725433526
2017-05-07 17:08:34,537 : INFO : Epoch
2017-05-07 17:08:34,538 : INFO : 59
2017-05-07 17:08:34,538 : INFO : dev percentage
2017-05-07 17:08:34,538 : INFO : 0.751146788991
2017-05-07 17:08:34,538 : INFO : Epoch
2017-05-07 17:08:34,538 : INFO : 59
2017-05-07 17:08:34,538 : INFO : test percentage
2017-05-07 17:08:34,538 : INFO : 0.776496430533
2017-05-07 17:26:51,476 : INFO : ==> Train loss   : 0.020648
2017-05-07 17:26:51,476 : INFO : Epoch
2017-05-07 17:26:51,476 : INFO : 60
2017-05-07 17:26:51,476 : INFO : train percentage
2017-05-07 17:26:51,477 : INFO : 0.996676300578
2017-05-07 17:26:51,477 : INFO : Epoch
2017-05-07 17:26:51,477 : INFO : 60
2017-05-07 17:26:51,477 : INFO : dev percentage
2017-05-07 17:26:51,477 : INFO : 0.753440366972
2017-05-07 17:26:51,477 : INFO : Epoch
2017-05-07 17:26:51,477 : INFO : 60
2017-05-07 17:26:51,477 : INFO : test percentage
2017-05-07 17:26:51,478 : INFO : 0.775947281713
2017-05-07 17:45:07,097 : INFO : ==> Train loss   : 0.021113
2017-05-07 17:45:07,097 : INFO : Epoch
2017-05-07 17:45:07,097 : INFO : 61
2017-05-07 17:45:07,097 : INFO : train percentage
2017-05-07 17:45:07,098 : INFO : 0.997687861272
2017-05-07 17:45:07,098 : INFO : Epoch
2017-05-07 17:45:07,098 : INFO : 61
2017-05-07 17:45:07,098 : INFO : dev percentage
2017-05-07 17:45:07,098 : INFO : 0.75
2017-05-07 17:45:07,098 : INFO : Epoch
2017-05-07 17:45:07,098 : INFO : 61
2017-05-07 17:45:07,098 : INFO : test percentage
2017-05-07 17:45:07,099 : INFO : 0.774848984075
2017-05-07 18:03:33,558 : INFO : ==> Train loss   : 0.020655
2017-05-07 18:03:33,558 : INFO : Epoch
2017-05-07 18:03:33,558 : INFO : 62
2017-05-07 18:03:33,559 : INFO : train percentage
2017-05-07 18:03:33,559 : INFO : 0.99725433526
2017-05-07 18:03:33,559 : INFO : Epoch
2017-05-07 18:03:33,559 : INFO : 62
2017-05-07 18:03:33,559 : INFO : dev percentage
2017-05-07 18:03:33,559 : INFO : 0.75
2017-05-07 18:03:33,559 : INFO : Epoch
2017-05-07 18:03:33,559 : INFO : 62
2017-05-07 18:03:33,560 : INFO : test percentage
2017-05-07 18:03:33,560 : INFO : 0.772652388797
2017-05-07 18:22:26,167 : INFO : ==> Train loss   : 0.020124
2017-05-07 18:22:26,167 : INFO : Epoch
2017-05-07 18:22:26,167 : INFO : 63
2017-05-07 18:22:26,167 : INFO : train percentage
2017-05-07 18:22:26,167 : INFO : 0.996965317919
2017-05-07 18:22:26,168 : INFO : Epoch
2017-05-07 18:22:26,168 : INFO : 63
2017-05-07 18:22:26,168 : INFO : dev percentage
2017-05-07 18:22:26,168 : INFO : 0.752293577982
2017-05-07 18:22:26,168 : INFO : Epoch
2017-05-07 18:22:26,168 : INFO : 63
2017-05-07 18:22:26,168 : INFO : test percentage
2017-05-07 18:22:26,169 : INFO : 0.773201537617
2017-05-07 18:40:20,299 : INFO : ==> Train loss   : 0.019469
2017-05-07 18:40:20,299 : INFO : Epoch
2017-05-07 18:40:20,299 : INFO : 64
2017-05-07 18:40:20,299 : INFO : train percentage
2017-05-07 18:40:20,299 : INFO : 0.996965317919
2017-05-07 18:40:20,299 : INFO : Epoch
2017-05-07 18:40:20,300 : INFO : 64
2017-05-07 18:40:20,300 : INFO : dev percentage
2017-05-07 18:40:20,300 : INFO : 0.753440366972
2017-05-07 18:40:20,300 : INFO : Epoch
2017-05-07 18:40:20,300 : INFO : 64
2017-05-07 18:40:20,300 : INFO : test percentage
2017-05-07 18:40:20,300 : INFO : 0.774299835255
2017-05-07 18:58:24,207 : INFO : ==> Train loss   : 0.018815
2017-05-07 18:58:24,207 : INFO : Epoch
2017-05-07 18:58:24,207 : INFO : 65
2017-05-07 18:58:24,207 : INFO : train percentage
2017-05-07 18:58:24,207 : INFO : 0.99725433526
2017-05-07 18:58:24,207 : INFO : Epoch
2017-05-07 18:58:24,207 : INFO : 65
2017-05-07 18:58:24,207 : INFO : dev percentage
2017-05-07 18:58:24,208 : INFO : 0.75
2017-05-07 18:58:24,208 : INFO : Epoch
2017-05-07 18:58:24,208 : INFO : 65
2017-05-07 18:58:24,208 : INFO : test percentage
2017-05-07 18:58:24,208 : INFO : 0.773750686436
2017-05-07 19:16:27,521 : INFO : ==> Train loss   : 0.017730
2017-05-07 19:16:27,521 : INFO : Epoch
2017-05-07 19:16:27,521 : INFO : 66
2017-05-07 19:16:27,521 : INFO : train percentage
2017-05-07 19:16:27,522 : INFO : 0.996965317919
2017-05-07 19:16:27,522 : INFO : Epoch
2017-05-07 19:16:27,522 : INFO : 66
2017-05-07 19:16:27,522 : INFO : dev percentage
2017-05-07 19:16:27,522 : INFO : 0.755733944954
2017-05-07 19:16:27,522 : INFO : Epoch
2017-05-07 19:16:27,522 : INFO : 66
2017-05-07 19:16:27,522 : INFO : test percentage
2017-05-07 19:16:27,523 : INFO : 0.774299835255
2017-05-07 19:33:48,006 : INFO : ==> Train loss   : 0.018650
2017-05-07 19:33:48,006 : INFO : Epoch
2017-05-07 19:33:48,006 : INFO : 67
2017-05-07 19:33:48,006 : INFO : train percentage
2017-05-07 19:33:48,006 : INFO : 0.997398843931
2017-05-07 19:33:48,006 : INFO : Epoch
2017-05-07 19:33:48,006 : INFO : 67
2017-05-07 19:33:48,006 : INFO : dev percentage
2017-05-07 19:33:48,007 : INFO : 0.756880733945
2017-05-07 19:33:48,007 : INFO : Epoch
2017-05-07 19:33:48,007 : INFO : 67
2017-05-07 19:33:48,007 : INFO : test percentage
2017-05-07 19:33:48,007 : INFO : 0.773750686436
2017-05-07 19:51:15,578 : INFO : ==> Train loss   : 0.019107
2017-05-07 19:51:15,578 : INFO : Epoch
2017-05-07 19:51:15,578 : INFO : 68
2017-05-07 19:51:15,579 : INFO : train percentage
2017-05-07 19:51:15,579 : INFO : 0.996965317919
2017-05-07 19:51:15,579 : INFO : Epoch
2017-05-07 19:51:15,579 : INFO : 68
2017-05-07 19:51:15,579 : INFO : dev percentage
2017-05-07 19:51:15,579 : INFO : 0.754587155963
2017-05-07 19:51:15,579 : INFO : Epoch
2017-05-07 19:51:15,579 : INFO : 68
2017-05-07 19:51:15,579 : INFO : test percentage
2017-05-07 19:51:15,580 : INFO : 0.775398132894
2017-05-07 20:08:43,730 : INFO : ==> Train loss   : 0.018389
2017-05-07 20:08:43,730 : INFO : Epoch
2017-05-07 20:08:43,730 : INFO : 69
2017-05-07 20:08:43,730 : INFO : train percentage
2017-05-07 20:08:43,730 : INFO : 0.99710982659
2017-05-07 20:08:43,730 : INFO : Epoch
2017-05-07 20:08:43,731 : INFO : 69
2017-05-07 20:08:43,731 : INFO : dev percentage
2017-05-07 20:08:43,731 : INFO : 0.752293577982
2017-05-07 20:08:43,731 : INFO : Epoch
2017-05-07 20:08:43,731 : INFO : 69
2017-05-07 20:08:43,731 : INFO : test percentage
2017-05-07 20:08:43,731 : INFO : 0.777594728171
2017-05-07 20:26:10,089 : INFO : ==> Train loss   : 0.019317
2017-05-07 20:26:10,090 : INFO : Epoch
2017-05-07 20:26:10,090 : INFO : 70
2017-05-07 20:26:10,090 : INFO : train percentage
2017-05-07 20:26:10,090 : INFO : 0.996965317919
2017-05-07 20:26:10,090 : INFO : Epoch
2017-05-07 20:26:10,090 : INFO : 70
2017-05-07 20:26:10,090 : INFO : dev percentage
2017-05-07 20:26:10,090 : INFO : 0.748853211009
2017-05-07 20:26:10,090 : INFO : Epoch
2017-05-07 20:26:10,091 : INFO : 70
2017-05-07 20:26:10,091 : INFO : test percentage
2017-05-07 20:26:10,091 : INFO : 0.779242174629
2017-05-07 20:43:28,835 : INFO : ==> Train loss   : 0.017957
2017-05-07 20:43:28,835 : INFO : Epoch
2017-05-07 20:43:28,835 : INFO : 71
2017-05-07 20:43:28,835 : INFO : train percentage
2017-05-07 20:43:28,836 : INFO : 0.99725433526
2017-05-07 20:43:28,836 : INFO : Epoch
2017-05-07 20:43:28,836 : INFO : 71
2017-05-07 20:43:28,836 : INFO : dev percentage
2017-05-07 20:43:28,836 : INFO : 0.746559633028
2017-05-07 20:43:28,836 : INFO : Epoch
2017-05-07 20:43:28,836 : INFO : 71
2017-05-07 20:43:28,836 : INFO : test percentage
2017-05-07 20:43:28,837 : INFO : 0.772103239978
2017-05-07 21:00:58,554 : INFO : ==> Train loss   : 0.017128
2017-05-07 21:00:58,554 : INFO : Epoch
2017-05-07 21:00:58,554 : INFO : 72
2017-05-07 21:00:58,554 : INFO : train percentage
2017-05-07 21:00:58,554 : INFO : 0.99725433526
2017-05-07 21:00:58,554 : INFO : Epoch
2017-05-07 21:00:58,554 : INFO : 72
2017-05-07 21:00:58,555 : INFO : dev percentage
2017-05-07 21:00:58,555 : INFO : 0.755733944954
2017-05-07 21:00:58,555 : INFO : Epoch
2017-05-07 21:00:58,555 : INFO : 72
2017-05-07 21:00:58,555 : INFO : test percentage
2017-05-07 21:00:58,555 : INFO : 0.777594728171
2017-05-07 21:18:28,828 : INFO : ==> Train loss   : 0.017247
2017-05-07 21:18:28,828 : INFO : Epoch
2017-05-07 21:18:28,828 : INFO : 73
2017-05-07 21:18:28,828 : INFO : train percentage
2017-05-07 21:18:28,828 : INFO : 0.997398843931
2017-05-07 21:18:28,829 : INFO : Epoch
2017-05-07 21:18:28,829 : INFO : 73
2017-05-07 21:18:28,829 : INFO : dev percentage
2017-05-07 21:18:28,829 : INFO : 0.751146788991
2017-05-07 21:18:28,829 : INFO : Epoch
2017-05-07 21:18:28,829 : INFO : 73
2017-05-07 21:18:28,829 : INFO : test percentage
2017-05-07 21:18:28,829 : INFO : 0.773201537617
2017-05-07 21:37:14,217 : INFO : ==> Train loss   : 0.018200
2017-05-07 21:37:14,217 : INFO : Epoch
2017-05-07 21:37:14,217 : INFO : 74
2017-05-07 21:37:14,217 : INFO : train percentage
2017-05-07 21:37:14,217 : INFO : 0.997398843931
2017-05-07 21:37:14,217 : INFO : Epoch
2017-05-07 21:37:14,217 : INFO : 74
2017-05-07 21:37:14,218 : INFO : dev percentage
2017-05-07 21:37:14,218 : INFO : 0.75
2017-05-07 21:37:14,218 : INFO : Epoch
2017-05-07 21:37:14,218 : INFO : 74
2017-05-07 21:37:14,218 : INFO : test percentage
2017-05-07 21:37:14,218 : INFO : 0.772103239978
2017-05-07 21:55:19,136 : INFO : ==> Train loss   : 0.018003
2017-05-07 21:55:19,136 : INFO : Epoch
2017-05-07 21:55:19,136 : INFO : 75
2017-05-07 21:55:19,137 : INFO : train percentage
2017-05-07 21:55:19,137 : INFO : 0.997543352601
2017-05-07 21:55:19,137 : INFO : Epoch
2017-05-07 21:55:19,137 : INFO : 75
2017-05-07 21:55:19,137 : INFO : dev percentage
2017-05-07 21:55:19,137 : INFO : 0.75
2017-05-07 21:55:19,137 : INFO : Epoch
2017-05-07 21:55:19,138 : INFO : 75
2017-05-07 21:55:19,138 : INFO : test percentage
2017-05-07 21:55:19,138 : INFO : 0.773201537617
2017-05-07 22:14:09,929 : INFO : ==> Train loss   : 0.017665
2017-05-07 22:14:09,929 : INFO : Epoch
2017-05-07 22:14:09,929 : INFO : 76
2017-05-07 22:14:09,929 : INFO : train percentage
2017-05-07 22:14:09,929 : INFO : 0.997398843931
2017-05-07 22:14:09,930 : INFO : Epoch
2017-05-07 22:14:09,930 : INFO : 76
2017-05-07 22:14:09,930 : INFO : dev percentage
2017-05-07 22:14:09,930 : INFO : 0.746559633028
2017-05-07 22:14:09,930 : INFO : Epoch
2017-05-07 22:14:09,930 : INFO : 76
2017-05-07 22:14:09,930 : INFO : test percentage
2017-05-07 22:14:09,930 : INFO : 0.772103239978
2017-05-07 22:31:19,130 : INFO : ==> Train loss   : 0.018197
2017-05-07 22:31:19,131 : INFO : Epoch
2017-05-07 22:31:19,131 : INFO : 77
2017-05-07 22:31:19,131 : INFO : train percentage
2017-05-07 22:31:19,131 : INFO : 0.99725433526
2017-05-07 22:31:19,131 : INFO : Epoch
2017-05-07 22:31:19,131 : INFO : 77
2017-05-07 22:31:19,131 : INFO : dev percentage
2017-05-07 22:31:19,132 : INFO : 0.744266055046
2017-05-07 22:31:19,132 : INFO : Epoch
2017-05-07 22:31:19,132 : INFO : 77
2017-05-07 22:31:19,132 : INFO : test percentage
2017-05-07 22:31:19,132 : INFO : 0.772652388797
2017-05-07 22:48:46,349 : INFO : ==> Train loss   : 0.017173
2017-05-07 22:48:46,349 : INFO : Epoch
2017-05-07 22:48:46,349 : INFO : 78
2017-05-07 22:48:46,349 : INFO : train percentage
2017-05-07 22:48:46,349 : INFO : 0.996965317919
2017-05-07 22:48:46,349 : INFO : Epoch
2017-05-07 22:48:46,349 : INFO : 78
2017-05-07 22:48:46,350 : INFO : dev percentage
2017-05-07 22:48:46,350 : INFO : 0.740825688073
2017-05-07 22:48:46,350 : INFO : Epoch
2017-05-07 22:48:46,350 : INFO : 78
2017-05-07 22:48:46,350 : INFO : test percentage
2017-05-07 22:48:46,350 : INFO : 0.771554091159
2017-05-07 23:07:11,982 : INFO : ==> Train loss   : 0.017019
2017-05-07 23:07:11,982 : INFO : Epoch
2017-05-07 23:07:11,982 : INFO : 79
2017-05-07 23:07:11,983 : INFO : train percentage
2017-05-07 23:07:11,983 : INFO : 0.99725433526
2017-05-07 23:07:11,983 : INFO : Epoch
2017-05-07 23:07:11,983 : INFO : 79
2017-05-07 23:07:11,983 : INFO : dev percentage
2017-05-07 23:07:11,983 : INFO : 0.748853211009
2017-05-07 23:07:11,983 : INFO : Epoch
2017-05-07 23:07:11,983 : INFO : 79
2017-05-07 23:07:11,983 : INFO : test percentage
2017-05-07 23:07:11,984 : INFO : 0.774299835255
2017-05-07 23:26:02,188 : INFO : ==> Train loss   : 0.016782
2017-05-07 23:26:02,188 : INFO : Epoch
2017-05-07 23:26:02,188 : INFO : 80
2017-05-07 23:26:02,189 : INFO : train percentage
2017-05-07 23:26:02,189 : INFO : 0.99710982659
2017-05-07 23:26:02,189 : INFO : Epoch
2017-05-07 23:26:02,189 : INFO : 80
2017-05-07 23:26:02,189 : INFO : dev percentage
2017-05-07 23:26:02,189 : INFO : 0.754587155963
2017-05-07 23:26:02,189 : INFO : Epoch
2017-05-07 23:26:02,189 : INFO : 80
2017-05-07 23:26:02,190 : INFO : test percentage
2017-05-07 23:26:02,190 : INFO : 0.771004942339
2017-05-07 23:44:28,743 : INFO : ==> Train loss   : 0.016884
2017-05-07 23:44:28,743 : INFO : Epoch
2017-05-07 23:44:28,743 : INFO : 81
2017-05-07 23:44:28,744 : INFO : train percentage
2017-05-07 23:44:28,744 : INFO : 0.997543352601
2017-05-07 23:44:28,744 : INFO : Epoch
2017-05-07 23:44:28,744 : INFO : 81
2017-05-07 23:44:28,744 : INFO : dev percentage
2017-05-07 23:44:28,744 : INFO : 0.752293577982
2017-05-07 23:44:28,744 : INFO : Epoch
2017-05-07 23:44:28,744 : INFO : 81
2017-05-07 23:44:28,744 : INFO : test percentage
2017-05-07 23:44:28,745 : INFO : 0.777045579352
2017-05-08 00:02:53,618 : INFO : ==> Train loss   : 0.017147
2017-05-08 00:02:53,618 : INFO : Epoch
2017-05-08 00:02:53,618 : INFO : 82
2017-05-08 00:02:53,619 : INFO : train percentage
2017-05-08 00:02:53,619 : INFO : 0.99710982659
2017-05-08 00:02:53,619 : INFO : Epoch
2017-05-08 00:02:53,619 : INFO : 82
2017-05-08 00:02:53,619 : INFO : dev percentage
2017-05-08 00:02:53,619 : INFO : 0.755733944954
2017-05-08 00:02:53,619 : INFO : Epoch
2017-05-08 00:02:53,619 : INFO : 82
2017-05-08 00:02:53,619 : INFO : test percentage
2017-05-08 00:02:53,620 : INFO : 0.775398132894
2017-05-08 00:21:39,589 : INFO : ==> Train loss   : 0.016372
2017-05-08 00:21:39,589 : INFO : Epoch
2017-05-08 00:21:39,589 : INFO : 83
2017-05-08 00:21:39,589 : INFO : train percentage
2017-05-08 00:21:39,589 : INFO : 0.99725433526
2017-05-08 00:21:39,589 : INFO : Epoch
2017-05-08 00:21:39,590 : INFO : 83
2017-05-08 00:21:39,590 : INFO : dev percentage
2017-05-08 00:21:39,590 : INFO : 0.751146788991
2017-05-08 00:21:39,590 : INFO : Epoch
2017-05-08 00:21:39,590 : INFO : 83
2017-05-08 00:21:39,590 : INFO : test percentage
2017-05-08 00:21:39,590 : INFO : 0.774848984075
2017-05-08 00:40:47,929 : INFO : ==> Train loss   : 0.016820
2017-05-08 00:40:47,929 : INFO : Epoch
2017-05-08 00:40:47,930 : INFO : 84
2017-05-08 00:40:47,930 : INFO : train percentage
2017-05-08 00:40:47,930 : INFO : 0.99710982659
2017-05-08 00:40:47,930 : INFO : Epoch
2017-05-08 00:40:47,930 : INFO : 84
2017-05-08 00:40:47,930 : INFO : dev percentage
2017-05-08 00:40:47,930 : INFO : 0.748853211009
2017-05-08 00:40:47,930 : INFO : Epoch
2017-05-08 00:40:47,931 : INFO : 84
2017-05-08 00:40:47,931 : INFO : test percentage
2017-05-08 00:40:47,931 : INFO : 0.774299835255
2017-05-08 00:43:45,953 : INFO : LOG_FILE
2017-05-08 00:43:45,953 : INFO : _________________________________start___________________________________
2017-05-08 00:43:45,960 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 00:43:46,075 : INFO : ==> SST vocabulary size : 21705
2017-05-08 00:43:52,044 : INFO : _param count_
2017-05-08 00:43:52,044 : INFO : torch.Size([168, 300])
2017-05-08 00:43:52,044 : INFO : torch.Size([168])
2017-05-08 00:43:52,044 : INFO : torch.Size([168, 168])
2017-05-08 00:43:52,045 : INFO : torch.Size([168])
2017-05-08 00:43:52,045 : INFO : torch.Size([168, 300])
2017-05-08 00:43:52,045 : INFO : torch.Size([168])
2017-05-08 00:43:52,045 : INFO : torch.Size([168, 168])
2017-05-08 00:43:52,045 : INFO : torch.Size([168])
2017-05-08 00:43:52,045 : INFO : torch.Size([168, 300])
2017-05-08 00:43:52,045 : INFO : torch.Size([168])
2017-05-08 00:43:52,046 : INFO : torch.Size([168, 168])
2017-05-08 00:43:52,046 : INFO : torch.Size([168])
2017-05-08 00:43:52,046 : INFO : torch.Size([168, 300])
2017-05-08 00:43:52,046 : INFO : torch.Size([168])
2017-05-08 00:43:52,046 : INFO : torch.Size([168, 168])
2017-05-08 00:43:52,046 : INFO : torch.Size([168])
2017-05-08 00:43:52,046 : INFO : torch.Size([3, 168])
2017-05-08 00:43:52,047 : INFO : torch.Size([3])
2017-05-08 00:43:52,047 : INFO : sum
2017-05-08 00:43:52,047 : INFO : 316347
2017-05-08 00:43:52,047 : INFO : ____________
2017-05-08 00:44:21,964 : INFO : LOG_FILE
2017-05-08 00:44:21,964 : INFO : _________________________________start___________________________________
2017-05-08 00:44:21,971 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 00:44:22,087 : INFO : ==> SST vocabulary size : 21705
2017-05-08 00:44:28,189 : INFO : _param count_
2017-05-08 00:44:28,189 : INFO : torch.Size([168, 300])
2017-05-08 00:44:28,189 : INFO : torch.Size([168])
2017-05-08 00:44:28,189 : INFO : torch.Size([168, 168])
2017-05-08 00:44:28,189 : INFO : torch.Size([168])
2017-05-08 00:44:28,190 : INFO : torch.Size([168, 300])
2017-05-08 00:44:28,190 : INFO : torch.Size([168])
2017-05-08 00:44:28,190 : INFO : torch.Size([168, 168])
2017-05-08 00:44:28,190 : INFO : torch.Size([168])
2017-05-08 00:44:28,190 : INFO : torch.Size([168, 300])
2017-05-08 00:44:28,190 : INFO : torch.Size([168])
2017-05-08 00:44:28,190 : INFO : torch.Size([168, 168])
2017-05-08 00:44:28,191 : INFO : torch.Size([168])
2017-05-08 00:44:28,191 : INFO : torch.Size([168, 300])
2017-05-08 00:44:28,191 : INFO : torch.Size([168])
2017-05-08 00:44:28,191 : INFO : torch.Size([168, 168])
2017-05-08 00:44:28,191 : INFO : torch.Size([168])
2017-05-08 00:44:28,191 : INFO : torch.Size([3, 168])
2017-05-08 00:44:28,191 : INFO : torch.Size([3])
2017-05-08 00:44:28,192 : INFO : sum
2017-05-08 00:44:28,192 : INFO : 316347
2017-05-08 00:44:28,192 : INFO : ____________
2017-05-08 01:04:43,461 : INFO : ==> Train loss   : 0.436014
2017-05-08 01:04:43,462 : INFO : Epoch
2017-05-08 01:04:43,462 : INFO : 0
2017-05-08 01:04:43,462 : INFO : train percentage
2017-05-08 01:04:43,462 : INFO : 0.851300578035
2017-05-08 01:04:43,462 : INFO : Epoch
2017-05-08 01:04:43,462 : INFO : 0
2017-05-08 01:04:43,462 : INFO : dev percentage
2017-05-08 01:04:43,462 : INFO : 0.809633027523
2017-05-08 01:04:43,463 : INFO : Epoch
2017-05-08 01:04:43,463 : INFO : 0
2017-05-08 01:04:43,463 : INFO : test percentage
2017-05-08 01:04:43,463 : INFO : 0.818780889621
2017-05-08 01:25:31,803 : INFO : ==> Train loss   : 0.255467
2017-05-08 01:25:31,803 : INFO : Epoch
2017-05-08 01:25:31,803 : INFO : 1
2017-05-08 01:25:31,803 : INFO : train percentage
2017-05-08 01:25:31,803 : INFO : 0.906069364162
2017-05-08 01:25:31,804 : INFO : Epoch
2017-05-08 01:25:31,804 : INFO : 1
2017-05-08 01:25:31,804 : INFO : dev percentage
2017-05-08 01:25:31,804 : INFO : 0.81995412844
2017-05-08 01:25:31,804 : INFO : Epoch
2017-05-08 01:25:31,804 : INFO : 1
2017-05-08 01:25:31,804 : INFO : test percentage
2017-05-08 01:25:31,804 : INFO : 0.84404173531
2017-05-08 01:47:02,183 : INFO : ==> Train loss   : 0.214070
2017-05-08 01:47:02,183 : INFO : Epoch
2017-05-08 01:47:02,183 : INFO : 2
2017-05-08 01:47:02,183 : INFO : train percentage
2017-05-08 01:47:02,183 : INFO : 0.932803468208
2017-05-08 01:47:02,184 : INFO : Epoch
2017-05-08 01:47:02,184 : INFO : 2
2017-05-08 01:47:02,184 : INFO : dev percentage
2017-05-08 01:47:02,184 : INFO : 0.817660550459
2017-05-08 01:47:02,184 : INFO : Epoch
2017-05-08 01:47:02,184 : INFO : 2
2017-05-08 01:47:02,184 : INFO : test percentage
2017-05-08 01:47:02,185 : INFO : 0.82756727073
2017-05-08 02:25:15,314 : INFO : ==> Train loss   : 0.138695
2017-05-08 02:25:16,043 : INFO : Epoch
2017-05-08 02:25:16,044 : INFO : 3
2017-05-08 02:25:16,044 : INFO : train percentage
2017-05-08 02:25:16,044 : INFO : 0.96112716763
2017-05-08 02:25:16,044 : INFO : Epoch
2017-05-08 02:25:16,044 : INFO : 3
2017-05-08 02:25:16,044 : INFO : dev percentage
2017-05-08 02:25:16,044 : INFO : 0.815366972477
2017-05-08 02:25:16,044 : INFO : Epoch
2017-05-08 02:25:16,044 : INFO : 3
2017-05-08 02:25:16,045 : INFO : test percentage
2017-05-08 02:25:16,045 : INFO : 0.83580450302
2017-05-08 02:49:27,754 : INFO : ==> Train loss   : 0.095291
2017-05-08 02:49:27,819 : INFO : Epoch
2017-05-08 02:49:27,819 : INFO : 4
2017-05-08 02:49:27,819 : INFO : train percentage
2017-05-08 02:49:27,819 : INFO : 0.976300578035
2017-05-08 02:49:27,819 : INFO : Epoch
2017-05-08 02:49:27,819 : INFO : 4
2017-05-08 02:49:27,819 : INFO : dev percentage
2017-05-08 02:49:27,819 : INFO : 0.810779816514
2017-05-08 02:49:27,820 : INFO : Epoch
2017-05-08 02:49:27,820 : INFO : 4
2017-05-08 02:49:27,820 : INFO : test percentage
2017-05-08 02:49:27,820 : INFO : 0.823174080176
2017-05-08 03:08:30,316 : INFO : ==> Train loss   : 0.073368
2017-05-08 03:08:30,318 : INFO : Epoch
2017-05-08 03:08:30,318 : INFO : 5
2017-05-08 03:08:30,318 : INFO : train percentage
2017-05-08 03:08:30,318 : INFO : 0.980057803468
2017-05-08 03:08:30,318 : INFO : Epoch
2017-05-08 03:08:30,319 : INFO : 5
2017-05-08 03:08:30,319 : INFO : dev percentage
2017-05-08 03:08:30,319 : INFO : 0.797018348624
2017-05-08 03:08:30,319 : INFO : Epoch
2017-05-08 03:08:30,319 : INFO : 5
2017-05-08 03:08:30,319 : INFO : test percentage
2017-05-08 03:08:30,319 : INFO : 0.823723228995
2017-05-08 03:27:07,059 : INFO : ==> Train loss   : 0.058428
2017-05-08 03:27:07,062 : INFO : Epoch
2017-05-08 03:27:07,062 : INFO : 6
2017-05-08 03:27:07,062 : INFO : train percentage
2017-05-08 03:27:07,063 : INFO : 0.984393063584
2017-05-08 03:27:07,063 : INFO : Epoch
2017-05-08 03:27:07,063 : INFO : 6
2017-05-08 03:27:07,063 : INFO : dev percentage
2017-05-08 03:27:07,063 : INFO : 0.794724770642
2017-05-08 03:27:07,063 : INFO : Epoch
2017-05-08 03:27:07,063 : INFO : 6
2017-05-08 03:27:07,063 : INFO : test percentage
2017-05-08 03:27:07,063 : INFO : 0.814387699066
2017-05-08 03:46:48,214 : INFO : ==> Train loss   : 0.036682
2017-05-08 03:46:48,218 : INFO : Epoch
2017-05-08 03:46:48,218 : INFO : 7
2017-05-08 03:46:48,219 : INFO : train percentage
2017-05-08 03:46:48,219 : INFO : 0.991907514451
2017-05-08 03:46:48,219 : INFO : Epoch
2017-05-08 03:46:48,219 : INFO : 7
2017-05-08 03:46:48,219 : INFO : dev percentage
2017-05-08 03:46:48,219 : INFO : 0.794724770642
2017-05-08 03:46:48,219 : INFO : Epoch
2017-05-08 03:46:48,219 : INFO : 7
2017-05-08 03:46:48,220 : INFO : test percentage
2017-05-08 03:46:48,220 : INFO : 0.818231740802
2017-05-08 04:05:19,217 : INFO : ==> Train loss   : 0.034313
2017-05-08 04:05:19,219 : INFO : Epoch
2017-05-08 04:05:19,219 : INFO : 8
2017-05-08 04:05:19,219 : INFO : train percentage
2017-05-08 04:05:19,219 : INFO : 0.990751445087
2017-05-08 04:05:19,220 : INFO : Epoch
2017-05-08 04:05:19,220 : INFO : 8
2017-05-08 04:05:19,220 : INFO : dev percentage
2017-05-08 04:05:19,220 : INFO : 0.793577981651
2017-05-08 04:05:19,220 : INFO : Epoch
2017-05-08 04:05:19,220 : INFO : 8
2017-05-08 04:05:19,220 : INFO : test percentage
2017-05-08 04:05:19,220 : INFO : 0.805601317957
2017-05-08 04:24:12,377 : INFO : ==> Train loss   : 0.025835
2017-05-08 04:24:12,592 : INFO : Epoch
2017-05-08 04:24:12,592 : INFO : 9
2017-05-08 04:24:12,592 : INFO : train percentage
2017-05-08 04:24:12,593 : INFO : 0.994653179191
2017-05-08 04:24:12,593 : INFO : Epoch
2017-05-08 04:24:12,593 : INFO : 9
2017-05-08 04:24:12,593 : INFO : dev percentage
2017-05-08 04:24:12,593 : INFO : 0.79128440367
2017-05-08 04:24:12,593 : INFO : Epoch
2017-05-08 04:24:12,593 : INFO : 9
2017-05-08 04:24:12,594 : INFO : test percentage
2017-05-08 04:24:12,594 : INFO : 0.814387699066
2017-05-08 04:43:21,756 : INFO : ==> Train loss   : 0.020609
2017-05-08 04:43:21,756 : INFO : Epoch
2017-05-08 04:43:21,756 : INFO : 10
2017-05-08 04:43:21,756 : INFO : train percentage
2017-05-08 04:43:21,757 : INFO : 0.99710982659
2017-05-08 04:43:21,757 : INFO : Epoch
2017-05-08 04:43:21,757 : INFO : 10
2017-05-08 04:43:21,757 : INFO : dev percentage
2017-05-08 04:43:21,757 : INFO : 0.795871559633
2017-05-08 04:43:21,757 : INFO : Epoch
2017-05-08 04:43:21,757 : INFO : 10
2017-05-08 04:43:21,757 : INFO : test percentage
2017-05-08 04:43:21,757 : INFO : 0.804503020319
2017-05-08 05:01:00,059 : INFO : ==> Train loss   : 0.011988
2017-05-08 05:01:00,060 : INFO : Epoch
2017-05-08 05:01:00,060 : INFO : 11
2017-05-08 05:01:00,060 : INFO : train percentage
2017-05-08 05:01:00,060 : INFO : 0.997543352601
2017-05-08 05:01:00,060 : INFO : Epoch
2017-05-08 05:01:00,060 : INFO : 11
2017-05-08 05:01:00,061 : INFO : dev percentage
2017-05-08 05:01:00,061 : INFO : 0.793577981651
2017-05-08 05:01:00,061 : INFO : Epoch
2017-05-08 05:01:00,061 : INFO : 11
2017-05-08 05:01:00,061 : INFO : test percentage
2017-05-08 05:01:00,061 : INFO : 0.813289401428
2017-05-08 05:18:40,947 : INFO : ==> Train loss   : 0.010500
2017-05-08 05:18:40,947 : INFO : Epoch
2017-05-08 05:18:40,948 : INFO : 12
2017-05-08 05:18:40,948 : INFO : train percentage
2017-05-08 05:18:40,948 : INFO : 0.997976878613
2017-05-08 05:18:40,948 : INFO : Epoch
2017-05-08 05:18:40,948 : INFO : 12
2017-05-08 05:18:40,948 : INFO : dev percentage
2017-05-08 05:18:40,948 : INFO : 0.800458715596
2017-05-08 05:18:40,948 : INFO : Epoch
2017-05-08 05:18:40,949 : INFO : 12
2017-05-08 05:18:40,949 : INFO : test percentage
2017-05-08 05:18:40,949 : INFO : 0.806699615596
2017-05-08 05:36:20,869 : INFO : ==> Train loss   : 0.011214
2017-05-08 05:36:20,869 : INFO : Epoch
2017-05-08 05:36:20,869 : INFO : 13
2017-05-08 05:36:20,869 : INFO : train percentage
2017-05-08 05:36:20,869 : INFO : 0.997543352601
2017-05-08 05:36:20,869 : INFO : Epoch
2017-05-08 05:36:20,870 : INFO : 13
2017-05-08 05:36:20,870 : INFO : dev percentage
2017-05-08 05:36:20,870 : INFO : 0.79128440367
2017-05-08 05:36:20,870 : INFO : Epoch
2017-05-08 05:36:20,870 : INFO : 13
2017-05-08 05:36:20,870 : INFO : test percentage
2017-05-08 05:36:20,870 : INFO : 0.81109280615
2017-05-08 05:53:58,961 : INFO : ==> Train loss   : 0.010016
2017-05-08 05:53:58,961 : INFO : Epoch
2017-05-08 05:53:58,961 : INFO : 14
2017-05-08 05:53:58,961 : INFO : train percentage
2017-05-08 05:53:58,961 : INFO : 0.997976878613
2017-05-08 05:53:58,961 : INFO : Epoch
2017-05-08 05:53:58,961 : INFO : 14
2017-05-08 05:53:58,961 : INFO : dev percentage
2017-05-08 05:53:58,962 : INFO : 0.780963302752
2017-05-08 05:53:58,962 : INFO : Epoch
2017-05-08 05:53:58,962 : INFO : 14
2017-05-08 05:53:58,962 : INFO : test percentage
2017-05-08 05:53:58,962 : INFO : 0.808347062054
2017-05-08 06:11:36,605 : INFO : ==> Train loss   : 0.006614
2017-05-08 06:11:36,605 : INFO : Epoch
2017-05-08 06:11:36,605 : INFO : 15
2017-05-08 06:11:36,605 : INFO : train percentage
2017-05-08 06:11:36,605 : INFO : 0.999132947977
2017-05-08 06:11:36,605 : INFO : Epoch
2017-05-08 06:11:36,605 : INFO : 15
2017-05-08 06:11:36,606 : INFO : dev percentage
2017-05-08 06:11:36,606 : INFO : 0.793577981651
2017-05-08 06:11:36,606 : INFO : Epoch
2017-05-08 06:11:36,606 : INFO : 15
2017-05-08 06:11:36,606 : INFO : test percentage
2017-05-08 06:11:36,606 : INFO : 0.807248764415
2017-05-08 06:29:16,887 : INFO : ==> Train loss   : 0.006354
2017-05-08 06:29:16,887 : INFO : Epoch
2017-05-08 06:29:16,887 : INFO : 16
2017-05-08 06:29:16,887 : INFO : train percentage
2017-05-08 06:29:16,887 : INFO : 0.998554913295
2017-05-08 06:29:16,888 : INFO : Epoch
2017-05-08 06:29:16,888 : INFO : 16
2017-05-08 06:29:16,888 : INFO : dev percentage
2017-05-08 06:29:16,888 : INFO : 0.787844036697
2017-05-08 06:29:16,888 : INFO : Epoch
2017-05-08 06:29:16,888 : INFO : 16
2017-05-08 06:29:16,888 : INFO : test percentage
2017-05-08 06:29:16,888 : INFO : 0.805052169138
2017-05-08 06:46:56,775 : INFO : ==> Train loss   : 0.003761
2017-05-08 06:46:56,775 : INFO : Epoch
2017-05-08 06:46:56,775 : INFO : 17
2017-05-08 06:46:56,775 : INFO : train percentage
2017-05-08 06:46:56,775 : INFO : 0.999710982659
2017-05-08 06:46:56,775 : INFO : Epoch
2017-05-08 06:46:56,775 : INFO : 17
2017-05-08 06:46:56,776 : INFO : dev percentage
2017-05-08 06:46:56,776 : INFO : 0.792431192661
2017-05-08 06:46:56,776 : INFO : Epoch
2017-05-08 06:46:56,776 : INFO : 17
2017-05-08 06:46:56,776 : INFO : test percentage
2017-05-08 06:46:56,776 : INFO : 0.807248764415
2017-05-08 07:04:35,188 : INFO : ==> Train loss   : 0.004071
2017-05-08 07:04:35,188 : INFO : Epoch
2017-05-08 07:04:35,189 : INFO : 18
2017-05-08 07:04:35,189 : INFO : train percentage
2017-05-08 07:04:35,189 : INFO : 0.999566473988
2017-05-08 07:04:35,189 : INFO : Epoch
2017-05-08 07:04:35,189 : INFO : 18
2017-05-08 07:04:35,189 : INFO : dev percentage
2017-05-08 07:04:35,189 : INFO : 0.794724770642
2017-05-08 07:04:35,189 : INFO : Epoch
2017-05-08 07:04:35,190 : INFO : 18
2017-05-08 07:04:35,190 : INFO : test percentage
2017-05-08 07:04:35,190 : INFO : 0.809994508512
2017-05-08 07:22:13,099 : INFO : ==> Train loss   : 0.002665
2017-05-08 07:22:13,099 : INFO : Epoch
2017-05-08 07:22:13,099 : INFO : 19
2017-05-08 07:22:13,100 : INFO : train percentage
2017-05-08 07:22:13,100 : INFO : 0.999710982659
2017-05-08 07:22:13,100 : INFO : Epoch
2017-05-08 07:22:13,101 : INFO : 19
2017-05-08 07:22:13,101 : INFO : dev percentage
2017-05-08 07:22:13,101 : INFO : 0.799311926606
2017-05-08 07:22:13,101 : INFO : Epoch
2017-05-08 07:22:13,101 : INFO : 19
2017-05-08 07:22:13,102 : INFO : test percentage
2017-05-08 07:22:13,102 : INFO : 0.808347062054
2017-05-08 07:39:52,750 : INFO : ==> Train loss   : 0.003521
2017-05-08 07:39:52,751 : INFO : Epoch
2017-05-08 07:39:52,751 : INFO : 20
2017-05-08 07:39:52,751 : INFO : train percentage
2017-05-08 07:39:52,751 : INFO : 0.999277456647
2017-05-08 07:39:52,751 : INFO : Epoch
2017-05-08 07:39:52,751 : INFO : 20
2017-05-08 07:39:52,751 : INFO : dev percentage
2017-05-08 07:39:52,751 : INFO : 0.79128440367
2017-05-08 07:39:52,752 : INFO : Epoch
2017-05-08 07:39:52,752 : INFO : 20
2017-05-08 07:39:52,752 : INFO : test percentage
2017-05-08 07:39:52,752 : INFO : 0.807248764415
2017-05-08 07:57:20,196 : INFO : ==> Train loss   : 0.002570
2017-05-08 07:57:20,196 : INFO : Epoch
2017-05-08 07:57:20,197 : INFO : 21
2017-05-08 07:57:20,197 : INFO : train percentage
2017-05-08 07:57:20,197 : INFO : 0.999566473988
2017-05-08 07:57:20,197 : INFO : Epoch
2017-05-08 07:57:20,197 : INFO : 21
2017-05-08 07:57:20,197 : INFO : dev percentage
2017-05-08 07:57:20,197 : INFO : 0.790137614679
2017-05-08 07:57:20,197 : INFO : Epoch
2017-05-08 07:57:20,198 : INFO : 21
2017-05-08 07:57:20,198 : INFO : test percentage
2017-05-08 07:57:20,198 : INFO : 0.80340472268
2017-05-08 08:14:46,165 : INFO : ==> Train loss   : 0.003498
2017-05-08 08:14:46,166 : INFO : Epoch
2017-05-08 08:14:46,166 : INFO : 22
2017-05-08 08:14:46,166 : INFO : train percentage
2017-05-08 08:14:46,166 : INFO : 0.999277456647
2017-05-08 08:14:46,166 : INFO : Epoch
2017-05-08 08:14:46,166 : INFO : 22
2017-05-08 08:14:46,166 : INFO : dev percentage
2017-05-08 08:14:46,167 : INFO : 0.792431192661
2017-05-08 08:14:46,167 : INFO : Epoch
2017-05-08 08:14:46,167 : INFO : 22
2017-05-08 08:14:46,167 : INFO : test percentage
2017-05-08 08:14:46,167 : INFO : 0.803953871499
2017-05-08 08:32:14,004 : INFO : ==> Train loss   : 0.001379
2017-05-08 08:32:14,004 : INFO : Epoch
2017-05-08 08:32:14,004 : INFO : 23
2017-05-08 08:32:14,004 : INFO : train percentage
2017-05-08 08:32:14,004 : INFO : 1.0
2017-05-08 08:32:14,004 : INFO : Epoch
2017-05-08 08:32:14,005 : INFO : 23
2017-05-08 08:32:14,005 : INFO : dev percentage
2017-05-08 08:32:14,005 : INFO : 0.795871559633
2017-05-08 08:32:14,005 : INFO : Epoch
2017-05-08 08:32:14,005 : INFO : 23
2017-05-08 08:32:14,005 : INFO : test percentage
2017-05-08 08:32:14,005 : INFO : 0.798462383306
2017-05-08 08:49:36,685 : INFO : ==> Train loss   : 0.003965
2017-05-08 08:49:36,686 : INFO : Epoch
2017-05-08 08:49:36,686 : INFO : 24
2017-05-08 08:49:36,686 : INFO : train percentage
2017-05-08 08:49:36,686 : INFO : 0.999566473988
2017-05-08 08:49:36,686 : INFO : Epoch
2017-05-08 08:49:36,686 : INFO : 24
2017-05-08 08:49:36,686 : INFO : dev percentage
2017-05-08 08:49:36,686 : INFO : 0.793577981651
2017-05-08 08:49:36,687 : INFO : Epoch
2017-05-08 08:49:36,687 : INFO : 24
2017-05-08 08:49:36,687 : INFO : test percentage
2017-05-08 08:49:36,687 : INFO : 0.796814936848
2017-05-08 09:06:51,831 : INFO : ==> Train loss   : 0.002367
2017-05-08 09:06:51,831 : INFO : Epoch
2017-05-08 09:06:51,831 : INFO : 25
2017-05-08 09:06:51,832 : INFO : train percentage
2017-05-08 09:06:51,832 : INFO : 0.999710982659
2017-05-08 09:06:51,832 : INFO : Epoch
2017-05-08 09:06:51,832 : INFO : 25
2017-05-08 09:06:51,832 : INFO : dev percentage
2017-05-08 09:06:51,832 : INFO : 0.792431192661
2017-05-08 09:06:51,832 : INFO : Epoch
2017-05-08 09:06:51,832 : INFO : 25
2017-05-08 09:06:51,833 : INFO : test percentage
2017-05-08 09:06:51,833 : INFO : 0.796814936848
2017-05-08 09:24:13,056 : INFO : ==> Train loss   : 0.001457
2017-05-08 09:24:13,057 : INFO : Epoch
2017-05-08 09:24:13,057 : INFO : 26
2017-05-08 09:24:13,057 : INFO : train percentage
2017-05-08 09:24:13,057 : INFO : 0.999710982659
2017-05-08 09:24:13,057 : INFO : Epoch
2017-05-08 09:24:13,057 : INFO : 26
2017-05-08 09:24:13,058 : INFO : dev percentage
2017-05-08 09:24:13,058 : INFO : 0.788990825688
2017-05-08 09:24:13,058 : INFO : Epoch
2017-05-08 09:24:13,058 : INFO : 26
2017-05-08 09:24:13,058 : INFO : test percentage
2017-05-08 09:24:13,058 : INFO : 0.801757276222
2017-05-08 09:41:34,436 : INFO : ==> Train loss   : 0.001550
2017-05-08 09:41:34,436 : INFO : Epoch
2017-05-08 09:41:34,436 : INFO : 27
2017-05-08 09:41:34,436 : INFO : train percentage
2017-05-08 09:41:34,436 : INFO : 0.999855491329
2017-05-08 09:41:34,437 : INFO : Epoch
2017-05-08 09:41:34,437 : INFO : 27
2017-05-08 09:41:34,437 : INFO : dev percentage
2017-05-08 09:41:34,437 : INFO : 0.785550458716
2017-05-08 09:41:34,437 : INFO : Epoch
2017-05-08 09:41:34,437 : INFO : 27
2017-05-08 09:41:34,437 : INFO : test percentage
2017-05-08 09:41:34,437 : INFO : 0.802855573861
2017-05-08 09:59:04,437 : INFO : ==> Train loss   : 0.001631
2017-05-08 09:59:04,437 : INFO : Epoch
2017-05-08 09:59:04,437 : INFO : 28
2017-05-08 09:59:04,437 : INFO : train percentage
2017-05-08 09:59:04,437 : INFO : 0.999710982659
2017-05-08 09:59:04,437 : INFO : Epoch
2017-05-08 09:59:04,437 : INFO : 28
2017-05-08 09:59:04,438 : INFO : dev percentage
2017-05-08 09:59:04,438 : INFO : 0.786697247706
2017-05-08 09:59:04,438 : INFO : Epoch
2017-05-08 09:59:04,438 : INFO : 28
2017-05-08 09:59:04,438 : INFO : test percentage
2017-05-08 09:59:04,438 : INFO : 0.800658978583
2017-05-08 10:16:32,952 : INFO : ==> Train loss   : 0.003040
2017-05-08 10:16:32,952 : INFO : Epoch
2017-05-08 10:16:32,952 : INFO : 29
2017-05-08 10:16:32,952 : INFO : train percentage
2017-05-08 10:16:32,952 : INFO : 0.999855491329
2017-05-08 10:16:32,952 : INFO : Epoch
2017-05-08 10:16:32,952 : INFO : 29
2017-05-08 10:16:32,952 : INFO : dev percentage
2017-05-08 10:16:32,953 : INFO : 0.784403669725
2017-05-08 10:16:32,953 : INFO : Epoch
2017-05-08 10:16:32,953 : INFO : 29
2017-05-08 10:16:32,953 : INFO : test percentage
2017-05-08 10:16:32,953 : INFO : 0.806699615596
2017-05-08 10:33:57,706 : INFO : ==> Train loss   : 0.001160
2017-05-08 10:33:57,706 : INFO : Epoch
2017-05-08 10:33:57,706 : INFO : 30
2017-05-08 10:33:57,706 : INFO : train percentage
2017-05-08 10:33:57,707 : INFO : 0.999855491329
2017-05-08 10:33:57,707 : INFO : Epoch
2017-05-08 10:33:57,707 : INFO : 30
2017-05-08 10:33:57,707 : INFO : dev percentage
2017-05-08 10:33:57,707 : INFO : 0.793577981651
2017-05-08 10:33:57,707 : INFO : Epoch
2017-05-08 10:33:57,707 : INFO : 30
2017-05-08 10:33:57,707 : INFO : test percentage
2017-05-08 10:33:57,708 : INFO : 0.802306425041
2017-05-08 10:51:18,252 : INFO : ==> Train loss   : 0.002327
2017-05-08 10:51:18,252 : INFO : Epoch
2017-05-08 10:51:18,252 : INFO : 31
2017-05-08 10:51:18,252 : INFO : train percentage
2017-05-08 10:51:18,253 : INFO : 1.0
2017-05-08 10:51:18,253 : INFO : Epoch
2017-05-08 10:51:18,253 : INFO : 31
2017-05-08 10:51:18,253 : INFO : dev percentage
2017-05-08 10:51:18,253 : INFO : 0.786697247706
2017-05-08 10:51:18,253 : INFO : Epoch
2017-05-08 10:51:18,253 : INFO : 31
2017-05-08 10:51:18,253 : INFO : test percentage
2017-05-08 10:51:18,254 : INFO : 0.805601317957
2017-05-08 11:08:48,212 : INFO : ==> Train loss   : 0.003027
2017-05-08 11:08:48,213 : INFO : Epoch
2017-05-08 11:08:48,213 : INFO : 32
2017-05-08 11:08:48,213 : INFO : train percentage
2017-05-08 11:08:48,213 : INFO : 0.999421965318
2017-05-08 11:08:48,213 : INFO : Epoch
2017-05-08 11:08:48,213 : INFO : 32
2017-05-08 11:08:48,213 : INFO : dev percentage
2017-05-08 11:08:48,213 : INFO : 0.786697247706
2017-05-08 11:08:48,213 : INFO : Epoch
2017-05-08 11:08:48,214 : INFO : 32
2017-05-08 11:08:48,214 : INFO : test percentage
2017-05-08 11:08:48,214 : INFO : 0.798462383306
2017-05-08 11:26:15,911 : INFO : ==> Train loss   : 0.001711
2017-05-08 11:26:15,911 : INFO : Epoch
2017-05-08 11:26:15,912 : INFO : 33
2017-05-08 11:26:15,912 : INFO : train percentage
2017-05-08 11:26:15,912 : INFO : 0.999855491329
2017-05-08 11:26:15,912 : INFO : Epoch
2017-05-08 11:26:15,912 : INFO : 33
2017-05-08 11:26:15,912 : INFO : dev percentage
2017-05-08 11:26:15,912 : INFO : 0.79128440367
2017-05-08 11:26:15,912 : INFO : Epoch
2017-05-08 11:26:15,913 : INFO : 33
2017-05-08 11:26:15,913 : INFO : test percentage
2017-05-08 11:26:15,913 : INFO : 0.798462383306
2017-05-08 11:43:33,326 : INFO : LOG_FILE
2017-05-08 11:43:33,327 : INFO : _________________________________start___________________________________
2017-05-08 11:43:33,333 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 11:43:33,437 : INFO : ==> SST vocabulary size : 21705
2017-05-08 11:43:39,101 : INFO : _param count_
2017-05-08 11:43:39,102 : INFO : torch.Size([168, 300])
2017-05-08 11:43:39,102 : INFO : torch.Size([168])
2017-05-08 11:43:39,102 : INFO : torch.Size([168, 168])
2017-05-08 11:43:39,102 : INFO : torch.Size([168])
2017-05-08 11:43:39,102 : INFO : torch.Size([168, 300])
2017-05-08 11:43:39,102 : INFO : torch.Size([168])
2017-05-08 11:43:39,102 : INFO : torch.Size([168, 168])
2017-05-08 11:43:39,103 : INFO : torch.Size([168])
2017-05-08 11:43:39,103 : INFO : torch.Size([168, 300])
2017-05-08 11:43:39,103 : INFO : torch.Size([168])
2017-05-08 11:43:39,103 : INFO : torch.Size([168, 168])
2017-05-08 11:43:39,103 : INFO : torch.Size([168])
2017-05-08 11:43:39,103 : INFO : torch.Size([168, 300])
2017-05-08 11:43:39,103 : INFO : torch.Size([168])
2017-05-08 11:43:39,103 : INFO : torch.Size([168, 168])
2017-05-08 11:43:39,104 : INFO : torch.Size([168])
2017-05-08 11:43:39,104 : INFO : torch.Size([3, 168])
2017-05-08 11:43:39,104 : INFO : torch.Size([3])
2017-05-08 11:43:39,104 : INFO : sum
2017-05-08 11:43:39,104 : INFO : 316347
2017-05-08 11:43:39,104 : INFO : ____________
2017-05-08 11:45:26,542 : INFO : LOG_FILE
2017-05-08 11:45:26,542 : INFO : _________________________________start___________________________________
2017-05-08 11:45:26,549 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 11:45:26,650 : INFO : ==> SST vocabulary size : 21705
2017-05-08 11:45:32,419 : INFO : _param count_
2017-05-08 11:45:32,419 : INFO : torch.Size([168, 300])
2017-05-08 11:45:32,419 : INFO : torch.Size([168])
2017-05-08 11:45:32,419 : INFO : torch.Size([168, 168])
2017-05-08 11:45:32,419 : INFO : torch.Size([168])
2017-05-08 11:45:32,419 : INFO : torch.Size([168, 300])
2017-05-08 11:45:32,420 : INFO : torch.Size([168])
2017-05-08 11:45:32,420 : INFO : torch.Size([168, 168])
2017-05-08 11:45:32,420 : INFO : torch.Size([168])
2017-05-08 11:45:32,420 : INFO : torch.Size([168, 300])
2017-05-08 11:45:32,420 : INFO : torch.Size([168])
2017-05-08 11:45:32,420 : INFO : torch.Size([168, 168])
2017-05-08 11:45:32,420 : INFO : torch.Size([168])
2017-05-08 11:45:32,421 : INFO : torch.Size([168, 300])
2017-05-08 11:45:32,421 : INFO : torch.Size([168])
2017-05-08 11:45:32,421 : INFO : torch.Size([168, 168])
2017-05-08 11:45:32,421 : INFO : torch.Size([168])
2017-05-08 11:45:32,421 : INFO : torch.Size([3, 168])
2017-05-08 11:45:32,421 : INFO : torch.Size([3])
2017-05-08 11:45:32,421 : INFO : sum
2017-05-08 11:45:32,422 : INFO : 316347
2017-05-08 11:45:32,422 : INFO : ____________
2017-05-08 12:02:55,061 : INFO : ==> Train loss   : 0.436014
2017-05-08 12:02:55,061 : INFO : Epoch
2017-05-08 12:02:55,061 : INFO : 0
2017-05-08 12:02:55,061 : INFO : train percentage
2017-05-08 12:02:55,061 : INFO : 0.851300578035
2017-05-08 12:02:55,062 : INFO : Epoch
2017-05-08 12:02:55,062 : INFO : 0
2017-05-08 12:02:55,062 : INFO : dev percentage
2017-05-08 12:02:55,062 : INFO : 0.809633027523
2017-05-08 12:02:55,062 : INFO : Epoch
2017-05-08 12:02:55,062 : INFO : 0
2017-05-08 12:02:55,062 : INFO : test percentage
2017-05-08 12:02:55,062 : INFO : 0.818780889621
2017-05-08 12:04:52,269 : INFO : ==> Train loss   : 0.436014
2017-05-08 12:04:52,269 : INFO : Epoch
2017-05-08 12:04:52,270 : INFO : 0
2017-05-08 12:04:52,270 : INFO : train percentage
2017-05-08 12:04:52,270 : INFO : 0.851300578035
2017-05-08 12:04:52,270 : INFO : Epoch
2017-05-08 12:04:52,270 : INFO : 0
2017-05-08 12:04:52,270 : INFO : dev percentage
2017-05-08 12:04:52,270 : INFO : 0.809633027523
2017-05-08 12:04:52,271 : INFO : Epoch
2017-05-08 12:04:52,271 : INFO : 0
2017-05-08 12:04:52,271 : INFO : test percentage
2017-05-08 12:04:52,271 : INFO : 0.818780889621
2017-05-08 12:06:01,221 : INFO : LOG_FILE
2017-05-08 12:06:01,222 : INFO : _________________________________start___________________________________
2017-05-08 12:06:01,234 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 12:06:01,546 : INFO : ==> SST vocabulary size : 21705
2017-05-08 12:06:07,615 : INFO : _param count_
2017-05-08 12:06:07,619 : INFO : torch.Size([168, 300])
2017-05-08 12:06:07,620 : INFO : torch.Size([168])
2017-05-08 12:06:07,621 : INFO : torch.Size([168, 168])
2017-05-08 12:06:07,622 : INFO : torch.Size([168])
2017-05-08 12:06:07,623 : INFO : torch.Size([168, 300])
2017-05-08 12:06:07,624 : INFO : torch.Size([168])
2017-05-08 12:06:07,625 : INFO : torch.Size([168, 168])
2017-05-08 12:06:07,626 : INFO : torch.Size([168])
2017-05-08 12:06:07,627 : INFO : torch.Size([168, 300])
2017-05-08 12:06:07,628 : INFO : torch.Size([168])
2017-05-08 12:06:07,628 : INFO : torch.Size([168, 168])
2017-05-08 12:06:07,629 : INFO : torch.Size([168])
2017-05-08 12:06:07,631 : INFO : torch.Size([168, 300])
2017-05-08 12:06:07,631 : INFO : torch.Size([168])
2017-05-08 12:06:07,632 : INFO : torch.Size([168, 168])
2017-05-08 12:06:07,633 : INFO : torch.Size([168])
2017-05-08 12:06:07,633 : INFO : torch.Size([3, 168])
2017-05-08 12:06:07,634 : INFO : torch.Size([3])
2017-05-08 12:06:07,634 : INFO : sum
2017-05-08 12:06:07,635 : INFO : 316347
2017-05-08 12:06:07,635 : INFO : ____________
2017-05-08 12:06:17,999 : INFO : LOG_FILE
2017-05-08 12:06:17,999 : INFO : _________________________________start___________________________________
2017-05-08 12:06:18,014 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 12:06:18,327 : INFO : ==> SST vocabulary size : 21705
2017-05-08 12:06:24,418 : INFO : _param count_
2017-05-08 12:06:24,420 : INFO : torch.Size([168, 300])
2017-05-08 12:06:24,421 : INFO : torch.Size([168])
2017-05-08 12:06:24,421 : INFO : torch.Size([168, 168])
2017-05-08 12:06:24,422 : INFO : torch.Size([168])
2017-05-08 12:06:24,423 : INFO : torch.Size([168, 300])
2017-05-08 12:06:24,423 : INFO : torch.Size([168])
2017-05-08 12:06:24,424 : INFO : torch.Size([168, 168])
2017-05-08 12:06:24,425 : INFO : torch.Size([168])
2017-05-08 12:06:24,425 : INFO : torch.Size([168, 300])
2017-05-08 12:06:24,426 : INFO : torch.Size([168])
2017-05-08 12:06:24,426 : INFO : torch.Size([168, 168])
2017-05-08 12:06:24,427 : INFO : torch.Size([168])
2017-05-08 12:06:24,427 : INFO : torch.Size([168, 300])
2017-05-08 12:06:24,428 : INFO : torch.Size([168])
2017-05-08 12:06:24,428 : INFO : torch.Size([168, 168])
2017-05-08 12:06:24,429 : INFO : torch.Size([168])
2017-05-08 12:06:24,429 : INFO : torch.Size([3, 168])
2017-05-08 12:06:24,430 : INFO : torch.Size([3])
2017-05-08 12:06:24,430 : INFO : sum
2017-05-08 12:06:24,431 : INFO : 316347
2017-05-08 12:06:24,432 : INFO : ____________
2017-05-08 12:06:47,658 : INFO : LOG_FILE
2017-05-08 12:06:47,659 : INFO : _________________________________start___________________________________
2017-05-08 12:06:47,672 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 12:06:47,967 : INFO : ==> SST vocabulary size : 21705
2017-05-08 12:06:54,038 : INFO : _param count_
2017-05-08 12:06:54,041 : INFO : torch.Size([168, 300])
2017-05-08 12:06:54,042 : INFO : torch.Size([168])
2017-05-08 12:06:54,042 : INFO : torch.Size([168, 168])
2017-05-08 12:06:54,043 : INFO : torch.Size([168])
2017-05-08 12:06:54,043 : INFO : torch.Size([168, 300])
2017-05-08 12:06:54,044 : INFO : torch.Size([168])
2017-05-08 12:06:54,045 : INFO : torch.Size([168, 168])
2017-05-08 12:06:54,045 : INFO : torch.Size([168])
2017-05-08 12:06:54,046 : INFO : torch.Size([168, 300])
2017-05-08 12:06:54,046 : INFO : torch.Size([168])
2017-05-08 12:06:54,047 : INFO : torch.Size([168, 168])
2017-05-08 12:06:54,047 : INFO : torch.Size([168])
2017-05-08 12:06:54,048 : INFO : torch.Size([168, 300])
2017-05-08 12:06:54,048 : INFO : torch.Size([168])
2017-05-08 12:06:54,049 : INFO : torch.Size([168, 168])
2017-05-08 12:06:54,049 : INFO : torch.Size([168])
2017-05-08 12:06:54,050 : INFO : torch.Size([3, 168])
2017-05-08 12:06:54,051 : INFO : torch.Size([3])
2017-05-08 12:06:54,051 : INFO : sum
2017-05-08 12:06:54,052 : INFO : 316347
2017-05-08 12:06:54,052 : INFO : ____________
2017-05-08 12:22:45,954 : INFO : ==> Train loss   : 0.255467
2017-05-08 12:22:45,955 : INFO : Epoch
2017-05-08 12:22:45,955 : INFO : 1
2017-05-08 12:22:45,955 : INFO : train percentage
2017-05-08 12:22:45,955 : INFO : 0.906069364162
2017-05-08 12:22:45,955 : INFO : Epoch
2017-05-08 12:22:45,955 : INFO : 1
2017-05-08 12:22:45,955 : INFO : dev percentage
2017-05-08 12:22:45,955 : INFO : 0.81995412844
2017-05-08 12:22:45,956 : INFO : Epoch
2017-05-08 12:22:45,956 : INFO : 1
2017-05-08 12:22:45,956 : INFO : test percentage
2017-05-08 12:22:45,956 : INFO : 0.84404173531
2017-05-08 12:40:19,491 : INFO : ==> Train loss   : 0.214070
2017-05-08 12:40:19,491 : INFO : Epoch
2017-05-08 12:40:19,491 : INFO : 2
2017-05-08 12:40:19,491 : INFO : train percentage
2017-05-08 12:40:19,491 : INFO : 0.932803468208
2017-05-08 12:40:19,491 : INFO : Epoch
2017-05-08 12:40:19,492 : INFO : 2
2017-05-08 12:40:19,492 : INFO : dev percentage
2017-05-08 12:40:19,492 : INFO : 0.817660550459
2017-05-08 12:40:19,492 : INFO : Epoch
2017-05-08 12:40:19,492 : INFO : 2
2017-05-08 12:40:19,492 : INFO : test percentage
2017-05-08 12:40:19,492 : INFO : 0.82756727073
2017-05-08 12:57:37,080 : INFO : ==> Train loss   : 0.138695
2017-05-08 12:57:37,080 : INFO : Epoch
2017-05-08 12:57:37,080 : INFO : 3
2017-05-08 12:57:37,080 : INFO : train percentage
2017-05-08 12:57:37,081 : INFO : 0.96112716763
2017-05-08 12:57:37,081 : INFO : Epoch
2017-05-08 12:57:37,081 : INFO : 3
2017-05-08 12:57:37,081 : INFO : dev percentage
2017-05-08 12:57:37,081 : INFO : 0.815366972477
2017-05-08 12:57:37,081 : INFO : Epoch
2017-05-08 12:57:37,081 : INFO : 3
2017-05-08 12:57:37,081 : INFO : test percentage
2017-05-08 12:57:37,082 : INFO : 0.83580450302
2017-05-08 13:14:48,914 : INFO : ==> Train loss   : 0.095291
2017-05-08 13:14:48,914 : INFO : Epoch
2017-05-08 13:14:48,914 : INFO : 4
2017-05-08 13:14:48,914 : INFO : train percentage
2017-05-08 13:14:48,914 : INFO : 0.976300578035
2017-05-08 13:14:48,914 : INFO : Epoch
2017-05-08 13:14:48,914 : INFO : 4
2017-05-08 13:14:48,915 : INFO : dev percentage
2017-05-08 13:14:48,915 : INFO : 0.810779816514
2017-05-08 13:14:48,915 : INFO : Epoch
2017-05-08 13:14:48,915 : INFO : 4
2017-05-08 13:14:48,915 : INFO : test percentage
2017-05-08 13:14:48,915 : INFO : 0.823174080176
2017-05-08 13:32:27,746 : INFO : ==> Train loss   : 0.073368
2017-05-08 13:32:27,746 : INFO : Epoch
2017-05-08 13:32:27,746 : INFO : 5
2017-05-08 13:32:27,746 : INFO : train percentage
2017-05-08 13:32:27,747 : INFO : 0.980057803468
2017-05-08 13:32:27,747 : INFO : Epoch
2017-05-08 13:32:27,747 : INFO : 5
2017-05-08 13:32:27,747 : INFO : dev percentage
2017-05-08 13:32:27,747 : INFO : 0.797018348624
2017-05-08 13:32:27,747 : INFO : Epoch
2017-05-08 13:32:27,747 : INFO : 5
2017-05-08 13:32:27,747 : INFO : test percentage
2017-05-08 13:32:27,747 : INFO : 0.823723228995
2017-05-08 13:50:05,511 : INFO : ==> Train loss   : 0.058428
2017-05-08 13:50:05,512 : INFO : Epoch
2017-05-08 13:50:05,512 : INFO : 6
2017-05-08 13:50:05,512 : INFO : train percentage
2017-05-08 13:50:05,512 : INFO : 0.984393063584
2017-05-08 13:50:05,513 : INFO : Epoch
2017-05-08 13:50:05,513 : INFO : 6
2017-05-08 13:50:05,513 : INFO : dev percentage
2017-05-08 13:50:05,513 : INFO : 0.794724770642
2017-05-08 13:50:05,513 : INFO : Epoch
2017-05-08 13:50:05,513 : INFO : 6
2017-05-08 13:50:05,513 : INFO : test percentage
2017-05-08 13:50:05,514 : INFO : 0.814387699066
2017-05-08 14:08:08,901 : INFO : ==> Train loss   : 0.036682
2017-05-08 14:08:08,901 : INFO : Epoch
2017-05-08 14:08:08,901 : INFO : 7
2017-05-08 14:08:08,901 : INFO : train percentage
2017-05-08 14:08:08,901 : INFO : 0.991907514451
2017-05-08 14:08:08,901 : INFO : Epoch
2017-05-08 14:08:08,902 : INFO : 7
2017-05-08 14:08:08,902 : INFO : dev percentage
2017-05-08 14:08:08,902 : INFO : 0.794724770642
2017-05-08 14:08:08,902 : INFO : Epoch
2017-05-08 14:08:08,902 : INFO : 7
2017-05-08 14:08:08,902 : INFO : test percentage
2017-05-08 14:08:08,902 : INFO : 0.818231740802
2017-05-08 14:25:49,876 : INFO : ==> Train loss   : 0.034313
2017-05-08 14:25:49,876 : INFO : Epoch
2017-05-08 14:25:49,876 : INFO : 8
2017-05-08 14:25:49,876 : INFO : train percentage
2017-05-08 14:25:49,876 : INFO : 0.990751445087
2017-05-08 14:25:49,876 : INFO : Epoch
2017-05-08 14:25:49,877 : INFO : 8
2017-05-08 14:25:49,877 : INFO : dev percentage
2017-05-08 14:25:49,877 : INFO : 0.793577981651
2017-05-08 14:25:49,877 : INFO : Epoch
2017-05-08 14:25:49,877 : INFO : 8
2017-05-08 14:25:49,877 : INFO : test percentage
2017-05-08 14:25:49,877 : INFO : 0.805601317957
2017-05-08 14:43:51,000 : INFO : ==> Train loss   : 0.025835
2017-05-08 14:43:51,001 : INFO : Epoch
2017-05-08 14:43:51,001 : INFO : 9
2017-05-08 14:43:51,001 : INFO : train percentage
2017-05-08 14:43:51,001 : INFO : 0.994653179191
2017-05-08 14:43:51,001 : INFO : Epoch
2017-05-08 14:43:51,001 : INFO : 9
2017-05-08 14:43:51,001 : INFO : dev percentage
2017-05-08 14:43:51,001 : INFO : 0.79128440367
2017-05-08 14:43:51,001 : INFO : Epoch
2017-05-08 14:43:51,002 : INFO : 9
2017-05-08 14:43:51,002 : INFO : test percentage
2017-05-08 14:43:51,002 : INFO : 0.814387699066
2017-05-08 15:01:42,152 : INFO : ==> Train loss   : 0.020609
2017-05-08 15:01:42,152 : INFO : Epoch
2017-05-08 15:01:42,152 : INFO : 10
2017-05-08 15:01:42,152 : INFO : train percentage
2017-05-08 15:01:42,153 : INFO : 0.99710982659
2017-05-08 15:01:42,153 : INFO : Epoch
2017-05-08 15:01:42,153 : INFO : 10
2017-05-08 15:01:42,153 : INFO : dev percentage
2017-05-08 15:01:42,153 : INFO : 0.795871559633
2017-05-08 15:01:42,153 : INFO : Epoch
2017-05-08 15:01:42,153 : INFO : 10
2017-05-08 15:01:42,154 : INFO : test percentage
2017-05-08 15:01:42,154 : INFO : 0.804503020319
2017-05-08 15:19:22,005 : INFO : ==> Train loss   : 0.011988
2017-05-08 15:19:22,005 : INFO : Epoch
2017-05-08 15:19:22,005 : INFO : 11
2017-05-08 15:19:22,006 : INFO : train percentage
2017-05-08 15:19:22,006 : INFO : 0.997543352601
2017-05-08 15:19:22,006 : INFO : Epoch
2017-05-08 15:19:22,006 : INFO : 11
2017-05-08 15:19:22,006 : INFO : dev percentage
2017-05-08 15:19:22,006 : INFO : 0.793577981651
2017-05-08 15:19:22,006 : INFO : Epoch
2017-05-08 15:19:22,006 : INFO : 11
2017-05-08 15:19:22,007 : INFO : test percentage
2017-05-08 15:19:22,007 : INFO : 0.813289401428
2017-05-08 15:37:37,892 : INFO : ==> Train loss   : 0.010500
2017-05-08 15:37:37,892 : INFO : Epoch
2017-05-08 15:37:37,892 : INFO : 12
2017-05-08 15:37:37,893 : INFO : train percentage
2017-05-08 15:37:37,893 : INFO : 0.997976878613
2017-05-08 15:37:37,893 : INFO : Epoch
2017-05-08 15:37:37,893 : INFO : 12
2017-05-08 15:37:37,893 : INFO : dev percentage
2017-05-08 15:37:37,893 : INFO : 0.800458715596
2017-05-08 15:37:37,893 : INFO : Epoch
2017-05-08 15:37:37,893 : INFO : 12
2017-05-08 15:37:37,894 : INFO : test percentage
2017-05-08 15:37:37,894 : INFO : 0.806699615596
2017-05-08 15:55:28,865 : INFO : ==> Train loss   : 0.011214
2017-05-08 15:55:28,866 : INFO : Epoch
2017-05-08 15:55:28,866 : INFO : 13
2017-05-08 15:55:28,866 : INFO : train percentage
2017-05-08 15:55:28,866 : INFO : 0.997543352601
2017-05-08 15:55:28,866 : INFO : Epoch
2017-05-08 15:55:28,866 : INFO : 13
2017-05-08 15:55:28,866 : INFO : dev percentage
2017-05-08 15:55:28,866 : INFO : 0.79128440367
2017-05-08 15:55:28,867 : INFO : Epoch
2017-05-08 15:55:28,867 : INFO : 13
2017-05-08 15:55:28,867 : INFO : test percentage
2017-05-08 15:55:28,867 : INFO : 0.81109280615
2017-05-08 16:13:15,008 : INFO : ==> Train loss   : 0.010016
2017-05-08 16:13:15,009 : INFO : Epoch
2017-05-08 16:13:15,009 : INFO : 14
2017-05-08 16:13:15,009 : INFO : train percentage
2017-05-08 16:13:15,009 : INFO : 0.997976878613
2017-05-08 16:13:15,009 : INFO : Epoch
2017-05-08 16:13:15,009 : INFO : 14
2017-05-08 16:13:15,009 : INFO : dev percentage
2017-05-08 16:13:15,009 : INFO : 0.780963302752
2017-05-08 16:13:15,010 : INFO : Epoch
2017-05-08 16:13:15,010 : INFO : 14
2017-05-08 16:13:15,010 : INFO : test percentage
2017-05-08 16:13:15,010 : INFO : 0.808347062054
2017-05-08 16:30:49,338 : INFO : ==> Train loss   : 0.006614
2017-05-08 16:30:49,338 : INFO : Epoch
2017-05-08 16:30:49,338 : INFO : 15
2017-05-08 16:30:49,338 : INFO : train percentage
2017-05-08 16:30:49,338 : INFO : 0.999132947977
2017-05-08 16:30:49,338 : INFO : Epoch
2017-05-08 16:30:49,338 : INFO : 15
2017-05-08 16:30:49,338 : INFO : dev percentage
2017-05-08 16:30:49,339 : INFO : 0.793577981651
2017-05-08 16:30:49,339 : INFO : Epoch
2017-05-08 16:30:49,339 : INFO : 15
2017-05-08 16:30:49,339 : INFO : test percentage
2017-05-08 16:30:49,339 : INFO : 0.807248764415
2017-05-08 16:38:46,369 : INFO : LOG_FILE
2017-05-08 16:38:46,369 : INFO : _________________________________start___________________________________
2017-05-08 16:38:46,375 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 16:38:46,480 : INFO : ==> SST vocabulary size : 21705
2017-05-08 16:38:52,184 : INFO : _param count_
2017-05-08 16:38:52,184 : INFO : torch.Size([168, 300])
2017-05-08 16:38:52,184 : INFO : torch.Size([168])
2017-05-08 16:38:52,184 : INFO : torch.Size([168, 168])
2017-05-08 16:38:52,184 : INFO : torch.Size([168])
2017-05-08 16:38:52,185 : INFO : torch.Size([168, 300])
2017-05-08 16:38:52,185 : INFO : torch.Size([168])
2017-05-08 16:38:52,185 : INFO : torch.Size([168, 168])
2017-05-08 16:38:52,185 : INFO : torch.Size([168])
2017-05-08 16:38:52,185 : INFO : torch.Size([168, 300])
2017-05-08 16:38:52,185 : INFO : torch.Size([168])
2017-05-08 16:38:52,185 : INFO : torch.Size([168, 168])
2017-05-08 16:38:52,185 : INFO : torch.Size([168])
2017-05-08 16:38:52,186 : INFO : torch.Size([168, 300])
2017-05-08 16:38:52,186 : INFO : torch.Size([168])
2017-05-08 16:38:52,186 : INFO : torch.Size([168, 168])
2017-05-08 16:38:52,186 : INFO : torch.Size([168])
2017-05-08 16:38:52,186 : INFO : torch.Size([3, 168])
2017-05-08 16:38:52,186 : INFO : torch.Size([3])
2017-05-08 16:38:52,186 : INFO : sum
2017-05-08 16:38:52,186 : INFO : 316347
2017-05-08 16:38:52,187 : INFO : ____________
2017-05-08 16:43:07,749 : INFO : LOG_FILE
2017-05-08 16:43:07,750 : INFO : _________________________________start___________________________________
2017-05-08 16:43:07,763 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 16:43:08,049 : INFO : ==> SST vocabulary size : 21705
2017-05-08 16:48:27,738 : INFO : ==> Train loss   : 0.006354
2017-05-08 16:48:27,739 : INFO : Epoch
2017-05-08 16:48:27,739 : INFO : 16
2017-05-08 16:48:27,739 : INFO : train percentage
2017-05-08 16:48:27,739 : INFO : 0.998554913295
2017-05-08 16:48:27,739 : INFO : Epoch
2017-05-08 16:48:27,739 : INFO : 16
2017-05-08 16:48:27,739 : INFO : dev percentage
2017-05-08 16:48:27,740 : INFO : 0.787844036697
2017-05-08 16:48:27,740 : INFO : Epoch
2017-05-08 16:48:27,740 : INFO : 16
2017-05-08 16:48:27,740 : INFO : test percentage
2017-05-08 16:48:27,740 : INFO : 0.805052169138
2017-05-08 16:51:56,588 : INFO : LOG_FILE
2017-05-08 16:51:56,588 : INFO : _________________________________start___________________________________
2017-05-08 16:51:56,601 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 16:51:56,915 : INFO : ==> SST vocabulary size : 21705
2017-05-08 16:57:40,156 : INFO : LOG_FILE
2017-05-08 16:57:40,157 : INFO : _________________________________start___________________________________
2017-05-08 16:57:40,169 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 16:57:40,463 : INFO : ==> SST vocabulary size : 21705
2017-05-08 16:57:59,070 : INFO : _param count_
2017-05-08 16:57:59,072 : INFO : torch.Size([168, 300])
2017-05-08 16:57:59,073 : INFO : torch.Size([168])
2017-05-08 16:57:59,074 : INFO : torch.Size([168, 168])
2017-05-08 16:57:59,074 : INFO : torch.Size([168])
2017-05-08 16:57:59,075 : INFO : torch.Size([168, 168])
2017-05-08 16:57:59,075 : INFO : torch.Size([168])
2017-05-08 16:57:59,076 : INFO : torch.Size([168, 300])
2017-05-08 16:57:59,077 : INFO : torch.Size([168])
2017-05-08 16:57:59,077 : INFO : torch.Size([168, 300])
2017-05-08 16:57:59,078 : INFO : torch.Size([168])
2017-05-08 16:57:59,078 : INFO : torch.Size([168, 168])
2017-05-08 16:57:59,079 : INFO : torch.Size([168])
2017-05-08 16:57:59,079 : INFO : torch.Size([168, 300])
2017-05-08 16:57:59,080 : INFO : torch.Size([168])
2017-05-08 16:57:59,081 : INFO : torch.Size([168, 168])
2017-05-08 16:57:59,081 : INFO : torch.Size([168])
2017-05-08 16:57:59,082 : INFO : torch.Size([3, 168])
2017-05-08 16:57:59,083 : INFO : torch.Size([3])
2017-05-08 16:57:59,083 : INFO : sum
2017-05-08 16:57:59,084 : INFO : 316347
2017-05-08 16:57:59,084 : INFO : ____________
2017-05-08 16:59:19,480 : INFO : LOG_FILE
2017-05-08 16:59:19,481 : INFO : _________________________________start___________________________________
2017-05-08 16:59:19,493 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 16:59:19,801 : INFO : ==> SST vocabulary size : 21705
2017-05-08 16:59:26,034 : INFO : _param count_
2017-05-08 16:59:26,037 : INFO : torch.Size([168, 300])
2017-05-08 16:59:26,037 : INFO : torch.Size([168])
2017-05-08 16:59:26,038 : INFO : torch.Size([168, 168])
2017-05-08 16:59:26,038 : INFO : torch.Size([168])
2017-05-08 16:59:26,039 : INFO : torch.Size([168, 168])
2017-05-08 16:59:26,040 : INFO : torch.Size([168])
2017-05-08 16:59:26,040 : INFO : torch.Size([168, 300])
2017-05-08 16:59:26,041 : INFO : torch.Size([168])
2017-05-08 16:59:26,041 : INFO : torch.Size([168, 300])
2017-05-08 16:59:26,042 : INFO : torch.Size([168])
2017-05-08 16:59:26,042 : INFO : torch.Size([168, 168])
2017-05-08 16:59:26,043 : INFO : torch.Size([168])
2017-05-08 16:59:26,043 : INFO : torch.Size([168, 300])
2017-05-08 16:59:26,044 : INFO : torch.Size([168])
2017-05-08 16:59:26,045 : INFO : torch.Size([168, 168])
2017-05-08 16:59:26,046 : INFO : torch.Size([168])
2017-05-08 16:59:26,046 : INFO : torch.Size([3, 168])
2017-05-08 16:59:26,047 : INFO : torch.Size([3])
2017-05-08 16:59:26,047 : INFO : sum
2017-05-08 16:59:26,048 : INFO : 316347
2017-05-08 16:59:26,048 : INFO : ____________
2017-05-08 17:05:59,720 : INFO : ==> Train loss   : 0.003761
2017-05-08 17:05:59,721 : INFO : Epoch
2017-05-08 17:05:59,721 : INFO : 17
2017-05-08 17:05:59,721 : INFO : train percentage
2017-05-08 17:05:59,721 : INFO : 0.999710982659
2017-05-08 17:05:59,721 : INFO : Epoch
2017-05-08 17:05:59,721 : INFO : 17
2017-05-08 17:05:59,721 : INFO : dev percentage
2017-05-08 17:05:59,722 : INFO : 0.792431192661
2017-05-08 17:05:59,722 : INFO : Epoch
2017-05-08 17:05:59,722 : INFO : 17
2017-05-08 17:05:59,722 : INFO : test percentage
2017-05-08 17:05:59,722 : INFO : 0.807248764415
2017-05-08 17:18:50,324 : INFO : LOG_FILE
2017-05-08 17:18:50,325 : INFO : _________________________________start___________________________________
2017-05-08 17:18:50,337 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 17:18:50,634 : INFO : ==> SST vocabulary size : 21705
2017-05-08 17:18:56,738 : INFO : _param count_
2017-05-08 17:18:56,740 : INFO : torch.Size([168, 300])
2017-05-08 17:18:56,741 : INFO : torch.Size([168])
2017-05-08 17:18:56,742 : INFO : torch.Size([168, 168])
2017-05-08 17:18:56,742 : INFO : torch.Size([168])
2017-05-08 17:18:56,743 : INFO : torch.Size([168, 168])
2017-05-08 17:18:56,743 : INFO : torch.Size([168])
2017-05-08 17:18:56,744 : INFO : torch.Size([168, 300])
2017-05-08 17:18:56,744 : INFO : torch.Size([168])
2017-05-08 17:18:56,745 : INFO : torch.Size([168, 300])
2017-05-08 17:18:56,745 : INFO : torch.Size([168])
2017-05-08 17:18:56,746 : INFO : torch.Size([168, 168])
2017-05-08 17:18:56,746 : INFO : torch.Size([168])
2017-05-08 17:18:56,747 : INFO : torch.Size([168, 300])
2017-05-08 17:18:56,748 : INFO : torch.Size([168])
2017-05-08 17:18:56,748 : INFO : torch.Size([168, 168])
2017-05-08 17:18:56,749 : INFO : torch.Size([168])
2017-05-08 17:18:56,749 : INFO : torch.Size([3, 168])
2017-05-08 17:18:56,750 : INFO : torch.Size([3])
2017-05-08 17:18:56,750 : INFO : sum
2017-05-08 17:18:56,751 : INFO : 316347
2017-05-08 17:18:56,751 : INFO : ____________
2017-05-08 17:21:08,857 : INFO : LOG_FILE
2017-05-08 17:21:08,857 : INFO : _________________________________start___________________________________
2017-05-08 17:21:08,872 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 17:21:09,179 : INFO : ==> SST vocabulary size : 21705
2017-05-08 17:21:15,274 : INFO : _param count_
2017-05-08 17:21:15,276 : INFO : torch.Size([168, 300])
2017-05-08 17:21:15,277 : INFO : torch.Size([168])
2017-05-08 17:21:15,277 : INFO : torch.Size([168, 168])
2017-05-08 17:21:15,278 : INFO : torch.Size([168])
2017-05-08 17:21:15,279 : INFO : torch.Size([168, 168])
2017-05-08 17:21:15,280 : INFO : torch.Size([168])
2017-05-08 17:21:15,280 : INFO : torch.Size([168, 300])
2017-05-08 17:21:15,281 : INFO : torch.Size([168])
2017-05-08 17:21:15,281 : INFO : torch.Size([168, 300])
2017-05-08 17:21:15,282 : INFO : torch.Size([168])
2017-05-08 17:21:15,282 : INFO : torch.Size([168, 168])
2017-05-08 17:21:15,283 : INFO : torch.Size([168])
2017-05-08 17:21:15,283 : INFO : torch.Size([168, 300])
2017-05-08 17:21:15,284 : INFO : torch.Size([168])
2017-05-08 17:21:15,284 : INFO : torch.Size([168, 168])
2017-05-08 17:21:15,285 : INFO : torch.Size([168])
2017-05-08 17:21:15,286 : INFO : torch.Size([3, 168])
2017-05-08 17:21:15,286 : INFO : torch.Size([3])
2017-05-08 17:21:15,287 : INFO : sum
2017-05-08 17:21:15,287 : INFO : 316347
2017-05-08 17:21:15,288 : INFO : ____________
2017-05-08 17:23:15,805 : INFO : LOG_FILE
2017-05-08 17:23:15,806 : INFO : _________________________________start___________________________________
2017-05-08 17:23:15,820 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 17:23:16,115 : INFO : ==> SST vocabulary size : 21705
2017-05-08 17:23:22,170 : INFO : _param count_
2017-05-08 17:23:22,172 : INFO : torch.Size([168, 300])
2017-05-08 17:23:22,173 : INFO : torch.Size([168])
2017-05-08 17:23:22,174 : INFO : torch.Size([168, 168])
2017-05-08 17:23:22,174 : INFO : torch.Size([168])
2017-05-08 17:23:22,175 : INFO : torch.Size([168, 168])
2017-05-08 17:23:22,175 : INFO : torch.Size([168])
2017-05-08 17:23:22,176 : INFO : torch.Size([168, 300])
2017-05-08 17:23:22,176 : INFO : torch.Size([168])
2017-05-08 17:23:22,177 : INFO : torch.Size([168, 300])
2017-05-08 17:23:22,178 : INFO : torch.Size([168])
2017-05-08 17:23:22,178 : INFO : torch.Size([168, 168])
2017-05-08 17:23:22,179 : INFO : torch.Size([168])
2017-05-08 17:23:22,179 : INFO : torch.Size([168, 300])
2017-05-08 17:23:22,180 : INFO : torch.Size([168])
2017-05-08 17:23:22,180 : INFO : torch.Size([168, 168])
2017-05-08 17:23:22,181 : INFO : torch.Size([168])
2017-05-08 17:23:22,182 : INFO : torch.Size([3, 168])
2017-05-08 17:23:22,182 : INFO : torch.Size([3])
2017-05-08 17:23:22,183 : INFO : sum
2017-05-08 17:23:22,183 : INFO : 316347
2017-05-08 17:23:22,184 : INFO : ____________
2017-05-08 17:50:16,117 : INFO : LOG_FILE
2017-05-08 17:50:16,118 : INFO : _________________________________start___________________________________
2017-05-08 17:50:16,130 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 17:50:16,437 : INFO : ==> SST vocabulary size : 21705
2017-05-08 17:50:22,922 : INFO : _param count_
2017-05-08 17:50:22,925 : INFO : torch.Size([168, 300])
2017-05-08 17:50:22,925 : INFO : torch.Size([168])
2017-05-08 17:50:22,926 : INFO : torch.Size([168, 168])
2017-05-08 17:50:22,927 : INFO : torch.Size([168])
2017-05-08 17:50:22,927 : INFO : torch.Size([168, 168])
2017-05-08 17:50:22,928 : INFO : torch.Size([168])
2017-05-08 17:50:22,928 : INFO : torch.Size([168, 300])
2017-05-08 17:50:22,929 : INFO : torch.Size([168])
2017-05-08 17:50:22,929 : INFO : torch.Size([168, 300])
2017-05-08 17:50:22,930 : INFO : torch.Size([168])
2017-05-08 17:50:22,930 : INFO : torch.Size([168, 168])
2017-05-08 17:50:22,931 : INFO : torch.Size([168])
2017-05-08 17:50:22,931 : INFO : torch.Size([168, 300])
2017-05-08 17:50:22,932 : INFO : torch.Size([168])
2017-05-08 17:50:22,933 : INFO : torch.Size([168, 168])
2017-05-08 17:50:22,933 : INFO : torch.Size([168])
2017-05-08 17:50:22,934 : INFO : torch.Size([3, 168])
2017-05-08 17:50:22,934 : INFO : torch.Size([3])
2017-05-08 17:50:22,935 : INFO : sum
2017-05-08 17:50:22,935 : INFO : 316347
2017-05-08 17:50:22,936 : INFO : ____________
2017-05-08 17:54:59,679 : INFO : LOG_FILE
2017-05-08 17:54:59,680 : INFO : _________________________________start___________________________________
2017-05-08 17:54:59,692 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 17:54:59,998 : INFO : ==> SST vocabulary size : 21705
2017-05-08 17:55:06,082 : INFO : _param count_
2017-05-08 17:55:06,085 : INFO : torch.Size([168, 300])
2017-05-08 17:55:06,086 : INFO : torch.Size([168])
2017-05-08 17:55:06,087 : INFO : torch.Size([168, 168])
2017-05-08 17:55:06,088 : INFO : torch.Size([168])
2017-05-08 17:55:06,089 : INFO : torch.Size([168, 168])
2017-05-08 17:55:06,090 : INFO : torch.Size([168])
2017-05-08 17:55:06,091 : INFO : torch.Size([168, 300])
2017-05-08 17:55:06,091 : INFO : torch.Size([168])
2017-05-08 17:55:06,092 : INFO : torch.Size([168, 300])
2017-05-08 17:55:06,093 : INFO : torch.Size([168])
2017-05-08 17:55:06,094 : INFO : torch.Size([168, 168])
2017-05-08 17:55:06,095 : INFO : torch.Size([168])
2017-05-08 17:55:06,096 : INFO : torch.Size([168, 300])
2017-05-08 17:55:06,096 : INFO : torch.Size([168])
2017-05-08 17:55:06,097 : INFO : torch.Size([168, 168])
2017-05-08 17:55:06,098 : INFO : torch.Size([168])
2017-05-08 17:55:06,099 : INFO : torch.Size([3, 168])
2017-05-08 17:55:06,100 : INFO : torch.Size([3])
2017-05-08 17:55:06,101 : INFO : sum
2017-05-08 17:55:06,101 : INFO : 316347
2017-05-08 17:55:06,102 : INFO : ____________
2017-05-08 20:37:10,011 : INFO : LOG_FILE
2017-05-08 20:37:10,012 : INFO : _________________________________start___________________________________
2017-05-08 20:37:10,025 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 20:37:10,354 : INFO : ==> SST vocabulary size : 21705
2017-05-08 20:37:16,347 : INFO : _param count_
2017-05-08 20:37:16,349 : INFO : torch.Size([168, 300])
2017-05-08 20:37:16,350 : INFO : torch.Size([168])
2017-05-08 20:37:16,350 : INFO : torch.Size([168, 168])
2017-05-08 20:37:16,351 : INFO : torch.Size([168])
2017-05-08 20:37:16,351 : INFO : torch.Size([168, 168])
2017-05-08 20:37:16,352 : INFO : torch.Size([168])
2017-05-08 20:37:16,353 : INFO : torch.Size([168, 300])
2017-05-08 20:37:16,353 : INFO : torch.Size([168])
2017-05-08 20:37:16,354 : INFO : torch.Size([168, 300])
2017-05-08 20:37:16,354 : INFO : torch.Size([168])
2017-05-08 20:37:16,355 : INFO : torch.Size([168, 168])
2017-05-08 20:37:16,355 : INFO : torch.Size([168])
2017-05-08 20:37:16,356 : INFO : torch.Size([168, 300])
2017-05-08 20:37:16,356 : INFO : torch.Size([168])
2017-05-08 20:37:16,357 : INFO : torch.Size([168, 168])
2017-05-08 20:37:16,358 : INFO : torch.Size([168])
2017-05-08 20:37:16,358 : INFO : torch.Size([3, 168])
2017-05-08 20:37:16,359 : INFO : torch.Size([3])
2017-05-08 20:37:16,359 : INFO : sum
2017-05-08 20:37:16,360 : INFO : 316347
2017-05-08 20:37:16,360 : INFO : ____________
2017-05-08 21:04:20,777 : INFO : LOG_FILE
2017-05-08 21:04:20,777 : INFO : _________________________________start___________________________________
2017-05-08 21:04:20,790 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 21:04:21,097 : INFO : ==> SST vocabulary size : 21705
2017-05-08 21:04:27,094 : INFO : _param count_
2017-05-08 21:04:27,097 : INFO : torch.Size([168, 300])
2017-05-08 21:04:27,098 : INFO : torch.Size([168])
2017-05-08 21:04:27,099 : INFO : torch.Size([168, 168])
2017-05-08 21:04:27,100 : INFO : torch.Size([168])
2017-05-08 21:04:27,100 : INFO : torch.Size([168, 168])
2017-05-08 21:04:27,101 : INFO : torch.Size([168])
2017-05-08 21:04:27,102 : INFO : torch.Size([168, 300])
2017-05-08 21:04:27,103 : INFO : torch.Size([168])
2017-05-08 21:04:27,104 : INFO : torch.Size([168, 300])
2017-05-08 21:04:27,104 : INFO : torch.Size([168])
2017-05-08 21:04:27,105 : INFO : torch.Size([168, 168])
2017-05-08 21:04:27,106 : INFO : torch.Size([168])
2017-05-08 21:04:27,107 : INFO : torch.Size([168, 300])
2017-05-08 21:04:27,107 : INFO : torch.Size([168])
2017-05-08 21:04:27,108 : INFO : torch.Size([168, 168])
2017-05-08 21:04:27,109 : INFO : torch.Size([168])
2017-05-08 21:04:27,110 : INFO : torch.Size([3, 168])
2017-05-08 21:04:27,111 : INFO : torch.Size([3])
2017-05-08 21:04:27,111 : INFO : sum
2017-05-08 21:04:27,112 : INFO : 316347
2017-05-08 21:04:27,113 : INFO : ____________
2017-05-08 21:10:11,693 : INFO : LOG_FILE
2017-05-08 21:10:11,693 : INFO : _________________________________start___________________________________
2017-05-08 21:10:11,700 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 21:10:11,804 : INFO : ==> SST vocabulary size : 21705
2017-05-08 21:10:17,353 : INFO : _param count_
2017-05-08 21:10:17,354 : INFO : torch.Size([168, 300])
2017-05-08 21:10:17,354 : INFO : torch.Size([168])
2017-05-08 21:10:17,354 : INFO : torch.Size([168, 168])
2017-05-08 21:10:17,354 : INFO : torch.Size([168])
2017-05-08 21:10:17,354 : INFO : torch.Size([168, 168])
2017-05-08 21:10:17,355 : INFO : torch.Size([168])
2017-05-08 21:10:17,355 : INFO : torch.Size([168, 300])
2017-05-08 21:10:17,355 : INFO : torch.Size([168])
2017-05-08 21:10:17,355 : INFO : torch.Size([168, 300])
2017-05-08 21:10:17,355 : INFO : torch.Size([168])
2017-05-08 21:10:17,355 : INFO : torch.Size([168, 168])
2017-05-08 21:10:17,355 : INFO : torch.Size([168])
2017-05-08 21:10:17,355 : INFO : torch.Size([168, 300])
2017-05-08 21:10:17,355 : INFO : torch.Size([168])
2017-05-08 21:10:17,356 : INFO : torch.Size([168, 168])
2017-05-08 21:10:17,356 : INFO : torch.Size([168])
2017-05-08 21:10:17,356 : INFO : torch.Size([3, 168])
2017-05-08 21:10:17,356 : INFO : torch.Size([3])
2017-05-08 21:10:17,356 : INFO : sum
2017-05-08 21:10:17,356 : INFO : 316347
2017-05-08 21:10:17,356 : INFO : ____________
2017-05-08 21:11:06,338 : INFO : LOG_FILE
2017-05-08 21:11:06,339 : INFO : _________________________________start___________________________________
2017-05-08 21:11:06,352 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 21:11:06,649 : INFO : ==> SST vocabulary size : 21705
2017-05-08 21:11:12,655 : INFO : _param count_
2017-05-08 21:11:12,659 : INFO : torch.Size([168, 300])
2017-05-08 21:11:12,659 : INFO : torch.Size([168])
2017-05-08 21:11:12,660 : INFO : torch.Size([168, 168])
2017-05-08 21:11:12,661 : INFO : torch.Size([168])
2017-05-08 21:11:12,662 : INFO : torch.Size([168, 168])
2017-05-08 21:11:12,663 : INFO : torch.Size([168])
2017-05-08 21:11:12,664 : INFO : torch.Size([168, 300])
2017-05-08 21:11:12,665 : INFO : torch.Size([168])
2017-05-08 21:11:12,665 : INFO : torch.Size([168, 300])
2017-05-08 21:11:12,666 : INFO : torch.Size([168])
2017-05-08 21:11:12,666 : INFO : torch.Size([168, 168])
2017-05-08 21:11:12,667 : INFO : torch.Size([168])
2017-05-08 21:11:12,667 : INFO : torch.Size([168, 300])
2017-05-08 21:11:12,668 : INFO : torch.Size([168])
2017-05-08 21:11:12,668 : INFO : torch.Size([168, 168])
2017-05-08 21:11:12,669 : INFO : torch.Size([168])
2017-05-08 21:11:12,670 : INFO : torch.Size([3, 168])
2017-05-08 21:11:12,670 : INFO : torch.Size([3])
2017-05-08 21:11:12,671 : INFO : sum
2017-05-08 21:11:12,671 : INFO : 316347
2017-05-08 21:11:12,672 : INFO : ____________
2017-05-08 21:43:38,896 : INFO : LOG_FILE
2017-05-08 21:43:38,897 : INFO : _________________________________start___________________________________
2017-05-08 21:43:38,909 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 21:43:39,215 : INFO : ==> SST vocabulary size : 21705
2017-05-08 21:43:45,231 : INFO : _param count_
2017-05-08 21:43:45,233 : INFO : torch.Size([168, 300])
2017-05-08 21:43:45,234 : INFO : torch.Size([168])
2017-05-08 21:43:45,234 : INFO : torch.Size([168, 168])
2017-05-08 21:43:45,235 : INFO : torch.Size([168])
2017-05-08 21:43:45,235 : INFO : torch.Size([168, 168])
2017-05-08 21:43:45,236 : INFO : torch.Size([168])
2017-05-08 21:43:45,237 : INFO : torch.Size([168, 300])
2017-05-08 21:43:45,237 : INFO : torch.Size([168])
2017-05-08 21:43:45,238 : INFO : torch.Size([168, 300])
2017-05-08 21:43:45,238 : INFO : torch.Size([168])
2017-05-08 21:43:45,239 : INFO : torch.Size([168, 168])
2017-05-08 21:43:45,239 : INFO : torch.Size([168])
2017-05-08 21:43:45,240 : INFO : torch.Size([168, 300])
2017-05-08 21:43:45,240 : INFO : torch.Size([168])
2017-05-08 21:43:45,241 : INFO : torch.Size([168, 168])
2017-05-08 21:43:45,241 : INFO : torch.Size([168])
2017-05-08 21:43:45,242 : INFO : torch.Size([3, 168])
2017-05-08 21:43:45,243 : INFO : torch.Size([3])
2017-05-08 21:43:45,243 : INFO : sum
2017-05-08 21:43:45,244 : INFO : 316347
2017-05-08 21:43:45,244 : INFO : ____________
2017-05-08 23:04:28,422 : INFO : LOG_FILE
2017-05-08 23:04:28,422 : INFO : _________________________________start___________________________________
2017-05-08 23:04:28,428 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 23:04:28,460 : INFO : ==> SST vocabulary size : 21701
2017-05-08 23:04:38,456 : INFO : LOG_FILE
2017-05-08 23:04:38,456 : INFO : _________________________________start___________________________________
2017-05-08 23:04:38,464 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 23:04:38,493 : INFO : ==> SST vocabulary size : 21701
2017-05-08 23:04:51,959 : INFO : _param count_
2017-05-08 23:04:51,959 : INFO : torch.Size([168, 300])
2017-05-08 23:04:51,959 : INFO : torch.Size([168])
2017-05-08 23:04:51,959 : INFO : torch.Size([168, 168])
2017-05-08 23:04:51,960 : INFO : torch.Size([168])
2017-05-08 23:04:51,960 : INFO : torch.Size([168, 168])
2017-05-08 23:04:51,960 : INFO : torch.Size([168])
2017-05-08 23:04:51,960 : INFO : torch.Size([168, 300])
2017-05-08 23:04:51,960 : INFO : torch.Size([168])
2017-05-08 23:04:51,960 : INFO : torch.Size([168, 300])
2017-05-08 23:04:51,960 : INFO : torch.Size([168])
2017-05-08 23:04:51,961 : INFO : torch.Size([168, 168])
2017-05-08 23:04:51,961 : INFO : torch.Size([168])
2017-05-08 23:04:51,961 : INFO : torch.Size([168, 300])
2017-05-08 23:04:51,961 : INFO : torch.Size([168])
2017-05-08 23:04:51,961 : INFO : torch.Size([168, 168])
2017-05-08 23:04:51,961 : INFO : torch.Size([168])
2017-05-08 23:04:51,961 : INFO : torch.Size([3, 168])
2017-05-08 23:04:51,961 : INFO : torch.Size([3])
2017-05-08 23:04:51,962 : INFO : sum
2017-05-08 23:04:51,962 : INFO : 316347
2017-05-08 23:04:51,962 : INFO : ____________
2017-05-08 23:04:51,962 : INFO : ==> File not found, preparing, be patient
2017-05-08 23:07:32,489 : INFO : LOG_FILE
2017-05-08 23:07:32,489 : INFO : _________________________________start___________________________________
2017-05-08 23:07:32,496 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 23:07:32,523 : INFO : ==> SST vocabulary size : 21701
2017-05-08 23:07:45,705 : INFO : _param count_
2017-05-08 23:07:45,706 : INFO : torch.Size([168, 300])
2017-05-08 23:07:45,706 : INFO : torch.Size([168])
2017-05-08 23:07:45,706 : INFO : torch.Size([168, 168])
2017-05-08 23:07:45,706 : INFO : torch.Size([168])
2017-05-08 23:07:45,706 : INFO : torch.Size([168, 168])
2017-05-08 23:07:45,707 : INFO : torch.Size([168])
2017-05-08 23:07:45,707 : INFO : torch.Size([168, 300])
2017-05-08 23:07:45,707 : INFO : torch.Size([168])
2017-05-08 23:07:45,707 : INFO : torch.Size([168, 300])
2017-05-08 23:07:45,707 : INFO : torch.Size([168])
2017-05-08 23:07:45,707 : INFO : torch.Size([168, 168])
2017-05-08 23:07:45,707 : INFO : torch.Size([168])
2017-05-08 23:07:45,707 : INFO : torch.Size([168, 300])
2017-05-08 23:07:45,708 : INFO : torch.Size([168])
2017-05-08 23:07:45,708 : INFO : torch.Size([168, 168])
2017-05-08 23:07:45,708 : INFO : torch.Size([168])
2017-05-08 23:07:45,708 : INFO : torch.Size([3, 168])
2017-05-08 23:07:45,708 : INFO : torch.Size([3])
2017-05-08 23:07:45,708 : INFO : sum
2017-05-08 23:07:45,708 : INFO : 316347
2017-05-08 23:07:45,709 : INFO : ____________
2017-05-08 23:07:45,709 : INFO : ==> File found, loading to memory
2017-05-08 23:07:52,691 : INFO : ==> GLOVE vocabulary size: 2196016
2017-05-08 23:07:53,334 : INFO : done creating emb, quit
2017-05-08 23:07:53,335 : INFO : quit program due to memory leak during preprocess data, please rerun sentiment.py
2017-05-08 23:08:29,946 : INFO : LOG_FILE
2017-05-08 23:08:29,947 : INFO : _________________________________start___________________________________
2017-05-08 23:08:29,953 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 23:08:29,985 : INFO : ==> SST vocabulary size : 21701
2017-05-08 23:08:35,650 : INFO : _param count_
2017-05-08 23:08:35,651 : INFO : torch.Size([168, 300])
2017-05-08 23:08:35,651 : INFO : torch.Size([168])
2017-05-08 23:08:35,651 : INFO : torch.Size([168, 168])
2017-05-08 23:08:35,651 : INFO : torch.Size([168])
2017-05-08 23:08:35,651 : INFO : torch.Size([168, 168])
2017-05-08 23:08:35,651 : INFO : torch.Size([168])
2017-05-08 23:08:35,652 : INFO : torch.Size([168, 300])
2017-05-08 23:08:35,652 : INFO : torch.Size([168])
2017-05-08 23:08:35,652 : INFO : torch.Size([168, 300])
2017-05-08 23:08:35,652 : INFO : torch.Size([168])
2017-05-08 23:08:35,652 : INFO : torch.Size([168, 168])
2017-05-08 23:08:35,652 : INFO : torch.Size([168])
2017-05-08 23:08:35,652 : INFO : torch.Size([168, 300])
2017-05-08 23:08:35,653 : INFO : torch.Size([168])
2017-05-08 23:08:35,653 : INFO : torch.Size([168, 168])
2017-05-08 23:08:35,653 : INFO : torch.Size([168])
2017-05-08 23:08:35,653 : INFO : torch.Size([3, 168])
2017-05-08 23:08:35,653 : INFO : torch.Size([3])
2017-05-08 23:08:35,653 : INFO : sum
2017-05-08 23:08:35,653 : INFO : 316347
2017-05-08 23:08:35,653 : INFO : ____________
2017-05-08 23:12:45,955 : INFO : LOG_FILE
2017-05-08 23:12:45,956 : INFO : _________________________________start___________________________________
2017-05-08 23:12:45,962 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 23:12:45,992 : INFO : ==> SST vocabulary size : 21701
2017-05-08 23:12:51,575 : INFO : _param count_
2017-05-08 23:12:51,575 : INFO : torch.Size([168, 300])
2017-05-08 23:12:51,575 : INFO : torch.Size([168])
2017-05-08 23:12:51,575 : INFO : torch.Size([168, 168])
2017-05-08 23:12:51,576 : INFO : torch.Size([168])
2017-05-08 23:12:51,576 : INFO : torch.Size([168, 168])
2017-05-08 23:12:51,576 : INFO : torch.Size([168])
2017-05-08 23:12:51,576 : INFO : torch.Size([168, 300])
2017-05-08 23:12:51,576 : INFO : torch.Size([168])
2017-05-08 23:12:51,576 : INFO : torch.Size([168, 300])
2017-05-08 23:12:51,576 : INFO : torch.Size([168])
2017-05-08 23:12:51,577 : INFO : torch.Size([168, 168])
2017-05-08 23:12:51,577 : INFO : torch.Size([168])
2017-05-08 23:12:51,577 : INFO : torch.Size([168, 300])
2017-05-08 23:12:51,577 : INFO : torch.Size([168])
2017-05-08 23:12:51,577 : INFO : torch.Size([168, 168])
2017-05-08 23:12:51,577 : INFO : torch.Size([168])
2017-05-08 23:12:51,577 : INFO : torch.Size([3, 168])
2017-05-08 23:12:51,578 : INFO : torch.Size([3])
2017-05-08 23:12:51,578 : INFO : sum
2017-05-08 23:12:51,578 : INFO : 316347
2017-05-08 23:12:51,578 : INFO : ____________
2017-05-08 23:30:33,885 : INFO : ==> Train loss   : 0.384340
2017-05-08 23:30:33,885 : INFO : Epoch
2017-05-08 23:30:33,885 : INFO : 0
2017-05-08 23:30:33,885 : INFO : train percentage
2017-05-08 23:30:33,885 : INFO : 0.865895953757
2017-05-08 23:30:33,885 : INFO : Epoch
2017-05-08 23:30:33,886 : INFO : 0
2017-05-08 23:30:33,886 : INFO : dev percentage
2017-05-08 23:30:33,886 : INFO : 0.824541284404
2017-05-08 23:30:33,886 : INFO : Epoch
2017-05-08 23:30:33,886 : INFO : 0
2017-05-08 23:30:33,886 : INFO : test percentage
2017-05-08 23:30:33,886 : INFO : 0.841295991214
2017-05-08 23:40:43,643 : INFO : LOG_FILE
2017-05-08 23:40:43,644 : INFO : _________________________________start___________________________________
2017-05-08 23:40:43,656 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-08 23:40:43,862 : INFO : ==> SST vocabulary size : 21701
2017-05-08 23:40:49,895 : INFO : _param count_
2017-05-08 23:40:49,897 : INFO : torch.Size([168, 300])
2017-05-08 23:40:49,898 : INFO : torch.Size([168])
2017-05-08 23:40:49,899 : INFO : torch.Size([168, 168])
2017-05-08 23:40:49,899 : INFO : torch.Size([168])
2017-05-08 23:40:49,900 : INFO : torch.Size([168, 168])
2017-05-08 23:40:49,900 : INFO : torch.Size([168])
2017-05-08 23:40:49,901 : INFO : torch.Size([168, 300])
2017-05-08 23:40:49,901 : INFO : torch.Size([168])
2017-05-08 23:40:49,902 : INFO : torch.Size([168, 300])
2017-05-08 23:40:49,903 : INFO : torch.Size([168])
2017-05-08 23:40:49,903 : INFO : torch.Size([168, 168])
2017-05-08 23:40:49,904 : INFO : torch.Size([168])
2017-05-08 23:40:49,904 : INFO : torch.Size([168, 300])
2017-05-08 23:40:49,905 : INFO : torch.Size([168])
2017-05-08 23:40:49,905 : INFO : torch.Size([168, 168])
2017-05-08 23:40:49,906 : INFO : torch.Size([168])
2017-05-08 23:40:49,907 : INFO : torch.Size([3, 168])
2017-05-08 23:40:49,907 : INFO : torch.Size([3])
2017-05-08 23:40:49,908 : INFO : sum
2017-05-08 23:40:49,908 : INFO : 316347
2017-05-08 23:40:49,909 : INFO : ____________
2017-05-09 00:11:42,633 : INFO : LOG_FILE
2017-05-09 00:11:42,634 : INFO : _________________________________start___________________________________
2017-05-09 00:11:42,647 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:11:42,855 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:11:48,980 : INFO : _param count_
2017-05-09 00:11:48,983 : INFO : torch.Size([168, 300])
2017-05-09 00:11:48,983 : INFO : torch.Size([168])
2017-05-09 00:11:48,984 : INFO : torch.Size([168, 168])
2017-05-09 00:11:48,985 : INFO : torch.Size([168])
2017-05-09 00:11:48,985 : INFO : torch.Size([168, 168])
2017-05-09 00:11:48,986 : INFO : torch.Size([168])
2017-05-09 00:11:48,986 : INFO : torch.Size([168, 300])
2017-05-09 00:11:48,987 : INFO : torch.Size([168])
2017-05-09 00:11:48,987 : INFO : torch.Size([168, 300])
2017-05-09 00:11:48,988 : INFO : torch.Size([168])
2017-05-09 00:11:48,988 : INFO : torch.Size([168, 168])
2017-05-09 00:11:48,989 : INFO : torch.Size([168])
2017-05-09 00:11:48,990 : INFO : torch.Size([168, 300])
2017-05-09 00:11:48,990 : INFO : torch.Size([168])
2017-05-09 00:11:48,991 : INFO : torch.Size([168, 168])
2017-05-09 00:11:48,991 : INFO : torch.Size([168])
2017-05-09 00:11:48,992 : INFO : torch.Size([3, 168])
2017-05-09 00:11:48,992 : INFO : torch.Size([3])
2017-05-09 00:11:48,993 : INFO : sum
2017-05-09 00:11:48,993 : INFO : 316347
2017-05-09 00:11:48,994 : INFO : ____________
2017-05-09 00:13:05,548 : INFO : LOG_FILE
2017-05-09 00:13:05,549 : INFO : _________________________________start___________________________________
2017-05-09 00:13:05,562 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:13:05,821 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:13:11,884 : INFO : _param count_
2017-05-09 00:13:11,887 : INFO : torch.Size([168, 300])
2017-05-09 00:13:11,887 : INFO : torch.Size([168])
2017-05-09 00:13:11,888 : INFO : torch.Size([168, 168])
2017-05-09 00:13:11,888 : INFO : torch.Size([168])
2017-05-09 00:13:11,889 : INFO : torch.Size([168, 168])
2017-05-09 00:13:11,890 : INFO : torch.Size([168])
2017-05-09 00:13:11,890 : INFO : torch.Size([168, 300])
2017-05-09 00:13:11,891 : INFO : torch.Size([168])
2017-05-09 00:13:11,891 : INFO : torch.Size([168, 300])
2017-05-09 00:13:11,892 : INFO : torch.Size([168])
2017-05-09 00:13:11,892 : INFO : torch.Size([168, 168])
2017-05-09 00:13:11,893 : INFO : torch.Size([168])
2017-05-09 00:13:11,893 : INFO : torch.Size([168, 300])
2017-05-09 00:13:11,894 : INFO : torch.Size([168])
2017-05-09 00:13:11,895 : INFO : torch.Size([168, 168])
2017-05-09 00:13:11,895 : INFO : torch.Size([168])
2017-05-09 00:13:11,896 : INFO : torch.Size([3, 168])
2017-05-09 00:13:11,896 : INFO : torch.Size([3])
2017-05-09 00:13:11,897 : INFO : sum
2017-05-09 00:13:11,897 : INFO : 316347
2017-05-09 00:13:11,898 : INFO : ____________
2017-05-09 00:13:28,524 : INFO : LOG_FILE
2017-05-09 00:13:28,525 : INFO : _________________________________start___________________________________
2017-05-09 00:13:28,539 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:13:28,798 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:13:34,824 : INFO : _param count_
2017-05-09 00:13:34,827 : INFO : torch.Size([168, 300])
2017-05-09 00:13:34,827 : INFO : torch.Size([168])
2017-05-09 00:13:34,828 : INFO : torch.Size([168, 168])
2017-05-09 00:13:34,829 : INFO : torch.Size([168])
2017-05-09 00:13:34,829 : INFO : torch.Size([168, 168])
2017-05-09 00:13:34,830 : INFO : torch.Size([168])
2017-05-09 00:13:34,830 : INFO : torch.Size([168, 300])
2017-05-09 00:13:34,831 : INFO : torch.Size([168])
2017-05-09 00:13:34,831 : INFO : torch.Size([168, 300])
2017-05-09 00:13:34,832 : INFO : torch.Size([168])
2017-05-09 00:13:34,832 : INFO : torch.Size([168, 168])
2017-05-09 00:13:34,833 : INFO : torch.Size([168])
2017-05-09 00:13:34,834 : INFO : torch.Size([168, 300])
2017-05-09 00:13:34,834 : INFO : torch.Size([168])
2017-05-09 00:13:34,835 : INFO : torch.Size([168, 168])
2017-05-09 00:13:34,835 : INFO : torch.Size([168])
2017-05-09 00:13:34,836 : INFO : torch.Size([3, 168])
2017-05-09 00:13:34,836 : INFO : torch.Size([3])
2017-05-09 00:13:34,837 : INFO : sum
2017-05-09 00:13:34,837 : INFO : 316347
2017-05-09 00:13:34,838 : INFO : ____________
2017-05-09 00:13:40,235 : INFO : LOG_FILE
2017-05-09 00:13:40,236 : INFO : _________________________________start___________________________________
2017-05-09 00:13:40,248 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:13:40,508 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:27:48,401 : INFO : LOG_FILE
2017-05-09 00:27:48,402 : INFO : _________________________________start___________________________________
2017-05-09 00:27:48,415 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:27:48,722 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:27:54,849 : INFO : _param count_
2017-05-09 00:27:54,852 : INFO : torch.Size([168, 300])
2017-05-09 00:27:54,853 : INFO : torch.Size([168])
2017-05-09 00:27:54,853 : INFO : torch.Size([168, 168])
2017-05-09 00:27:54,854 : INFO : torch.Size([168])
2017-05-09 00:27:54,854 : INFO : torch.Size([168, 168])
2017-05-09 00:27:54,855 : INFO : torch.Size([168])
2017-05-09 00:27:54,855 : INFO : torch.Size([168, 300])
2017-05-09 00:27:54,856 : INFO : torch.Size([168])
2017-05-09 00:27:54,856 : INFO : torch.Size([168, 300])
2017-05-09 00:27:54,857 : INFO : torch.Size([168])
2017-05-09 00:27:54,858 : INFO : torch.Size([168, 168])
2017-05-09 00:27:54,858 : INFO : torch.Size([168])
2017-05-09 00:27:54,859 : INFO : torch.Size([168, 300])
2017-05-09 00:27:54,859 : INFO : torch.Size([168])
2017-05-09 00:27:54,860 : INFO : torch.Size([168, 168])
2017-05-09 00:27:54,860 : INFO : torch.Size([168])
2017-05-09 00:27:54,861 : INFO : torch.Size([3, 168])
2017-05-09 00:27:54,861 : INFO : torch.Size([3])
2017-05-09 00:27:54,862 : INFO : sum
2017-05-09 00:27:54,863 : INFO : 316347
2017-05-09 00:27:54,863 : INFO : ____________
2017-05-09 00:30:34,262 : INFO : LOG_FILE
2017-05-09 00:30:34,262 : INFO : _________________________________start___________________________________
2017-05-09 00:30:34,269 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:30:34,298 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:30:47,013 : INFO : _param count_
2017-05-09 00:30:47,013 : INFO : torch.Size([168, 300])
2017-05-09 00:30:47,014 : INFO : torch.Size([168])
2017-05-09 00:30:47,014 : INFO : torch.Size([168, 168])
2017-05-09 00:30:47,014 : INFO : torch.Size([168])
2017-05-09 00:30:47,014 : INFO : torch.Size([168, 168])
2017-05-09 00:30:47,014 : INFO : torch.Size([168])
2017-05-09 00:30:47,014 : INFO : torch.Size([168, 300])
2017-05-09 00:30:47,015 : INFO : torch.Size([168])
2017-05-09 00:30:47,015 : INFO : torch.Size([168, 300])
2017-05-09 00:30:47,015 : INFO : torch.Size([168])
2017-05-09 00:30:47,015 : INFO : torch.Size([168, 168])
2017-05-09 00:30:47,015 : INFO : torch.Size([168])
2017-05-09 00:30:47,015 : INFO : torch.Size([168, 300])
2017-05-09 00:30:47,015 : INFO : torch.Size([168])
2017-05-09 00:30:47,015 : INFO : torch.Size([168, 168])
2017-05-09 00:30:47,016 : INFO : torch.Size([168])
2017-05-09 00:30:47,016 : INFO : torch.Size([3, 168])
2017-05-09 00:30:47,016 : INFO : torch.Size([3])
2017-05-09 00:30:47,016 : INFO : sum
2017-05-09 00:30:47,016 : INFO : 316347
2017-05-09 00:30:47,016 : INFO : ____________
2017-05-09 00:30:47,016 : INFO : ==> File found, loading to memory
2017-05-09 00:30:54,387 : INFO : ==> GLOVE vocabulary size: 2196016
2017-05-09 00:30:55,055 : INFO : done creating emb, quit
2017-05-09 00:30:55,055 : INFO : quit program due to memory leak during preprocess data, please rerun sentiment.py
2017-05-09 00:31:03,769 : INFO : LOG_FILE
2017-05-09 00:31:03,769 : INFO : _________________________________start___________________________________
2017-05-09 00:31:03,782 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:31:04,041 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:31:10,217 : INFO : _param count_
2017-05-09 00:31:10,219 : INFO : torch.Size([168, 300])
2017-05-09 00:31:10,220 : INFO : torch.Size([168])
2017-05-09 00:31:10,221 : INFO : torch.Size([168, 168])
2017-05-09 00:31:10,221 : INFO : torch.Size([168])
2017-05-09 00:31:10,222 : INFO : torch.Size([168, 168])
2017-05-09 00:31:10,222 : INFO : torch.Size([168])
2017-05-09 00:31:10,223 : INFO : torch.Size([168, 300])
2017-05-09 00:31:10,223 : INFO : torch.Size([168])
2017-05-09 00:31:10,224 : INFO : torch.Size([168, 300])
2017-05-09 00:31:10,224 : INFO : torch.Size([168])
2017-05-09 00:31:10,225 : INFO : torch.Size([168, 168])
2017-05-09 00:31:10,225 : INFO : torch.Size([168])
2017-05-09 00:31:10,226 : INFO : torch.Size([168, 300])
2017-05-09 00:31:10,227 : INFO : torch.Size([168])
2017-05-09 00:31:10,227 : INFO : torch.Size([168, 168])
2017-05-09 00:31:10,228 : INFO : torch.Size([168])
2017-05-09 00:31:10,228 : INFO : torch.Size([3, 168])
2017-05-09 00:31:10,229 : INFO : torch.Size([3])
2017-05-09 00:31:10,229 : INFO : sum
2017-05-09 00:31:10,230 : INFO : 316347
2017-05-09 00:31:10,230 : INFO : ____________
2017-05-09 00:33:19,309 : INFO : LOG_FILE
2017-05-09 00:33:19,309 : INFO : _________________________________start___________________________________
2017-05-09 00:33:19,322 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:33:19,579 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:33:26,570 : INFO : LOG_FILE
2017-05-09 00:33:26,570 : INFO : _________________________________start___________________________________
2017-05-09 00:33:26,579 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:33:26,608 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:33:39,652 : INFO : _param count_
2017-05-09 00:33:39,652 : INFO : torch.Size([168, 300])
2017-05-09 00:33:39,652 : INFO : torch.Size([168])
2017-05-09 00:33:39,653 : INFO : torch.Size([168, 168])
2017-05-09 00:33:39,653 : INFO : torch.Size([168])
2017-05-09 00:33:39,653 : INFO : torch.Size([168, 168])
2017-05-09 00:33:39,653 : INFO : torch.Size([168])
2017-05-09 00:33:39,653 : INFO : torch.Size([168, 300])
2017-05-09 00:33:39,653 : INFO : torch.Size([168])
2017-05-09 00:33:39,653 : INFO : torch.Size([168, 300])
2017-05-09 00:33:39,653 : INFO : torch.Size([168])
2017-05-09 00:33:39,654 : INFO : torch.Size([168, 168])
2017-05-09 00:33:39,654 : INFO : torch.Size([168])
2017-05-09 00:33:39,654 : INFO : torch.Size([168, 300])
2017-05-09 00:33:39,654 : INFO : torch.Size([168])
2017-05-09 00:33:39,654 : INFO : torch.Size([168, 168])
2017-05-09 00:33:39,654 : INFO : torch.Size([168])
2017-05-09 00:33:39,654 : INFO : torch.Size([3, 168])
2017-05-09 00:33:39,655 : INFO : torch.Size([3])
2017-05-09 00:33:39,655 : INFO : sum
2017-05-09 00:33:39,655 : INFO : 316347
2017-05-09 00:33:39,655 : INFO : ____________
2017-05-09 00:33:39,655 : INFO : ==> File found, loading to memory
2017-05-09 00:33:46,869 : INFO : ==> GLOVE vocabulary size: 2196016
2017-05-09 00:33:47,535 : INFO : done creating emb, quit
2017-05-09 00:33:47,535 : INFO : quit program due to memory leak during preprocess data, please rerun sentiment.py
2017-05-09 00:34:18,568 : INFO : LOG_FILE
2017-05-09 00:34:18,569 : INFO : _________________________________start___________________________________
2017-05-09 00:34:18,582 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:34:18,795 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:34:25,326 : INFO : _param count_
2017-05-09 00:34:25,329 : INFO : torch.Size([168, 300])
2017-05-09 00:34:25,329 : INFO : torch.Size([168])
2017-05-09 00:34:25,330 : INFO : torch.Size([168, 168])
2017-05-09 00:34:25,330 : INFO : torch.Size([168])
2017-05-09 00:34:25,331 : INFO : torch.Size([168, 168])
2017-05-09 00:34:25,332 : INFO : torch.Size([168])
2017-05-09 00:34:25,332 : INFO : torch.Size([168, 300])
2017-05-09 00:34:25,333 : INFO : torch.Size([168])
2017-05-09 00:34:25,333 : INFO : torch.Size([168, 300])
2017-05-09 00:34:25,334 : INFO : torch.Size([168])
2017-05-09 00:34:25,334 : INFO : torch.Size([168, 168])
2017-05-09 00:34:25,335 : INFO : torch.Size([168])
2017-05-09 00:34:25,336 : INFO : torch.Size([168, 300])
2017-05-09 00:34:25,336 : INFO : torch.Size([168])
2017-05-09 00:34:25,337 : INFO : torch.Size([168, 168])
2017-05-09 00:34:25,337 : INFO : torch.Size([168])
2017-05-09 00:34:25,338 : INFO : torch.Size([3, 168])
2017-05-09 00:34:25,338 : INFO : torch.Size([3])
2017-05-09 00:34:25,339 : INFO : sum
2017-05-09 00:34:25,339 : INFO : 316347
2017-05-09 00:34:25,340 : INFO : ____________
2017-05-09 00:34:38,351 : INFO : LOG_FILE
2017-05-09 00:34:38,352 : INFO : _________________________________start___________________________________
2017-05-09 00:34:38,365 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:34:38,584 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:34:44,687 : INFO : _param count_
2017-05-09 00:34:44,689 : INFO : torch.Size([168, 300])
2017-05-09 00:34:44,690 : INFO : torch.Size([168])
2017-05-09 00:34:44,690 : INFO : torch.Size([168, 168])
2017-05-09 00:34:44,691 : INFO : torch.Size([168])
2017-05-09 00:34:44,691 : INFO : torch.Size([168, 168])
2017-05-09 00:34:44,692 : INFO : torch.Size([168])
2017-05-09 00:34:44,693 : INFO : torch.Size([168, 300])
2017-05-09 00:34:44,693 : INFO : torch.Size([168])
2017-05-09 00:34:44,694 : INFO : torch.Size([168, 300])
2017-05-09 00:34:44,694 : INFO : torch.Size([168])
2017-05-09 00:34:44,695 : INFO : torch.Size([168, 168])
2017-05-09 00:34:44,695 : INFO : torch.Size([168])
2017-05-09 00:34:44,696 : INFO : torch.Size([168, 300])
2017-05-09 00:34:44,696 : INFO : torch.Size([168])
2017-05-09 00:34:44,697 : INFO : torch.Size([168, 168])
2017-05-09 00:34:44,697 : INFO : torch.Size([168])
2017-05-09 00:34:44,698 : INFO : torch.Size([3, 168])
2017-05-09 00:34:44,699 : INFO : torch.Size([3])
2017-05-09 00:34:44,699 : INFO : sum
2017-05-09 00:34:44,700 : INFO : 316347
2017-05-09 00:34:44,700 : INFO : ____________
2017-05-09 00:39:45,390 : INFO : LOG_FILE
2017-05-09 00:39:45,391 : INFO : _________________________________start___________________________________
2017-05-09 00:39:45,413 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:39:45,719 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:39:51,820 : INFO : _param count_
2017-05-09 00:39:51,823 : INFO : torch.Size([168, 300])
2017-05-09 00:39:51,823 : INFO : torch.Size([168])
2017-05-09 00:39:51,824 : INFO : torch.Size([168, 168])
2017-05-09 00:39:51,824 : INFO : torch.Size([168])
2017-05-09 00:39:51,825 : INFO : torch.Size([168, 168])
2017-05-09 00:39:51,825 : INFO : torch.Size([168])
2017-05-09 00:39:51,826 : INFO : torch.Size([168, 300])
2017-05-09 00:39:51,827 : INFO : torch.Size([168])
2017-05-09 00:39:51,827 : INFO : torch.Size([168, 300])
2017-05-09 00:39:51,828 : INFO : torch.Size([168])
2017-05-09 00:39:51,828 : INFO : torch.Size([168, 168])
2017-05-09 00:39:51,829 : INFO : torch.Size([168])
2017-05-09 00:39:51,829 : INFO : torch.Size([168, 300])
2017-05-09 00:39:51,830 : INFO : torch.Size([168])
2017-05-09 00:39:51,830 : INFO : torch.Size([168, 168])
2017-05-09 00:39:51,831 : INFO : torch.Size([168])
2017-05-09 00:39:51,832 : INFO : torch.Size([3, 168])
2017-05-09 00:39:51,832 : INFO : torch.Size([3])
2017-05-09 00:39:51,833 : INFO : sum
2017-05-09 00:39:51,833 : INFO : 316347
2017-05-09 00:39:51,834 : INFO : ____________
2017-05-09 00:49:00,269 : INFO : LOG_FILE
2017-05-09 00:49:00,270 : INFO : _________________________________start___________________________________
2017-05-09 00:49:00,276 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:49:00,305 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:49:05,981 : INFO : _param count_
2017-05-09 00:49:05,981 : INFO : torch.Size([168, 300])
2017-05-09 00:49:05,981 : INFO : torch.Size([168])
2017-05-09 00:49:05,982 : INFO : torch.Size([168, 168])
2017-05-09 00:49:05,982 : INFO : torch.Size([168])
2017-05-09 00:49:05,982 : INFO : torch.Size([168, 168])
2017-05-09 00:49:05,982 : INFO : torch.Size([168])
2017-05-09 00:49:05,982 : INFO : torch.Size([168, 300])
2017-05-09 00:49:05,982 : INFO : torch.Size([168])
2017-05-09 00:49:05,982 : INFO : torch.Size([168, 300])
2017-05-09 00:49:05,983 : INFO : torch.Size([168])
2017-05-09 00:49:05,983 : INFO : torch.Size([168, 168])
2017-05-09 00:49:05,983 : INFO : torch.Size([168])
2017-05-09 00:49:05,983 : INFO : torch.Size([168, 300])
2017-05-09 00:49:05,983 : INFO : torch.Size([168])
2017-05-09 00:49:05,983 : INFO : torch.Size([168, 168])
2017-05-09 00:49:05,983 : INFO : torch.Size([168])
2017-05-09 00:49:05,984 : INFO : torch.Size([3, 168])
2017-05-09 00:49:05,984 : INFO : torch.Size([3])
2017-05-09 00:49:05,984 : INFO : sum
2017-05-09 00:49:05,984 : INFO : 316347
2017-05-09 00:49:05,984 : INFO : ____________
2017-05-09 00:49:28,644 : INFO : LOG_FILE
2017-05-09 00:49:28,644 : INFO : _________________________________start___________________________________
2017-05-09 00:49:28,651 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:49:28,680 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:49:41,390 : INFO : _param count_
2017-05-09 00:49:41,390 : INFO : torch.Size([168, 300])
2017-05-09 00:49:41,390 : INFO : torch.Size([168])
2017-05-09 00:49:41,391 : INFO : torch.Size([168, 168])
2017-05-09 00:49:41,391 : INFO : torch.Size([168])
2017-05-09 00:49:41,391 : INFO : torch.Size([168, 168])
2017-05-09 00:49:41,391 : INFO : torch.Size([168])
2017-05-09 00:49:41,391 : INFO : torch.Size([168, 300])
2017-05-09 00:49:41,391 : INFO : torch.Size([168])
2017-05-09 00:49:41,391 : INFO : torch.Size([168, 300])
2017-05-09 00:49:41,392 : INFO : torch.Size([168])
2017-05-09 00:49:41,392 : INFO : torch.Size([168, 168])
2017-05-09 00:49:41,392 : INFO : torch.Size([168])
2017-05-09 00:49:41,392 : INFO : torch.Size([168, 300])
2017-05-09 00:49:41,392 : INFO : torch.Size([168])
2017-05-09 00:49:41,392 : INFO : torch.Size([168, 168])
2017-05-09 00:49:41,392 : INFO : torch.Size([168])
2017-05-09 00:49:41,392 : INFO : torch.Size([3, 168])
2017-05-09 00:49:41,393 : INFO : torch.Size([3])
2017-05-09 00:49:41,393 : INFO : sum
2017-05-09 00:49:41,393 : INFO : 316347
2017-05-09 00:49:41,393 : INFO : ____________
2017-05-09 00:49:41,393 : INFO : ==> File not found, preparing, be patient
2017-05-09 00:53:26,287 : INFO : ==> GLOVE vocabulary size: 2196016
2017-05-09 00:53:27,033 : INFO : done creating emb, quit
2017-05-09 00:53:27,033 : INFO : quit program due to memory leak during preprocess data, please rerun sentiment.py
2017-05-09 00:56:14,901 : INFO : LOG_FILE
2017-05-09 00:56:14,902 : INFO : _________________________________start___________________________________
2017-05-09 00:56:14,912 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:56:14,938 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:56:20,428 : INFO : _param count_
2017-05-09 00:56:20,429 : INFO : torch.Size([168, 300])
2017-05-09 00:56:20,429 : INFO : torch.Size([168])
2017-05-09 00:56:20,429 : INFO : torch.Size([168, 168])
2017-05-09 00:56:20,429 : INFO : torch.Size([168])
2017-05-09 00:56:20,429 : INFO : torch.Size([168, 168])
2017-05-09 00:56:20,429 : INFO : torch.Size([168])
2017-05-09 00:56:20,430 : INFO : torch.Size([168, 300])
2017-05-09 00:56:20,430 : INFO : torch.Size([168])
2017-05-09 00:56:20,430 : INFO : torch.Size([168, 300])
2017-05-09 00:56:20,430 : INFO : torch.Size([168])
2017-05-09 00:56:20,430 : INFO : torch.Size([168, 168])
2017-05-09 00:56:20,430 : INFO : torch.Size([168])
2017-05-09 00:56:20,430 : INFO : torch.Size([168, 300])
2017-05-09 00:56:20,430 : INFO : torch.Size([168])
2017-05-09 00:56:20,431 : INFO : torch.Size([168, 168])
2017-05-09 00:56:20,431 : INFO : torch.Size([168])
2017-05-09 00:56:20,431 : INFO : torch.Size([3, 168])
2017-05-09 00:56:20,431 : INFO : torch.Size([3])
2017-05-09 00:56:20,431 : INFO : sum
2017-05-09 00:56:20,431 : INFO : 316347
2017-05-09 00:56:20,431 : INFO : ____________
2017-05-09 00:56:31,326 : INFO : LOG_FILE
2017-05-09 00:56:31,326 : INFO : _________________________________start___________________________________
2017-05-09 00:56:31,332 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:56:31,361 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:56:43,791 : INFO : _param count_
2017-05-09 00:56:43,791 : INFO : torch.Size([168, 300])
2017-05-09 00:56:43,791 : INFO : torch.Size([168])
2017-05-09 00:56:43,791 : INFO : torch.Size([168, 168])
2017-05-09 00:56:43,791 : INFO : torch.Size([168])
2017-05-09 00:56:43,792 : INFO : torch.Size([168, 168])
2017-05-09 00:56:43,792 : INFO : torch.Size([168])
2017-05-09 00:56:43,792 : INFO : torch.Size([168, 300])
2017-05-09 00:56:43,792 : INFO : torch.Size([168])
2017-05-09 00:56:43,792 : INFO : torch.Size([168, 300])
2017-05-09 00:56:43,792 : INFO : torch.Size([168])
2017-05-09 00:56:43,792 : INFO : torch.Size([168, 168])
2017-05-09 00:56:43,793 : INFO : torch.Size([168])
2017-05-09 00:56:43,793 : INFO : torch.Size([168, 300])
2017-05-09 00:56:43,793 : INFO : torch.Size([168])
2017-05-09 00:56:43,793 : INFO : torch.Size([168, 168])
2017-05-09 00:56:43,793 : INFO : torch.Size([168])
2017-05-09 00:56:43,793 : INFO : torch.Size([3, 168])
2017-05-09 00:56:43,794 : INFO : torch.Size([3])
2017-05-09 00:56:43,794 : INFO : sum
2017-05-09 00:56:43,794 : INFO : 316347
2017-05-09 00:56:43,794 : INFO : ____________
2017-05-09 00:56:43,794 : INFO : ==> File found, loading to memory
2017-05-09 00:56:52,261 : INFO : ==> GLOVE vocabulary size: 2196016
2017-05-09 00:56:52,892 : INFO : done creating emb, quit
2017-05-09 00:56:52,892 : INFO : quit program due to memory leak during preprocess data, please rerun sentiment.py
2017-05-09 00:58:25,894 : INFO : LOG_FILE
2017-05-09 00:58:25,895 : INFO : _________________________________start___________________________________
2017-05-09 00:58:25,908 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 00:58:26,153 : INFO : ==> SST vocabulary size : 21701
2017-05-09 00:58:32,200 : INFO : _param count_
2017-05-09 00:58:32,203 : INFO : torch.Size([168, 300])
2017-05-09 00:58:32,203 : INFO : torch.Size([168])
2017-05-09 00:58:32,204 : INFO : torch.Size([168, 168])
2017-05-09 00:58:32,204 : INFO : torch.Size([168])
2017-05-09 00:58:32,205 : INFO : torch.Size([168, 168])
2017-05-09 00:58:32,206 : INFO : torch.Size([168])
2017-05-09 00:58:32,206 : INFO : torch.Size([168, 300])
2017-05-09 00:58:32,207 : INFO : torch.Size([168])
2017-05-09 00:58:32,207 : INFO : torch.Size([168, 300])
2017-05-09 00:58:32,208 : INFO : torch.Size([168])
2017-05-09 00:58:32,208 : INFO : torch.Size([168, 168])
2017-05-09 00:58:32,209 : INFO : torch.Size([168])
2017-05-09 00:58:32,210 : INFO : torch.Size([168, 300])
2017-05-09 00:58:32,210 : INFO : torch.Size([168])
2017-05-09 00:58:32,211 : INFO : torch.Size([168, 168])
2017-05-09 00:58:32,211 : INFO : torch.Size([168])
2017-05-09 00:58:32,212 : INFO : torch.Size([3, 168])
2017-05-09 00:58:32,212 : INFO : torch.Size([3])
2017-05-09 00:58:32,213 : INFO : sum
2017-05-09 00:58:32,213 : INFO : 316347
2017-05-09 00:58:32,214 : INFO : ____________
2017-05-09 09:54:54,032 : INFO : LOG_FILE
2017-05-09 09:54:54,032 : INFO : _________________________________start___________________________________
2017-05-09 09:54:54,039 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 09:54:54,040 : INFO : ==> SST vocabulary size : 105
2017-05-09 09:54:56,345 : INFO : _param count_
2017-05-09 09:54:56,345 : INFO : torch.Size([168, 300])
2017-05-09 09:54:56,346 : INFO : torch.Size([168])
2017-05-09 09:54:56,346 : INFO : torch.Size([168, 168])
2017-05-09 09:54:56,346 : INFO : torch.Size([168])
2017-05-09 09:54:56,346 : INFO : torch.Size([168, 168])
2017-05-09 09:54:56,346 : INFO : torch.Size([168])
2017-05-09 09:54:56,346 : INFO : torch.Size([168, 300])
2017-05-09 09:54:56,346 : INFO : torch.Size([168])
2017-05-09 09:54:56,347 : INFO : torch.Size([168, 300])
2017-05-09 09:54:56,347 : INFO : torch.Size([168])
2017-05-09 09:54:56,347 : INFO : torch.Size([168, 168])
2017-05-09 09:54:56,347 : INFO : torch.Size([168])
2017-05-09 09:54:56,347 : INFO : torch.Size([168, 300])
2017-05-09 09:54:56,347 : INFO : torch.Size([168])
2017-05-09 09:54:56,347 : INFO : torch.Size([168, 168])
2017-05-09 09:54:56,348 : INFO : torch.Size([168])
2017-05-09 09:54:56,348 : INFO : torch.Size([3, 168])
2017-05-09 09:54:56,348 : INFO : torch.Size([3])
2017-05-09 09:54:56,348 : INFO : sum
2017-05-09 09:54:56,348 : INFO : 316347
2017-05-09 09:54:56,348 : INFO : ____________
2017-05-09 09:54:56,348 : INFO : ==> File found, loading to memory
2017-05-09 09:55:03,897 : INFO : ==> GLOVE vocabulary size: 2196016
2017-05-09 09:55:03,901 : INFO : done creating emb, quit
2017-05-09 09:55:03,901 : INFO : quit program
2017-05-09 09:57:35,360 : INFO : LOG_FILE
2017-05-09 09:57:35,361 : INFO : _________________________________start___________________________________
2017-05-09 09:57:35,367 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 09:57:35,369 : INFO : ==> SST vocabulary size : 105
2017-05-09 09:57:37,520 : INFO : _param count_
2017-05-09 09:57:37,521 : INFO : torch.Size([168, 300])
2017-05-09 09:57:37,521 : INFO : torch.Size([168])
2017-05-09 09:57:37,521 : INFO : torch.Size([168, 168])
2017-05-09 09:57:37,521 : INFO : torch.Size([168])
2017-05-09 09:57:37,521 : INFO : torch.Size([168, 168])
2017-05-09 09:57:37,521 : INFO : torch.Size([168])
2017-05-09 09:57:37,521 : INFO : torch.Size([168, 300])
2017-05-09 09:57:37,522 : INFO : torch.Size([168])
2017-05-09 09:57:37,522 : INFO : torch.Size([168, 300])
2017-05-09 09:57:37,522 : INFO : torch.Size([168])
2017-05-09 09:57:37,522 : INFO : torch.Size([168, 168])
2017-05-09 09:57:37,522 : INFO : torch.Size([168])
2017-05-09 09:57:37,522 : INFO : torch.Size([168, 300])
2017-05-09 09:57:37,522 : INFO : torch.Size([168])
2017-05-09 09:57:37,523 : INFO : torch.Size([168, 168])
2017-05-09 09:57:37,523 : INFO : torch.Size([168])
2017-05-09 09:57:37,523 : INFO : torch.Size([3, 168])
2017-05-09 09:57:37,523 : INFO : torch.Size([3])
2017-05-09 09:57:37,523 : INFO : sum
2017-05-09 09:57:37,523 : INFO : 316347
2017-05-09 09:57:37,523 : INFO : ____________
2017-05-09 09:57:38,340 : INFO : ==> Train loss   : 1.114841
2017-05-09 09:57:38,340 : INFO : Epoch
2017-05-09 09:57:38,340 : INFO : 0
2017-05-09 09:57:38,340 : INFO : train percentage
2017-05-09 09:57:38,340 : INFO : 0.0
2017-05-09 09:57:38,340 : INFO : Epoch
2017-05-09 09:57:38,340 : INFO : 0
2017-05-09 09:57:38,341 : INFO : dev percentage
2017-05-09 09:57:38,341 : INFO : 0.0
2017-05-09 09:57:38,341 : INFO : Epoch
2017-05-09 09:57:38,341 : INFO : 0
2017-05-09 09:57:38,341 : INFO : test percentage
2017-05-09 09:57:38,341 : INFO : 0.0
2017-05-09 09:57:38,943 : INFO : ==> Train loss   : 1.114841
2017-05-09 09:57:38,943 : INFO : Epoch
2017-05-09 09:57:38,943 : INFO : 1
2017-05-09 09:57:38,943 : INFO : train percentage
2017-05-09 09:57:38,943 : INFO : 0.0
2017-05-09 09:57:38,943 : INFO : Epoch
2017-05-09 09:57:38,943 : INFO : 1
2017-05-09 09:57:38,943 : INFO : dev percentage
2017-05-09 09:57:38,944 : INFO : 0.0
2017-05-09 09:57:38,944 : INFO : Epoch
2017-05-09 09:57:38,944 : INFO : 1
2017-05-09 09:57:38,944 : INFO : test percentage
2017-05-09 09:57:38,944 : INFO : 0.0
2017-05-09 09:57:39,561 : INFO : ==> Train loss   : 1.114841
2017-05-09 09:57:39,562 : INFO : Epoch
2017-05-09 09:57:39,562 : INFO : 2
2017-05-09 09:57:39,562 : INFO : train percentage
2017-05-09 09:57:39,562 : INFO : 0.0
2017-05-09 09:57:39,562 : INFO : Epoch
2017-05-09 09:57:39,562 : INFO : 2
2017-05-09 09:57:39,562 : INFO : dev percentage
2017-05-09 09:57:39,562 : INFO : 0.0
2017-05-09 09:57:39,562 : INFO : Epoch
2017-05-09 09:57:39,563 : INFO : 2
2017-05-09 09:57:39,563 : INFO : test percentage
2017-05-09 09:57:39,563 : INFO : 0.0
2017-05-09 09:57:40,178 : INFO : ==> Train loss   : 1.114841
2017-05-09 09:57:40,179 : INFO : Epoch
2017-05-09 09:57:40,179 : INFO : 3
2017-05-09 09:57:40,179 : INFO : train percentage
2017-05-09 09:57:40,179 : INFO : 0.0
2017-05-09 09:57:40,179 : INFO : Epoch
2017-05-09 09:57:40,179 : INFO : 3
2017-05-09 09:57:40,179 : INFO : dev percentage
2017-05-09 09:57:40,179 : INFO : 0.0
2017-05-09 09:57:40,180 : INFO : Epoch
2017-05-09 09:57:40,180 : INFO : 3
2017-05-09 09:57:40,180 : INFO : test percentage
2017-05-09 09:57:40,180 : INFO : 0.0
2017-05-09 09:57:40,787 : INFO : ==> Train loss   : 1.114841
2017-05-09 09:57:40,787 : INFO : Epoch
2017-05-09 09:57:40,787 : INFO : 4
2017-05-09 09:57:40,788 : INFO : train percentage
2017-05-09 09:57:40,788 : INFO : 0.0
2017-05-09 09:57:40,788 : INFO : Epoch
2017-05-09 09:57:40,788 : INFO : 4
2017-05-09 09:57:40,788 : INFO : dev percentage
2017-05-09 09:57:40,788 : INFO : 0.0
2017-05-09 09:57:40,788 : INFO : Epoch
2017-05-09 09:57:40,788 : INFO : 4
2017-05-09 09:57:40,789 : INFO : test percentage
2017-05-09 09:57:40,789 : INFO : 0.0
2017-05-09 10:01:35,763 : INFO : LOG_FILE
2017-05-09 10:01:35,763 : INFO : _________________________________start___________________________________
2017-05-09 10:01:35,770 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:01:35,771 : INFO : ==> SST vocabulary size : 105
2017-05-09 10:01:37,937 : INFO : _param count_
2017-05-09 10:01:37,938 : INFO : torch.Size([168, 300])
2017-05-09 10:01:37,938 : INFO : torch.Size([168])
2017-05-09 10:01:37,938 : INFO : torch.Size([168, 168])
2017-05-09 10:01:37,938 : INFO : torch.Size([168])
2017-05-09 10:01:37,938 : INFO : torch.Size([168, 168])
2017-05-09 10:01:37,938 : INFO : torch.Size([168])
2017-05-09 10:01:37,938 : INFO : torch.Size([168, 300])
2017-05-09 10:01:37,939 : INFO : torch.Size([168])
2017-05-09 10:01:37,939 : INFO : torch.Size([168, 300])
2017-05-09 10:01:37,939 : INFO : torch.Size([168])
2017-05-09 10:01:37,939 : INFO : torch.Size([168, 168])
2017-05-09 10:01:37,939 : INFO : torch.Size([168])
2017-05-09 10:01:37,939 : INFO : torch.Size([168, 300])
2017-05-09 10:01:37,939 : INFO : torch.Size([168])
2017-05-09 10:01:37,940 : INFO : torch.Size([168, 168])
2017-05-09 10:01:37,940 : INFO : torch.Size([168])
2017-05-09 10:01:37,940 : INFO : torch.Size([3, 168])
2017-05-09 10:01:37,940 : INFO : torch.Size([3])
2017-05-09 10:01:37,940 : INFO : sum
2017-05-09 10:01:37,940 : INFO : 316347
2017-05-09 10:01:37,940 : INFO : ____________
2017-05-09 10:01:38,729 : INFO : ==> Train loss   : 1.114841
2017-05-09 10:01:38,729 : INFO : Epoch
2017-05-09 10:01:38,729 : INFO : 0
2017-05-09 10:01:38,730 : INFO : train percentage
2017-05-09 10:01:38,730 : INFO : 0.0
2017-05-09 10:01:38,730 : INFO : Epoch
2017-05-09 10:01:38,730 : INFO : 0
2017-05-09 10:01:38,730 : INFO : dev percentage
2017-05-09 10:01:38,730 : INFO : 0.0
2017-05-09 10:01:38,730 : INFO : Epoch
2017-05-09 10:01:38,730 : INFO : 0
2017-05-09 10:01:38,730 : INFO : test percentage
2017-05-09 10:01:38,731 : INFO : 0.0
2017-05-09 10:01:39,340 : INFO : ==> Train loss   : 1.114841
2017-05-09 10:01:39,340 : INFO : Epoch
2017-05-09 10:01:39,340 : INFO : 1
2017-05-09 10:01:39,340 : INFO : train percentage
2017-05-09 10:01:39,340 : INFO : 0.0
2017-05-09 10:01:39,341 : INFO : Epoch
2017-05-09 10:01:39,341 : INFO : 1
2017-05-09 10:01:39,341 : INFO : dev percentage
2017-05-09 10:01:39,341 : INFO : 0.0
2017-05-09 10:01:39,341 : INFO : Epoch
2017-05-09 10:01:39,341 : INFO : 1
2017-05-09 10:01:39,341 : INFO : test percentage
2017-05-09 10:01:39,341 : INFO : 0.0
2017-05-09 10:01:39,973 : INFO : ==> Train loss   : 1.114841
2017-05-09 10:01:39,973 : INFO : Epoch
2017-05-09 10:01:39,973 : INFO : 2
2017-05-09 10:01:39,973 : INFO : train percentage
2017-05-09 10:01:39,973 : INFO : 0.0
2017-05-09 10:01:39,973 : INFO : Epoch
2017-05-09 10:01:39,974 : INFO : 2
2017-05-09 10:01:39,974 : INFO : dev percentage
2017-05-09 10:01:39,974 : INFO : 0.0
2017-05-09 10:01:39,974 : INFO : Epoch
2017-05-09 10:01:39,974 : INFO : 2
2017-05-09 10:01:39,974 : INFO : test percentage
2017-05-09 10:01:39,974 : INFO : 0.0
2017-05-09 10:01:40,581 : INFO : ==> Train loss   : 1.114841
2017-05-09 10:01:40,582 : INFO : Epoch
2017-05-09 10:01:40,582 : INFO : 3
2017-05-09 10:01:40,582 : INFO : train percentage
2017-05-09 10:01:40,582 : INFO : 0.0
2017-05-09 10:01:40,582 : INFO : Epoch
2017-05-09 10:01:40,582 : INFO : 3
2017-05-09 10:01:40,582 : INFO : dev percentage
2017-05-09 10:01:40,583 : INFO : 0.0
2017-05-09 10:01:40,583 : INFO : Epoch
2017-05-09 10:01:40,583 : INFO : 3
2017-05-09 10:01:40,583 : INFO : test percentage
2017-05-09 10:01:40,583 : INFO : 0.0
2017-05-09 10:01:53,088 : INFO : LOG_FILE
2017-05-09 10:01:53,088 : INFO : _________________________________start___________________________________
2017-05-09 10:01:53,095 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:01:53,096 : INFO : ==> SST vocabulary size : 105
2017-05-09 10:01:55,270 : INFO : _param count_
2017-05-09 10:01:55,271 : INFO : torch.Size([168, 300])
2017-05-09 10:01:55,271 : INFO : torch.Size([168])
2017-05-09 10:01:55,271 : INFO : torch.Size([168, 168])
2017-05-09 10:01:55,271 : INFO : torch.Size([168])
2017-05-09 10:01:55,271 : INFO : torch.Size([168, 168])
2017-05-09 10:01:55,271 : INFO : torch.Size([168])
2017-05-09 10:01:55,271 : INFO : torch.Size([168, 300])
2017-05-09 10:01:55,272 : INFO : torch.Size([168])
2017-05-09 10:01:55,272 : INFO : torch.Size([168, 300])
2017-05-09 10:01:55,272 : INFO : torch.Size([168])
2017-05-09 10:01:55,272 : INFO : torch.Size([168, 168])
2017-05-09 10:01:55,272 : INFO : torch.Size([168])
2017-05-09 10:01:55,272 : INFO : torch.Size([168, 300])
2017-05-09 10:01:55,272 : INFO : torch.Size([168])
2017-05-09 10:01:55,272 : INFO : torch.Size([168, 168])
2017-05-09 10:01:55,273 : INFO : torch.Size([168])
2017-05-09 10:01:55,273 : INFO : torch.Size([3, 168])
2017-05-09 10:01:55,273 : INFO : torch.Size([3])
2017-05-09 10:01:55,273 : INFO : sum
2017-05-09 10:01:55,273 : INFO : 316347
2017-05-09 10:01:55,273 : INFO : ____________
2017-05-09 10:01:55,273 : INFO : ==> File found, loading to memory
2017-05-09 10:02:02,038 : INFO : ==> GLOVE vocabulary size: 2196016
2017-05-09 10:02:02,041 : INFO : done creating emb, quit
2017-05-09 10:02:02,041 : INFO : quit program
2017-05-09 10:02:42,844 : INFO : LOG_FILE
2017-05-09 10:02:42,844 : INFO : _________________________________start___________________________________
2017-05-09 10:02:42,851 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:02:42,852 : INFO : ==> SST vocabulary size : 105
2017-05-09 10:02:45,009 : INFO : _param count_
2017-05-09 10:02:45,009 : INFO : torch.Size([168, 300])
2017-05-09 10:02:45,009 : INFO : torch.Size([168])
2017-05-09 10:02:45,009 : INFO : torch.Size([168, 168])
2017-05-09 10:02:45,009 : INFO : torch.Size([168])
2017-05-09 10:02:45,010 : INFO : torch.Size([168, 168])
2017-05-09 10:02:45,010 : INFO : torch.Size([168])
2017-05-09 10:02:45,010 : INFO : torch.Size([168, 300])
2017-05-09 10:02:45,010 : INFO : torch.Size([168])
2017-05-09 10:02:45,010 : INFO : torch.Size([168, 300])
2017-05-09 10:02:45,010 : INFO : torch.Size([168])
2017-05-09 10:02:45,010 : INFO : torch.Size([168, 168])
2017-05-09 10:02:45,011 : INFO : torch.Size([168])
2017-05-09 10:02:45,011 : INFO : torch.Size([168, 300])
2017-05-09 10:02:45,011 : INFO : torch.Size([168])
2017-05-09 10:02:45,011 : INFO : torch.Size([168, 168])
2017-05-09 10:02:45,011 : INFO : torch.Size([168])
2017-05-09 10:02:45,011 : INFO : torch.Size([3, 168])
2017-05-09 10:02:45,011 : INFO : torch.Size([3])
2017-05-09 10:02:45,011 : INFO : sum
2017-05-09 10:02:45,012 : INFO : 316347
2017-05-09 10:02:45,012 : INFO : ____________
2017-05-09 10:02:45,012 : INFO : ==> File found, loading to memory
2017-05-09 10:02:51,714 : INFO : ==> GLOVE vocabulary size: 2196016
2017-05-09 10:02:51,716 : INFO : done creating emb, quit
2017-05-09 10:02:51,716 : INFO : quit program
2017-05-09 10:03:21,738 : INFO : LOG_FILE
2017-05-09 10:03:21,739 : INFO : _________________________________start___________________________________
2017-05-09 10:03:21,746 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:03:21,748 : INFO : ==> SST vocabulary size : 105
2017-05-09 10:03:23,968 : INFO : _param count_
2017-05-09 10:03:23,969 : INFO : torch.Size([168, 300])
2017-05-09 10:03:23,969 : INFO : torch.Size([168])
2017-05-09 10:03:23,969 : INFO : torch.Size([168, 168])
2017-05-09 10:03:23,969 : INFO : torch.Size([168])
2017-05-09 10:03:23,969 : INFO : torch.Size([168, 168])
2017-05-09 10:03:23,969 : INFO : torch.Size([168])
2017-05-09 10:03:23,969 : INFO : torch.Size([168, 300])
2017-05-09 10:03:23,970 : INFO : torch.Size([168])
2017-05-09 10:03:23,970 : INFO : torch.Size([168, 300])
2017-05-09 10:03:23,970 : INFO : torch.Size([168])
2017-05-09 10:03:23,970 : INFO : torch.Size([168, 168])
2017-05-09 10:03:23,970 : INFO : torch.Size([168])
2017-05-09 10:03:23,970 : INFO : torch.Size([168, 300])
2017-05-09 10:03:23,970 : INFO : torch.Size([168])
2017-05-09 10:03:23,971 : INFO : torch.Size([168, 168])
2017-05-09 10:03:23,971 : INFO : torch.Size([168])
2017-05-09 10:03:23,971 : INFO : torch.Size([3, 168])
2017-05-09 10:03:23,971 : INFO : torch.Size([3])
2017-05-09 10:03:23,971 : INFO : sum
2017-05-09 10:03:23,971 : INFO : 316347
2017-05-09 10:03:23,971 : INFO : ____________
2017-05-09 10:03:23,971 : INFO : ==> File found, loading to memory
2017-05-09 10:03:30,640 : INFO : ==> GLOVE vocabulary size: 2196016
2017-05-09 10:03:30,643 : INFO : done creating emb, quit
2017-05-09 10:03:30,643 : INFO : quit program
2017-05-09 10:06:24,531 : INFO : LOG_FILE
2017-05-09 10:06:24,531 : INFO : _________________________________start___________________________________
2017-05-09 10:06:24,545 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:06:24,548 : INFO : ==> SST vocabulary size : 105
2017-05-09 10:06:26,706 : INFO : _param count_
2017-05-09 10:06:26,709 : INFO : torch.Size([168, 300])
2017-05-09 10:06:26,709 : INFO : torch.Size([168])
2017-05-09 10:06:26,710 : INFO : torch.Size([168, 168])
2017-05-09 10:06:26,710 : INFO : torch.Size([168])
2017-05-09 10:06:26,711 : INFO : torch.Size([168, 168])
2017-05-09 10:06:26,712 : INFO : torch.Size([168])
2017-05-09 10:06:26,712 : INFO : torch.Size([168, 300])
2017-05-09 10:06:26,713 : INFO : torch.Size([168])
2017-05-09 10:06:26,713 : INFO : torch.Size([168, 300])
2017-05-09 10:06:26,714 : INFO : torch.Size([168])
2017-05-09 10:06:26,714 : INFO : torch.Size([168, 168])
2017-05-09 10:06:26,715 : INFO : torch.Size([168])
2017-05-09 10:06:26,716 : INFO : torch.Size([168, 300])
2017-05-09 10:06:26,716 : INFO : torch.Size([168])
2017-05-09 10:06:26,717 : INFO : torch.Size([168, 168])
2017-05-09 10:06:26,717 : INFO : torch.Size([168])
2017-05-09 10:06:26,718 : INFO : torch.Size([3, 168])
2017-05-09 10:06:26,718 : INFO : torch.Size([3])
2017-05-09 10:06:26,719 : INFO : sum
2017-05-09 10:06:26,719 : INFO : 316347
2017-05-09 10:06:26,720 : INFO : ____________
2017-05-09 10:06:45,657 : INFO : LOG_FILE
2017-05-09 10:06:45,658 : INFO : _________________________________start___________________________________
2017-05-09 10:06:45,670 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:06:45,673 : INFO : ==> SST vocabulary size : 105
2017-05-09 10:06:47,812 : INFO : _param count_
2017-05-09 10:06:47,814 : INFO : torch.Size([168, 300])
2017-05-09 10:06:47,815 : INFO : torch.Size([168])
2017-05-09 10:06:47,816 : INFO : torch.Size([168, 168])
2017-05-09 10:06:47,816 : INFO : torch.Size([168])
2017-05-09 10:06:47,817 : INFO : torch.Size([168, 168])
2017-05-09 10:06:47,817 : INFO : torch.Size([168])
2017-05-09 10:06:47,818 : INFO : torch.Size([168, 300])
2017-05-09 10:06:47,818 : INFO : torch.Size([168])
2017-05-09 10:06:47,819 : INFO : torch.Size([168, 300])
2017-05-09 10:06:47,819 : INFO : torch.Size([168])
2017-05-09 10:06:47,820 : INFO : torch.Size([168, 168])
2017-05-09 10:06:47,821 : INFO : torch.Size([168])
2017-05-09 10:06:47,821 : INFO : torch.Size([168, 300])
2017-05-09 10:06:47,822 : INFO : torch.Size([168])
2017-05-09 10:06:47,822 : INFO : torch.Size([168, 168])
2017-05-09 10:06:47,823 : INFO : torch.Size([168])
2017-05-09 10:06:47,823 : INFO : torch.Size([3, 168])
2017-05-09 10:06:47,824 : INFO : torch.Size([3])
2017-05-09 10:06:47,824 : INFO : sum
2017-05-09 10:06:47,825 : INFO : 316347
2017-05-09 10:06:47,825 : INFO : ____________
2017-05-09 10:12:48,341 : INFO : LOG_FILE
2017-05-09 10:12:48,341 : INFO : _________________________________start___________________________________
2017-05-09 10:12:48,354 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:12:48,358 : INFO : ==> SST vocabulary size : 105
2017-05-09 10:12:50,584 : INFO : _param count_
2017-05-09 10:12:50,587 : INFO : torch.Size([168, 300])
2017-05-09 10:12:50,588 : INFO : torch.Size([168])
2017-05-09 10:12:50,588 : INFO : torch.Size([168, 168])
2017-05-09 10:12:50,589 : INFO : torch.Size([168])
2017-05-09 10:12:50,589 : INFO : torch.Size([168, 168])
2017-05-09 10:12:50,590 : INFO : torch.Size([168])
2017-05-09 10:12:50,590 : INFO : torch.Size([168, 300])
2017-05-09 10:12:50,591 : INFO : torch.Size([168])
2017-05-09 10:12:50,591 : INFO : torch.Size([168, 300])
2017-05-09 10:12:50,592 : INFO : torch.Size([168])
2017-05-09 10:12:50,592 : INFO : torch.Size([168, 168])
2017-05-09 10:12:50,593 : INFO : torch.Size([168])
2017-05-09 10:12:50,594 : INFO : torch.Size([168, 300])
2017-05-09 10:12:50,594 : INFO : torch.Size([168])
2017-05-09 10:12:50,595 : INFO : torch.Size([168, 168])
2017-05-09 10:12:50,595 : INFO : torch.Size([168])
2017-05-09 10:12:50,596 : INFO : torch.Size([3, 168])
2017-05-09 10:12:50,596 : INFO : torch.Size([3])
2017-05-09 10:12:50,597 : INFO : sum
2017-05-09 10:12:50,597 : INFO : 316347
2017-05-09 10:12:50,598 : INFO : ____________
2017-05-09 10:21:28,369 : INFO : LOG_FILE
2017-05-09 10:21:28,370 : INFO : _________________________________start___________________________________
2017-05-09 10:21:28,382 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:21:28,387 : INFO : ==> SST vocabulary size : 105
2017-05-09 10:21:30,721 : INFO : _param count_
2017-05-09 10:21:30,724 : INFO : torch.Size([168, 300])
2017-05-09 10:21:30,724 : INFO : torch.Size([168])
2017-05-09 10:21:30,725 : INFO : torch.Size([168, 168])
2017-05-09 10:21:30,725 : INFO : torch.Size([168])
2017-05-09 10:21:30,726 : INFO : torch.Size([168, 168])
2017-05-09 10:21:30,727 : INFO : torch.Size([168])
2017-05-09 10:21:30,727 : INFO : torch.Size([168, 300])
2017-05-09 10:21:30,728 : INFO : torch.Size([168])
2017-05-09 10:21:30,728 : INFO : torch.Size([168, 300])
2017-05-09 10:21:30,729 : INFO : torch.Size([168])
2017-05-09 10:21:30,730 : INFO : torch.Size([168, 168])
2017-05-09 10:21:30,730 : INFO : torch.Size([168])
2017-05-09 10:21:30,731 : INFO : torch.Size([168, 300])
2017-05-09 10:21:30,732 : INFO : torch.Size([168])
2017-05-09 10:21:30,733 : INFO : torch.Size([168, 168])
2017-05-09 10:21:30,734 : INFO : torch.Size([168])
2017-05-09 10:21:30,735 : INFO : torch.Size([3, 168])
2017-05-09 10:21:30,736 : INFO : torch.Size([3])
2017-05-09 10:21:30,737 : INFO : sum
2017-05-09 10:21:30,737 : INFO : 316347
2017-05-09 10:21:30,738 : INFO : ____________
2017-05-09 10:29:19,396 : INFO : LOG_FILE
2017-05-09 10:29:19,396 : INFO : _________________________________start___________________________________
2017-05-09 10:29:19,412 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:29:19,416 : INFO : ==> SST vocabulary size : 105
2017-05-09 10:29:21,646 : INFO : _param count_
2017-05-09 10:29:21,648 : INFO : torch.Size([168, 300])
2017-05-09 10:29:21,649 : INFO : torch.Size([168])
2017-05-09 10:29:21,649 : INFO : torch.Size([168, 168])
2017-05-09 10:29:21,650 : INFO : torch.Size([168])
2017-05-09 10:29:21,650 : INFO : torch.Size([168, 168])
2017-05-09 10:29:21,651 : INFO : torch.Size([168])
2017-05-09 10:29:21,651 : INFO : torch.Size([168, 300])
2017-05-09 10:29:21,652 : INFO : torch.Size([168])
2017-05-09 10:29:21,653 : INFO : torch.Size([168, 300])
2017-05-09 10:29:21,653 : INFO : torch.Size([168])
2017-05-09 10:29:21,654 : INFO : torch.Size([168, 168])
2017-05-09 10:29:21,654 : INFO : torch.Size([168])
2017-05-09 10:29:21,655 : INFO : torch.Size([168, 300])
2017-05-09 10:29:21,655 : INFO : torch.Size([168])
2017-05-09 10:29:21,656 : INFO : torch.Size([168, 168])
2017-05-09 10:29:21,656 : INFO : torch.Size([168])
2017-05-09 10:29:21,657 : INFO : torch.Size([3, 168])
2017-05-09 10:29:21,658 : INFO : torch.Size([3])
2017-05-09 10:29:21,658 : INFO : sum
2017-05-09 10:29:21,659 : INFO : 316347
2017-05-09 10:29:21,659 : INFO : ____________
2017-05-09 10:47:49,285 : INFO : LOG_FILE
2017-05-09 10:47:49,285 : INFO : _________________________________start___________________________________
2017-05-09 10:47:49,291 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:47:49,293 : INFO : ==> SST vocabulary size : 105
2017-05-09 10:47:51,528 : INFO : _param count_
2017-05-09 10:47:51,529 : INFO : torch.Size([168, 300])
2017-05-09 10:47:51,529 : INFO : torch.Size([168])
2017-05-09 10:47:51,529 : INFO : torch.Size([168, 168])
2017-05-09 10:47:51,529 : INFO : torch.Size([168])
2017-05-09 10:47:51,529 : INFO : torch.Size([168, 168])
2017-05-09 10:47:51,529 : INFO : torch.Size([168])
2017-05-09 10:47:51,530 : INFO : torch.Size([168, 300])
2017-05-09 10:47:51,530 : INFO : torch.Size([168])
2017-05-09 10:47:51,530 : INFO : torch.Size([168, 300])
2017-05-09 10:47:51,530 : INFO : torch.Size([168])
2017-05-09 10:47:51,530 : INFO : torch.Size([168, 168])
2017-05-09 10:47:51,530 : INFO : torch.Size([168])
2017-05-09 10:47:51,530 : INFO : torch.Size([168, 300])
2017-05-09 10:47:51,531 : INFO : torch.Size([168])
2017-05-09 10:47:51,531 : INFO : torch.Size([168, 168])
2017-05-09 10:47:51,531 : INFO : torch.Size([168])
2017-05-09 10:47:51,531 : INFO : torch.Size([3, 168])
2017-05-09 10:47:51,531 : INFO : torch.Size([3])
2017-05-09 10:47:51,531 : INFO : sum
2017-05-09 10:47:51,531 : INFO : 316347
2017-05-09 10:47:51,532 : INFO : ____________
2017-05-09 10:47:51,532 : INFO : ==> File found, loading to memory
2017-05-09 10:47:57,696 : INFO : ==> GLOVE vocabulary size: 2196016
2017-05-09 10:47:57,698 : INFO : done creating emb, quit
2017-05-09 10:47:57,698 : INFO : quit program
2017-05-09 10:48:00,839 : INFO : LOG_FILE
2017-05-09 10:48:00,840 : INFO : _________________________________start___________________________________
2017-05-09 10:48:00,852 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:48:00,855 : INFO : ==> SST vocabulary size : 105
2017-05-09 10:48:03,104 : INFO : _param count_
2017-05-09 10:48:03,107 : INFO : torch.Size([168, 300])
2017-05-09 10:48:03,107 : INFO : torch.Size([168])
2017-05-09 10:48:03,108 : INFO : torch.Size([168, 168])
2017-05-09 10:48:03,108 : INFO : torch.Size([168])
2017-05-09 10:48:03,109 : INFO : torch.Size([168, 168])
2017-05-09 10:48:03,109 : INFO : torch.Size([168])
2017-05-09 10:48:03,110 : INFO : torch.Size([168, 300])
2017-05-09 10:48:03,111 : INFO : torch.Size([168])
2017-05-09 10:48:03,111 : INFO : torch.Size([168, 300])
2017-05-09 10:48:03,112 : INFO : torch.Size([168])
2017-05-09 10:48:03,112 : INFO : torch.Size([168, 168])
2017-05-09 10:48:03,113 : INFO : torch.Size([168])
2017-05-09 10:48:03,113 : INFO : torch.Size([168, 300])
2017-05-09 10:48:03,114 : INFO : torch.Size([168])
2017-05-09 10:48:03,114 : INFO : torch.Size([168, 168])
2017-05-09 10:48:03,115 : INFO : torch.Size([168])
2017-05-09 10:48:03,115 : INFO : torch.Size([3, 168])
2017-05-09 10:48:03,116 : INFO : torch.Size([3])
2017-05-09 10:48:03,116 : INFO : sum
2017-05-09 10:48:03,117 : INFO : 316347
2017-05-09 10:48:03,117 : INFO : ____________
2017-05-09 10:51:27,381 : INFO : LOG_FILE
2017-05-09 10:51:27,381 : INFO : _________________________________start___________________________________
2017-05-09 10:51:27,394 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:51:27,398 : INFO : ==> SST vocabulary size : 105
2017-05-09 10:51:29,563 : INFO : _param count_
2017-05-09 10:51:29,566 : INFO : torch.Size([168, 300])
2017-05-09 10:51:29,566 : INFO : torch.Size([168])
2017-05-09 10:51:29,567 : INFO : torch.Size([168, 168])
2017-05-09 10:51:29,568 : INFO : torch.Size([168])
2017-05-09 10:51:29,568 : INFO : torch.Size([168, 168])
2017-05-09 10:51:29,569 : INFO : torch.Size([168])
2017-05-09 10:51:29,569 : INFO : torch.Size([168, 300])
2017-05-09 10:51:29,570 : INFO : torch.Size([168])
2017-05-09 10:51:29,570 : INFO : torch.Size([168, 300])
2017-05-09 10:51:29,571 : INFO : torch.Size([168])
2017-05-09 10:51:29,571 : INFO : torch.Size([168, 168])
2017-05-09 10:51:29,572 : INFO : torch.Size([168])
2017-05-09 10:51:29,573 : INFO : torch.Size([168, 300])
2017-05-09 10:51:29,573 : INFO : torch.Size([168])
2017-05-09 10:51:29,574 : INFO : torch.Size([168, 168])
2017-05-09 10:51:29,574 : INFO : torch.Size([168])
2017-05-09 10:51:29,575 : INFO : torch.Size([3, 168])
2017-05-09 10:51:29,575 : INFO : torch.Size([3])
2017-05-09 10:51:29,576 : INFO : sum
2017-05-09 10:51:29,576 : INFO : 316347
2017-05-09 10:51:29,577 : INFO : ____________
2017-05-09 10:53:29,185 : INFO : LOG_FILE
2017-05-09 10:53:29,186 : INFO : _________________________________start___________________________________
2017-05-09 10:53:29,199 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:53:29,202 : INFO : ==> SST vocabulary size : 105
2017-05-09 10:53:31,361 : INFO : _param count_
2017-05-09 10:53:31,363 : INFO : torch.Size([168, 300])
2017-05-09 10:53:31,364 : INFO : torch.Size([168])
2017-05-09 10:53:31,364 : INFO : torch.Size([168, 168])
2017-05-09 10:53:31,365 : INFO : torch.Size([168])
2017-05-09 10:53:31,365 : INFO : torch.Size([168, 168])
2017-05-09 10:53:31,366 : INFO : torch.Size([168])
2017-05-09 10:53:31,366 : INFO : torch.Size([168, 300])
2017-05-09 10:53:31,367 : INFO : torch.Size([168])
2017-05-09 10:53:31,368 : INFO : torch.Size([168, 300])
2017-05-09 10:53:31,368 : INFO : torch.Size([168])
2017-05-09 10:53:31,369 : INFO : torch.Size([168, 168])
2017-05-09 10:53:31,369 : INFO : torch.Size([168])
2017-05-09 10:53:31,370 : INFO : torch.Size([168, 300])
2017-05-09 10:53:31,370 : INFO : torch.Size([168])
2017-05-09 10:53:31,371 : INFO : torch.Size([168, 168])
2017-05-09 10:53:31,371 : INFO : torch.Size([168])
2017-05-09 10:53:31,372 : INFO : torch.Size([3, 168])
2017-05-09 10:53:31,372 : INFO : torch.Size([3])
2017-05-09 10:53:31,373 : INFO : sum
2017-05-09 10:53:31,374 : INFO : 316347
2017-05-09 10:53:31,374 : INFO : ____________
2017-05-09 10:53:39,503 : INFO : LOG_FILE
2017-05-09 10:53:39,503 : INFO : _________________________________start___________________________________
2017-05-09 10:53:39,517 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:53:39,520 : INFO : ==> SST vocabulary size : 105
2017-05-09 10:53:41,763 : INFO : _param count_
2017-05-09 10:53:41,765 : INFO : torch.Size([168, 300])
2017-05-09 10:53:41,766 : INFO : torch.Size([168])
2017-05-09 10:53:41,766 : INFO : torch.Size([168, 168])
2017-05-09 10:53:41,767 : INFO : torch.Size([168])
2017-05-09 10:53:41,768 : INFO : torch.Size([168, 168])
2017-05-09 10:53:41,768 : INFO : torch.Size([168])
2017-05-09 10:53:41,769 : INFO : torch.Size([168, 300])
2017-05-09 10:53:41,769 : INFO : torch.Size([168])
2017-05-09 10:53:41,770 : INFO : torch.Size([168, 300])
2017-05-09 10:53:41,770 : INFO : torch.Size([168])
2017-05-09 10:53:41,771 : INFO : torch.Size([168, 168])
2017-05-09 10:53:41,771 : INFO : torch.Size([168])
2017-05-09 10:53:41,772 : INFO : torch.Size([168, 300])
2017-05-09 10:53:41,772 : INFO : torch.Size([168])
2017-05-09 10:53:41,773 : INFO : torch.Size([168, 168])
2017-05-09 10:53:41,774 : INFO : torch.Size([168])
2017-05-09 10:53:41,774 : INFO : torch.Size([3, 168])
2017-05-09 10:53:41,775 : INFO : torch.Size([3])
2017-05-09 10:53:41,775 : INFO : sum
2017-05-09 10:53:41,776 : INFO : 316347
2017-05-09 10:53:41,776 : INFO : ____________
2017-05-09 10:54:37,247 : INFO : dog
2017-05-09 10:56:56,572 : INFO : LOG_FILE
2017-05-09 10:56:56,572 : INFO : _________________________________start___________________________________
2017-05-09 10:56:56,585 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 10:56:56,588 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:12:55,692 : INFO : LOG_FILE
2017-05-09 11:12:55,693 : INFO : _________________________________start___________________________________
2017-05-09 11:12:55,699 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:12:55,700 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:12:57,933 : INFO : _param count_
2017-05-09 11:12:57,933 : INFO : torch.Size([168, 300])
2017-05-09 11:12:57,933 : INFO : torch.Size([168])
2017-05-09 11:12:57,933 : INFO : torch.Size([168, 168])
2017-05-09 11:12:57,933 : INFO : torch.Size([168])
2017-05-09 11:12:57,934 : INFO : torch.Size([168, 168])
2017-05-09 11:12:57,934 : INFO : torch.Size([168])
2017-05-09 11:12:57,934 : INFO : torch.Size([168, 300])
2017-05-09 11:12:57,934 : INFO : torch.Size([168])
2017-05-09 11:12:57,934 : INFO : torch.Size([168, 300])
2017-05-09 11:12:57,934 : INFO : torch.Size([168])
2017-05-09 11:12:57,934 : INFO : torch.Size([168, 168])
2017-05-09 11:12:57,934 : INFO : torch.Size([168])
2017-05-09 11:12:57,935 : INFO : torch.Size([168, 300])
2017-05-09 11:12:57,935 : INFO : torch.Size([168])
2017-05-09 11:12:57,935 : INFO : torch.Size([168, 168])
2017-05-09 11:12:57,935 : INFO : torch.Size([168])
2017-05-09 11:12:57,935 : INFO : torch.Size([3, 168])
2017-05-09 11:12:57,935 : INFO : torch.Size([3])
2017-05-09 11:12:57,935 : INFO : sum
2017-05-09 11:12:57,935 : INFO : 316347
2017-05-09 11:12:57,936 : INFO : ____________
2017-05-09 11:12:57,936 : INFO : ==> File found, loading to memory
2017-05-09 11:13:04,702 : INFO : ==> GLOVE vocabulary size: 2196016
2017-05-09 11:13:04,707 : INFO : done creating emb, quit
2017-05-09 11:13:04,707 : INFO : quit program
2017-05-09 11:13:13,452 : INFO : LOG_FILE
2017-05-09 11:13:13,452 : INFO : _________________________________start___________________________________
2017-05-09 11:13:13,465 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:13:13,468 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:13:15,629 : INFO : _param count_
2017-05-09 11:13:15,631 : INFO : torch.Size([168, 300])
2017-05-09 11:13:15,632 : INFO : torch.Size([168])
2017-05-09 11:13:15,633 : INFO : torch.Size([168, 168])
2017-05-09 11:13:15,633 : INFO : torch.Size([168])
2017-05-09 11:13:15,634 : INFO : torch.Size([168, 168])
2017-05-09 11:13:15,634 : INFO : torch.Size([168])
2017-05-09 11:13:15,635 : INFO : torch.Size([168, 300])
2017-05-09 11:13:15,635 : INFO : torch.Size([168])
2017-05-09 11:13:15,636 : INFO : torch.Size([168, 300])
2017-05-09 11:13:15,637 : INFO : torch.Size([168])
2017-05-09 11:13:15,637 : INFO : torch.Size([168, 168])
2017-05-09 11:13:15,638 : INFO : torch.Size([168])
2017-05-09 11:13:15,638 : INFO : torch.Size([168, 300])
2017-05-09 11:13:15,639 : INFO : torch.Size([168])
2017-05-09 11:13:15,639 : INFO : torch.Size([168, 168])
2017-05-09 11:13:15,640 : INFO : torch.Size([168])
2017-05-09 11:13:15,640 : INFO : torch.Size([3, 168])
2017-05-09 11:13:15,641 : INFO : torch.Size([3])
2017-05-09 11:13:15,641 : INFO : sum
2017-05-09 11:13:15,642 : INFO : 316347
2017-05-09 11:13:15,643 : INFO : ____________
2017-05-09 11:14:48,404 : INFO : LOG_FILE
2017-05-09 11:14:48,405 : INFO : _________________________________start___________________________________
2017-05-09 11:14:48,418 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:14:48,421 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:37:05,301 : INFO : LOG_FILE
2017-05-09 11:37:05,302 : INFO : _________________________________start___________________________________
2017-05-09 11:37:05,315 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:37:05,318 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:47:05,445 : INFO : LOG_FILE
2017-05-09 11:47:05,445 : INFO : _________________________________start___________________________________
2017-05-09 11:47:05,458 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:47:05,461 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:47:07,351 : INFO : dog
2017-05-09 11:47:20,592 : INFO : 4
2017-05-09 11:47:52,672 : INFO : LOG_FILE
2017-05-09 11:47:52,673 : INFO : _________________________________start___________________________________
2017-05-09 11:47:52,686 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:47:52,688 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:47:54,527 : INFO : 4
2017-05-09 11:47:54,527 : INFO :  2
2017-05-09 11:47:54,528 : INFO :   1
2017-05-09 11:47:54,528 : INFO :  3
2017-05-09 11:47:54,528 : INFO :  13
2017-05-09 11:47:54,529 : INFO :   5
2017-05-09 11:47:54,529 : INFO :   6
2017-05-09 11:47:54,529 : INFO :   9
2017-05-09 11:47:54,529 : INFO :    7
2017-05-09 11:47:54,530 : INFO :    8
2017-05-09 11:47:54,530 : INFO :    10
2017-05-09 11:47:54,530 : INFO :   11
2017-05-09 11:47:54,530 : INFO :   12
2017-05-09 11:47:54,530 : INFO :   14
2017-05-09 11:47:54,530 : INFO :  15
2017-05-09 11:47:54,531 : INFO :  19
2017-05-09 11:47:54,531 : INFO :   16
2017-05-09 11:47:54,531 : INFO :   17
2017-05-09 11:47:54,531 : INFO :   18
2017-05-09 11:47:54,531 : INFO :   21
2017-05-09 11:47:54,531 : INFO :    20
2017-05-09 11:47:54,531 : INFO :    23
2017-05-09 11:47:54,532 : INFO :     22
2017-05-09 11:47:54,532 : INFO :    25
2017-05-09 11:47:54,532 : INFO :     24
2017-05-09 11:47:54,532 : INFO :     26
2017-05-09 11:47:54,532 : INFO :      28
2017-05-09 11:47:54,532 : INFO :       27
2017-05-09 11:47:54,533 : INFO :       29
2017-05-09 11:47:54,533 : INFO :       32
2017-05-09 11:47:54,533 : INFO :        30
2017-05-09 11:47:54,533 : INFO :        31
2017-05-09 11:47:54,533 : INFO :       33
2017-05-09 11:47:54,534 : INFO :       35
2017-05-09 11:47:54,534 : INFO :        34
2017-05-09 11:47:54,534 : INFO :  36
2017-05-09 11:48:45,692 : INFO : LOG_FILE
2017-05-09 11:48:45,693 : INFO : _________________________________start___________________________________
2017-05-09 11:48:45,705 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:48:45,708 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:49:04,307 : INFO : LOG_FILE
2017-05-09 11:49:04,307 : INFO : _________________________________start___________________________________
2017-05-09 11:49:04,321 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:49:04,324 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:49:10,053 : INFO : 4
2017-05-09 11:49:10,054 : INFO : | 2
2017-05-09 11:49:10,054 : INFO : | | 1
2017-05-09 11:49:10,054 : INFO : | 3
2017-05-09 11:49:10,054 : INFO : | 13
2017-05-09 11:49:10,054 : INFO : | | 5
2017-05-09 11:49:10,055 : INFO : | | 6
2017-05-09 11:49:10,055 : INFO : | | 9
2017-05-09 11:49:10,055 : INFO : | | | 7
2017-05-09 11:49:10,055 : INFO : | | | 8
2017-05-09 11:49:10,056 : INFO : | | | 10
2017-05-09 11:49:10,056 : INFO : | | 11
2017-05-09 11:49:10,056 : INFO : | | 12
2017-05-09 11:49:10,056 : INFO : | | 14
2017-05-09 11:49:10,056 : INFO : | 15
2017-05-09 11:49:10,057 : INFO : | 19
2017-05-09 11:49:10,057 : INFO : | | 16
2017-05-09 11:49:10,057 : INFO : | | 17
2017-05-09 11:49:10,057 : INFO : | | 18
2017-05-09 11:49:10,057 : INFO : | | 21
2017-05-09 11:49:10,057 : INFO : | | | 20
2017-05-09 11:49:10,058 : INFO : | | | 23
2017-05-09 11:49:10,058 : INFO : | | | | 22
2017-05-09 11:49:10,058 : INFO : | | | 25
2017-05-09 11:49:10,058 : INFO : | | | | 24
2017-05-09 11:49:10,058 : INFO : | | | | 26
2017-05-09 11:49:10,058 : INFO : | | | | | 28
2017-05-09 11:49:10,058 : INFO : | | | | | | 27
2017-05-09 11:49:10,059 : INFO : | | | | | | 29
2017-05-09 11:49:10,059 : INFO : | | | | | | 32
2017-05-09 11:49:10,059 : INFO : | | | | | | | 30
2017-05-09 11:49:10,059 : INFO : | | | | | | | 31
2017-05-09 11:49:10,060 : INFO : | | | | | | 33
2017-05-09 11:49:10,060 : INFO : | | | | | | 35
2017-05-09 11:49:10,060 : INFO : | | | | | | | 34
2017-05-09 11:49:10,060 : INFO : | 36
2017-05-09 11:53:29,961 : INFO : LOG_FILE
2017-05-09 11:53:29,961 : INFO : _________________________________start___________________________________
2017-05-09 11:53:29,974 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:53:29,977 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:53:35,114 : INFO : LOG_FILE
2017-05-09 11:53:35,114 : INFO : _________________________________start___________________________________
2017-05-09 11:53:35,121 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:53:35,122 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:53:37,340 : INFO : _param count_
2017-05-09 11:53:37,340 : INFO : torch.Size([168, 300])
2017-05-09 11:53:37,340 : INFO : torch.Size([168])
2017-05-09 11:53:37,340 : INFO : torch.Size([168, 168])
2017-05-09 11:53:37,340 : INFO : torch.Size([168])
2017-05-09 11:53:37,340 : INFO : torch.Size([168, 168])
2017-05-09 11:53:37,341 : INFO : torch.Size([168])
2017-05-09 11:53:37,341 : INFO : torch.Size([168, 300])
2017-05-09 11:53:37,341 : INFO : torch.Size([168])
2017-05-09 11:53:37,341 : INFO : torch.Size([168, 300])
2017-05-09 11:53:37,341 : INFO : torch.Size([168])
2017-05-09 11:53:37,341 : INFO : torch.Size([168, 168])
2017-05-09 11:53:37,341 : INFO : torch.Size([168])
2017-05-09 11:53:37,341 : INFO : torch.Size([168, 300])
2017-05-09 11:53:37,342 : INFO : torch.Size([168])
2017-05-09 11:53:37,342 : INFO : torch.Size([168, 168])
2017-05-09 11:53:37,342 : INFO : torch.Size([168])
2017-05-09 11:53:37,342 : INFO : torch.Size([3, 168])
2017-05-09 11:53:37,342 : INFO : torch.Size([3])
2017-05-09 11:53:37,342 : INFO : sum
2017-05-09 11:53:37,342 : INFO : 316347
2017-05-09 11:53:37,343 : INFO : ____________
2017-05-09 11:53:37,343 : INFO : ==> File found, loading to memory
2017-05-09 11:53:44,182 : INFO : ==> GLOVE vocabulary size: 2196016
2017-05-09 11:53:44,195 : INFO : done creating emb, quit
2017-05-09 11:53:44,195 : INFO : quit program
2017-05-09 11:54:01,317 : INFO : LOG_FILE
2017-05-09 11:54:01,317 : INFO : _________________________________start___________________________________
2017-05-09 11:54:01,330 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:54:01,333 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:54:03,459 : INFO : _param count_
2017-05-09 11:54:03,462 : INFO : torch.Size([168, 300])
2017-05-09 11:54:03,462 : INFO : torch.Size([168])
2017-05-09 11:54:03,463 : INFO : torch.Size([168, 168])
2017-05-09 11:54:03,464 : INFO : torch.Size([168])
2017-05-09 11:54:03,464 : INFO : torch.Size([168, 168])
2017-05-09 11:54:03,465 : INFO : torch.Size([168])
2017-05-09 11:54:03,465 : INFO : torch.Size([168, 300])
2017-05-09 11:54:03,466 : INFO : torch.Size([168])
2017-05-09 11:54:03,466 : INFO : torch.Size([168, 300])
2017-05-09 11:54:03,467 : INFO : torch.Size([168])
2017-05-09 11:54:03,467 : INFO : torch.Size([168, 168])
2017-05-09 11:54:03,468 : INFO : torch.Size([168])
2017-05-09 11:54:03,468 : INFO : torch.Size([168, 300])
2017-05-09 11:54:03,469 : INFO : torch.Size([168])
2017-05-09 11:54:03,470 : INFO : torch.Size([168, 168])
2017-05-09 11:54:03,470 : INFO : torch.Size([168])
2017-05-09 11:54:03,471 : INFO : torch.Size([3, 168])
2017-05-09 11:54:03,471 : INFO : torch.Size([3])
2017-05-09 11:54:03,472 : INFO : sum
2017-05-09 11:54:03,472 : INFO : 316347
2017-05-09 11:54:03,473 : INFO : ____________
2017-05-09 11:55:32,295 : INFO : LOG_FILE
2017-05-09 11:55:32,295 : INFO : _________________________________start___________________________________
2017-05-09 11:55:32,308 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:55:32,310 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:55:34,521 : INFO : _param count_
2017-05-09 11:55:34,524 : INFO : torch.Size([168, 300])
2017-05-09 11:55:34,525 : INFO : torch.Size([168])
2017-05-09 11:55:34,526 : INFO : torch.Size([168, 168])
2017-05-09 11:55:34,526 : INFO : torch.Size([168])
2017-05-09 11:55:34,527 : INFO : torch.Size([168, 168])
2017-05-09 11:55:34,528 : INFO : torch.Size([168])
2017-05-09 11:55:34,528 : INFO : torch.Size([168, 300])
2017-05-09 11:55:34,529 : INFO : torch.Size([168])
2017-05-09 11:55:34,530 : INFO : torch.Size([168, 300])
2017-05-09 11:55:34,531 : INFO : torch.Size([168])
2017-05-09 11:55:34,531 : INFO : torch.Size([168, 168])
2017-05-09 11:55:34,532 : INFO : torch.Size([168])
2017-05-09 11:55:34,533 : INFO : torch.Size([168, 300])
2017-05-09 11:55:34,533 : INFO : torch.Size([168])
2017-05-09 11:55:34,534 : INFO : torch.Size([168, 168])
2017-05-09 11:55:34,535 : INFO : torch.Size([168])
2017-05-09 11:55:34,536 : INFO : torch.Size([3, 168])
2017-05-09 11:55:34,536 : INFO : torch.Size([3])
2017-05-09 11:55:34,537 : INFO : sum
2017-05-09 11:55:34,538 : INFO : 316347
2017-05-09 11:55:34,538 : INFO : ____________
2017-05-09 11:56:51,120 : INFO : LOG_FILE
2017-05-09 11:56:51,121 : INFO : _________________________________start___________________________________
2017-05-09 11:56:51,133 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:56:51,136 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:56:53,305 : INFO : _param count_
2017-05-09 11:56:53,307 : INFO : torch.Size([168, 300])
2017-05-09 11:56:53,308 : INFO : torch.Size([168])
2017-05-09 11:56:53,308 : INFO : torch.Size([168, 168])
2017-05-09 11:56:53,309 : INFO : torch.Size([168])
2017-05-09 11:56:53,309 : INFO : torch.Size([168, 168])
2017-05-09 11:56:53,310 : INFO : torch.Size([168])
2017-05-09 11:56:53,311 : INFO : torch.Size([168, 300])
2017-05-09 11:56:53,311 : INFO : torch.Size([168])
2017-05-09 11:56:53,312 : INFO : torch.Size([168, 300])
2017-05-09 11:56:53,312 : INFO : torch.Size([168])
2017-05-09 11:56:53,313 : INFO : torch.Size([168, 168])
2017-05-09 11:56:53,313 : INFO : torch.Size([168])
2017-05-09 11:56:53,314 : INFO : torch.Size([168, 300])
2017-05-09 11:56:53,314 : INFO : torch.Size([168])
2017-05-09 11:56:53,315 : INFO : torch.Size([168, 168])
2017-05-09 11:56:53,316 : INFO : torch.Size([168])
2017-05-09 11:56:53,316 : INFO : torch.Size([3, 168])
2017-05-09 11:56:53,317 : INFO : torch.Size([3])
2017-05-09 11:56:53,317 : INFO : sum
2017-05-09 11:56:53,318 : INFO : 316347
2017-05-09 11:56:53,318 : INFO : ____________
2017-05-09 11:57:47,430 : INFO : LOG_FILE
2017-05-09 11:57:47,431 : INFO : _________________________________start___________________________________
2017-05-09 11:57:47,444 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:57:47,446 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:57:49,722 : INFO : _param count_
2017-05-09 11:57:49,724 : INFO : torch.Size([168, 300])
2017-05-09 11:57:49,725 : INFO : torch.Size([168])
2017-05-09 11:57:49,726 : INFO : torch.Size([168, 168])
2017-05-09 11:57:49,726 : INFO : torch.Size([168])
2017-05-09 11:57:49,727 : INFO : torch.Size([168, 168])
2017-05-09 11:57:49,727 : INFO : torch.Size([168])
2017-05-09 11:57:49,728 : INFO : torch.Size([168, 300])
2017-05-09 11:57:49,728 : INFO : torch.Size([168])
2017-05-09 11:57:49,729 : INFO : torch.Size([168, 300])
2017-05-09 11:57:49,729 : INFO : torch.Size([168])
2017-05-09 11:57:49,730 : INFO : torch.Size([168, 168])
2017-05-09 11:57:49,731 : INFO : torch.Size([168])
2017-05-09 11:57:49,731 : INFO : torch.Size([168, 300])
2017-05-09 11:57:49,732 : INFO : torch.Size([168])
2017-05-09 11:57:49,732 : INFO : torch.Size([168, 168])
2017-05-09 11:57:49,733 : INFO : torch.Size([168])
2017-05-09 11:57:49,733 : INFO : torch.Size([3, 168])
2017-05-09 11:57:49,734 : INFO : torch.Size([3])
2017-05-09 11:57:49,734 : INFO : sum
2017-05-09 11:57:49,735 : INFO : 316347
2017-05-09 11:57:49,735 : INFO : ____________
2017-05-09 11:58:25,715 : INFO : LOG_FILE
2017-05-09 11:58:25,715 : INFO : _________________________________start___________________________________
2017-05-09 11:58:25,728 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 11:58:25,731 : INFO : ==> SST vocabulary size : 105
2017-05-09 11:58:27,898 : INFO : _param count_
2017-05-09 11:58:27,901 : INFO : torch.Size([168, 300])
2017-05-09 11:58:27,901 : INFO : torch.Size([168])
2017-05-09 11:58:27,902 : INFO : torch.Size([168, 168])
2017-05-09 11:58:27,903 : INFO : torch.Size([168])
2017-05-09 11:58:27,903 : INFO : torch.Size([168, 168])
2017-05-09 11:58:27,904 : INFO : torch.Size([168])
2017-05-09 11:58:27,904 : INFO : torch.Size([168, 300])
2017-05-09 11:58:27,905 : INFO : torch.Size([168])
2017-05-09 11:58:27,905 : INFO : torch.Size([168, 300])
2017-05-09 11:58:27,906 : INFO : torch.Size([168])
2017-05-09 11:58:27,906 : INFO : torch.Size([168, 168])
2017-05-09 11:58:27,907 : INFO : torch.Size([168])
2017-05-09 11:58:27,908 : INFO : torch.Size([168, 300])
2017-05-09 11:58:27,908 : INFO : torch.Size([168])
2017-05-09 11:58:27,909 : INFO : torch.Size([168, 168])
2017-05-09 11:58:27,909 : INFO : torch.Size([168])
2017-05-09 11:58:27,910 : INFO : torch.Size([3, 168])
2017-05-09 11:58:27,910 : INFO : torch.Size([3])
2017-05-09 11:58:27,911 : INFO : sum
2017-05-09 11:58:27,911 : INFO : 316347
2017-05-09 11:58:27,912 : INFO : ____________
2017-05-09 12:06:14,955 : INFO : LOG_FILE
2017-05-09 12:06:14,956 : INFO : _________________________________start___________________________________
2017-05-09 12:06:14,969 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 12:06:14,972 : INFO : ==> SST vocabulary size : 105
2017-05-09 12:06:17,149 : INFO : _param count_
2017-05-09 12:06:17,152 : INFO : torch.Size([168, 300])
2017-05-09 12:06:17,153 : INFO : torch.Size([168])
2017-05-09 12:06:17,153 : INFO : torch.Size([168, 168])
2017-05-09 12:06:17,154 : INFO : torch.Size([168])
2017-05-09 12:06:17,154 : INFO : torch.Size([168, 168])
2017-05-09 12:06:17,155 : INFO : torch.Size([168])
2017-05-09 12:06:17,155 : INFO : torch.Size([168, 300])
2017-05-09 12:06:17,156 : INFO : torch.Size([168])
2017-05-09 12:06:17,157 : INFO : torch.Size([168, 300])
2017-05-09 12:06:17,157 : INFO : torch.Size([168])
2017-05-09 12:06:17,158 : INFO : torch.Size([168, 168])
2017-05-09 12:06:17,158 : INFO : torch.Size([168])
2017-05-09 12:06:17,159 : INFO : torch.Size([168, 300])
2017-05-09 12:06:17,159 : INFO : torch.Size([168])
2017-05-09 12:06:17,160 : INFO : torch.Size([168, 168])
2017-05-09 12:06:17,160 : INFO : torch.Size([168])
2017-05-09 12:06:17,161 : INFO : torch.Size([3, 168])
2017-05-09 12:06:17,162 : INFO : torch.Size([3])
2017-05-09 12:06:17,162 : INFO : sum
2017-05-09 12:06:17,163 : INFO : 316347
2017-05-09 12:06:17,163 : INFO : ____________
2017-05-09 12:10:04,878 : INFO : LOG_FILE
2017-05-09 12:10:04,879 : INFO : _________________________________start___________________________________
2017-05-09 12:10:04,891 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 12:10:04,894 : INFO : ==> SST vocabulary size : 105
2017-05-09 12:10:07,065 : INFO : _param count_
2017-05-09 12:10:07,067 : INFO : torch.Size([168, 300])
2017-05-09 12:10:07,068 : INFO : torch.Size([168])
2017-05-09 12:10:07,069 : INFO : torch.Size([168, 168])
2017-05-09 12:10:07,069 : INFO : torch.Size([168])
2017-05-09 12:10:07,070 : INFO : torch.Size([168, 168])
2017-05-09 12:10:07,070 : INFO : torch.Size([168])
2017-05-09 12:10:07,071 : INFO : torch.Size([168, 300])
2017-05-09 12:10:07,071 : INFO : torch.Size([168])
2017-05-09 12:10:07,072 : INFO : torch.Size([168, 300])
2017-05-09 12:10:07,073 : INFO : torch.Size([168])
2017-05-09 12:10:07,073 : INFO : torch.Size([168, 168])
2017-05-09 12:10:07,074 : INFO : torch.Size([168])
2017-05-09 12:10:07,074 : INFO : torch.Size([168, 300])
2017-05-09 12:10:07,075 : INFO : torch.Size([168])
2017-05-09 12:10:07,075 : INFO : torch.Size([168, 168])
2017-05-09 12:10:07,076 : INFO : torch.Size([168])
2017-05-09 12:10:07,077 : INFO : torch.Size([3, 168])
2017-05-09 12:10:07,077 : INFO : torch.Size([3])
2017-05-09 12:10:07,078 : INFO : sum
2017-05-09 12:10:07,078 : INFO : 316347
2017-05-09 12:10:07,079 : INFO : ____________
2017-05-09 12:24:50,356 : INFO : LOG_FILE
2017-05-09 12:24:50,356 : INFO : _________________________________start___________________________________
2017-05-09 12:24:50,369 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 12:24:50,372 : INFO : ==> SST vocabulary size : 105
2017-05-09 12:24:52,518 : INFO : _param count_
2017-05-09 12:24:52,520 : INFO : torch.Size([168, 300])
2017-05-09 12:24:52,521 : INFO : torch.Size([168])
2017-05-09 12:24:52,521 : INFO : torch.Size([168, 168])
2017-05-09 12:24:52,522 : INFO : torch.Size([168])
2017-05-09 12:24:52,523 : INFO : torch.Size([168, 168])
2017-05-09 12:24:52,523 : INFO : torch.Size([168])
2017-05-09 12:24:52,524 : INFO : torch.Size([168, 300])
2017-05-09 12:24:52,524 : INFO : torch.Size([168])
2017-05-09 12:24:52,525 : INFO : torch.Size([168, 300])
2017-05-09 12:24:52,525 : INFO : torch.Size([168])
2017-05-09 12:24:52,526 : INFO : torch.Size([168, 168])
2017-05-09 12:24:52,526 : INFO : torch.Size([168])
2017-05-09 12:24:52,527 : INFO : torch.Size([168, 300])
2017-05-09 12:24:52,528 : INFO : torch.Size([168])
2017-05-09 12:24:52,528 : INFO : torch.Size([168, 168])
2017-05-09 12:24:52,529 : INFO : torch.Size([168])
2017-05-09 12:24:52,529 : INFO : torch.Size([3, 168])
2017-05-09 12:24:52,530 : INFO : torch.Size([3])
2017-05-09 12:24:52,530 : INFO : sum
2017-05-09 12:24:52,531 : INFO : 316347
2017-05-09 12:24:52,531 : INFO : ____________
2017-05-09 12:25:42,646 : INFO : LOG_FILE
2017-05-09 12:25:42,647 : INFO : _________________________________start___________________________________
2017-05-09 12:25:42,660 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 12:25:42,663 : INFO : ==> SST vocabulary size : 105
2017-05-09 12:25:44,856 : INFO : _param count_
2017-05-09 12:25:44,859 : INFO : torch.Size([168, 300])
2017-05-09 12:25:44,859 : INFO : torch.Size([168])
2017-05-09 12:25:44,860 : INFO : torch.Size([168, 168])
2017-05-09 12:25:44,860 : INFO : torch.Size([168])
2017-05-09 12:25:44,861 : INFO : torch.Size([168, 168])
2017-05-09 12:25:44,861 : INFO : torch.Size([168])
2017-05-09 12:25:44,862 : INFO : torch.Size([168, 300])
2017-05-09 12:25:44,863 : INFO : torch.Size([168])
2017-05-09 12:25:44,863 : INFO : torch.Size([168, 300])
2017-05-09 12:25:44,864 : INFO : torch.Size([168])
2017-05-09 12:25:44,864 : INFO : torch.Size([168, 168])
2017-05-09 12:25:44,865 : INFO : torch.Size([168])
2017-05-09 12:25:44,865 : INFO : torch.Size([168, 300])
2017-05-09 12:25:44,866 : INFO : torch.Size([168])
2017-05-09 12:25:44,866 : INFO : torch.Size([168, 168])
2017-05-09 12:25:44,867 : INFO : torch.Size([168])
2017-05-09 12:25:44,868 : INFO : torch.Size([3, 168])
2017-05-09 12:25:44,868 : INFO : torch.Size([3])
2017-05-09 12:25:44,869 : INFO : sum
2017-05-09 12:25:44,869 : INFO : 316347
2017-05-09 12:25:44,870 : INFO : ____________
2017-05-09 12:26:32,691 : INFO : LOG_FILE
2017-05-09 12:26:32,692 : INFO : _________________________________start___________________________________
2017-05-09 12:26:32,705 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 12:26:32,708 : INFO : ==> SST vocabulary size : 105
2017-05-09 12:26:34,901 : INFO : _param count_
2017-05-09 12:26:34,904 : INFO : torch.Size([168, 300])
2017-05-09 12:26:34,904 : INFO : torch.Size([168])
2017-05-09 12:26:34,905 : INFO : torch.Size([168, 168])
2017-05-09 12:26:34,905 : INFO : torch.Size([168])
2017-05-09 12:26:34,906 : INFO : torch.Size([168, 168])
2017-05-09 12:26:34,907 : INFO : torch.Size([168])
2017-05-09 12:26:34,907 : INFO : torch.Size([168, 300])
2017-05-09 12:26:34,908 : INFO : torch.Size([168])
2017-05-09 12:26:34,908 : INFO : torch.Size([168, 300])
2017-05-09 12:26:34,909 : INFO : torch.Size([168])
2017-05-09 12:26:34,909 : INFO : torch.Size([168, 168])
2017-05-09 12:26:34,910 : INFO : torch.Size([168])
2017-05-09 12:26:34,910 : INFO : torch.Size([168, 300])
2017-05-09 12:26:34,911 : INFO : torch.Size([168])
2017-05-09 12:26:34,912 : INFO : torch.Size([168, 168])
2017-05-09 12:26:34,912 : INFO : torch.Size([168])
2017-05-09 12:26:34,913 : INFO : torch.Size([3, 168])
2017-05-09 12:26:34,913 : INFO : torch.Size([3])
2017-05-09 12:26:34,914 : INFO : sum
2017-05-09 12:26:34,914 : INFO : 316347
2017-05-09 12:26:34,915 : INFO : ____________
2017-05-09 12:28:04,526 : INFO : LOG_FILE
2017-05-09 12:28:04,527 : INFO : _________________________________start___________________________________
2017-05-09 12:28:04,540 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 12:28:04,543 : INFO : ==> SST vocabulary size : 105
2017-05-09 12:28:06,685 : INFO : _param count_
2017-05-09 12:28:06,688 : INFO : torch.Size([168, 300])
2017-05-09 12:28:06,688 : INFO : torch.Size([168])
2017-05-09 12:28:06,689 : INFO : torch.Size([168, 168])
2017-05-09 12:28:06,689 : INFO : torch.Size([168])
2017-05-09 12:28:06,690 : INFO : torch.Size([168, 168])
2017-05-09 12:28:06,691 : INFO : torch.Size([168])
2017-05-09 12:28:06,691 : INFO : torch.Size([168, 300])
2017-05-09 12:28:06,692 : INFO : torch.Size([168])
2017-05-09 12:28:06,692 : INFO : torch.Size([168, 300])
2017-05-09 12:28:06,693 : INFO : torch.Size([168])
2017-05-09 12:28:06,693 : INFO : torch.Size([168, 168])
2017-05-09 12:28:06,694 : INFO : torch.Size([168])
2017-05-09 12:28:06,694 : INFO : torch.Size([168, 300])
2017-05-09 12:28:06,695 : INFO : torch.Size([168])
2017-05-09 12:28:06,696 : INFO : torch.Size([168, 168])
2017-05-09 12:28:06,696 : INFO : torch.Size([168])
2017-05-09 12:28:06,697 : INFO : torch.Size([3, 168])
2017-05-09 12:28:06,697 : INFO : torch.Size([3])
2017-05-09 12:28:06,698 : INFO : sum
2017-05-09 12:28:06,698 : INFO : 316347
2017-05-09 12:28:06,699 : INFO : ____________
2017-05-09 12:28:06,903 : INFO : (1, Variable containing:
2017-05-09 12:28:06,904 : INFO :  1.1577
2017-05-09 12:28:06,904 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:06,905 : INFO : )
2017-05-09 12:28:06,910 : INFO : (2, Variable containing:
2017-05-09 12:28:06,911 : INFO :  1.0745
2017-05-09 12:28:06,911 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:06,912 : INFO : )
2017-05-09 12:28:06,917 : INFO : (3, Variable containing:
2017-05-09 12:28:06,917 : INFO :  1.1601
2017-05-09 12:28:06,918 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:06,919 : INFO : )
2017-05-09 12:28:06,924 : INFO : (6, Variable containing:
2017-05-09 12:28:06,925 : INFO :  1.1322
2017-05-09 12:28:06,925 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:06,926 : INFO : )
2017-05-09 12:28:06,931 : INFO : (7, Variable containing:
2017-05-09 12:28:06,932 : INFO :  1.1577
2017-05-09 12:28:06,932 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:06,933 : INFO : )
2017-05-09 12:28:06,938 : INFO : (10, Variable containing:
2017-05-09 12:28:06,939 : INFO :  1.1718
2017-05-09 12:28:06,939 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:06,940 : INFO : )
2017-05-09 12:28:06,945 : INFO : (11, Variable containing:
2017-05-09 12:28:06,945 : INFO :  2.3070
2017-05-09 12:28:06,946 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:06,947 : INFO : )
2017-05-09 12:28:06,952 : INFO : (9, Variable containing:
2017-05-09 12:28:06,952 : INFO :  3.4675
2017-05-09 12:28:06,953 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:06,953 : INFO : )
2017-05-09 12:28:06,959 : INFO : (8, Variable containing:
2017-05-09 12:28:06,960 : INFO :  5.7359
2017-05-09 12:28:06,960 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:06,961 : INFO : )
2017-05-09 12:28:06,966 : INFO : (12, Variable containing:
2017-05-09 12:28:06,966 : INFO :  1.1340
2017-05-09 12:28:06,967 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:06,967 : INFO : )
2017-05-09 12:28:06,973 : INFO : (13, Variable containing:
2017-05-09 12:28:06,974 : INFO :  8.0021
2017-05-09 12:28:06,974 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:06,975 : INFO : )
2017-05-09 12:28:06,980 : INFO : (5, Variable containing:
2017-05-09 12:28:06,981 : INFO :  9.1744
2017-05-09 12:28:06,981 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:06,982 : INFO : )
2017-05-09 12:28:06,989 : INFO : (4, Variable containing:
2017-05-09 12:28:06,989 : INFO :  13.6702
2017-05-09 12:28:06,990 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:06,990 : INFO : )
2017-05-09 12:28:06,996 : INFO : (14, Variable containing:
2017-05-09 12:28:06,996 : INFO :  1.1887
2017-05-09 12:28:06,997 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:06,997 : INFO : )
2017-05-09 12:28:07,003 : INFO : (15, Variable containing:
2017-05-09 12:28:07,003 : INFO :  1.1875
2017-05-09 12:28:07,004 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,004 : INFO : )
2017-05-09 12:28:07,010 : INFO : (17, Variable containing:
2017-05-09 12:28:07,011 : INFO :  1.1765
2017-05-09 12:28:07,011 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,011 : INFO : )
2017-05-09 12:28:07,017 : INFO : (18, Variable containing:
2017-05-09 12:28:07,017 : INFO :  1.1860
2017-05-09 12:28:07,018 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,019 : INFO : )
2017-05-09 12:28:07,024 : INFO : (21, Variable containing:
2017-05-09 12:28:07,024 : INFO :  1.1484
2017-05-09 12:28:07,025 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,026 : INFO : )
2017-05-09 12:28:07,031 : INFO : (20, Variable containing:
2017-05-09 12:28:07,031 : INFO :  2.3133
2017-05-09 12:28:07,032 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,032 : INFO : )
2017-05-09 12:28:07,038 : INFO : (19, Variable containing:
2017-05-09 12:28:07,039 : INFO :  4.6814
2017-05-09 12:28:07,040 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,040 : INFO : )
2017-05-09 12:28:07,046 : INFO : (22, Variable containing:
2017-05-09 12:28:07,046 : INFO :  1.1969
2017-05-09 12:28:07,047 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,047 : INFO : )
2017-05-09 12:28:07,053 : INFO : (23, Variable containing:
2017-05-09 12:28:07,053 : INFO :  1.0630
2017-05-09 12:28:07,054 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,054 : INFO : )
2017-05-09 12:28:07,060 : INFO : (24, Variable containing:
2017-05-09 12:28:07,060 : INFO :  1.0733
2017-05-09 12:28:07,061 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,061 : INFO : )
2017-05-09 12:28:07,067 : INFO : (26, Variable containing:
2017-05-09 12:28:07,067 : INFO :  1.1568
2017-05-09 12:28:07,068 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,068 : INFO : )
2017-05-09 12:28:07,074 : INFO : (27, Variable containing:
2017-05-09 12:28:07,074 : INFO :  1.1412
2017-05-09 12:28:07,075 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,075 : INFO : )
2017-05-09 12:28:07,081 : INFO : (29, Variable containing:
2017-05-09 12:28:07,081 : INFO :  1.1548
2017-05-09 12:28:07,082 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,082 : INFO : )
2017-05-09 12:28:07,088 : INFO : (28, Variable containing:
2017-05-09 12:28:07,089 : INFO :  4.5922
2017-05-09 12:28:07,089 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,090 : INFO : )
2017-05-09 12:28:07,095 : INFO : (30, Variable containing:
2017-05-09 12:28:07,096 : INFO :  1.1657
2017-05-09 12:28:07,096 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,097 : INFO : )
2017-05-09 12:28:07,104 : INFO : (33, Variable containing:
2017-05-09 12:28:07,104 : INFO :  1.1146
2017-05-09 12:28:07,105 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,105 : INFO : )
2017-05-09 12:28:07,112 : INFO : (35, Variable containing:
2017-05-09 12:28:07,113 : INFO :  1.1548
2017-05-09 12:28:07,114 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,115 : INFO : )
2017-05-09 12:28:07,123 : INFO : (34, Variable containing:
2017-05-09 12:28:07,124 : INFO :  3.3807
2017-05-09 12:28:07,125 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,125 : INFO : )
2017-05-09 12:28:07,131 : INFO : (36, Variable containing:
2017-05-09 12:28:07,131 : INFO :  4.5081
2017-05-09 12:28:07,132 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,132 : INFO : )
2017-05-09 12:28:07,138 : INFO : (32, Variable containing:
2017-05-09 12:28:07,138 : INFO :  5.6627
2017-05-09 12:28:07,139 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,139 : INFO : )
2017-05-09 12:28:07,145 : INFO : (31, Variable containing:
2017-05-09 12:28:07,146 : INFO :  12.5857
2017-05-09 12:28:07,146 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,147 : INFO : )
2017-05-09 12:28:07,155 : INFO : (25, Variable containing:
2017-05-09 12:28:07,155 : INFO :  22.9706
2017-05-09 12:28:07,156 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,156 : INFO : )
2017-05-09 12:28:07,162 : INFO : (37, Variable containing:
2017-05-09 12:28:07,162 : INFO :  1.2072
2017-05-09 12:28:07,163 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,163 : INFO : )
2017-05-09 12:28:07,170 : INFO : (16, Variable containing:
2017-05-09 12:28:07,171 : INFO :  41.3159
2017-05-09 12:28:07,171 : INFO : [torch.cuda.FloatTensor of size 1 (GPU 0)]
2017-05-09 12:28:07,172 : INFO : )
2017-05-09 12:28:25,615 : INFO : LOG_FILE
2017-05-09 12:28:25,616 : INFO : _________________________________start___________________________________
2017-05-09 12:28:25,629 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 12:28:25,632 : INFO : ==> SST vocabulary size : 105
2017-05-09 12:28:27,794 : INFO : _param count_
2017-05-09 12:28:27,797 : INFO : torch.Size([168, 300])
2017-05-09 12:28:27,797 : INFO : torch.Size([168])
2017-05-09 12:28:27,798 : INFO : torch.Size([168, 168])
2017-05-09 12:28:27,798 : INFO : torch.Size([168])
2017-05-09 12:28:27,799 : INFO : torch.Size([168, 168])
2017-05-09 12:28:27,800 : INFO : torch.Size([168])
2017-05-09 12:28:27,800 : INFO : torch.Size([168, 300])
2017-05-09 12:28:27,801 : INFO : torch.Size([168])
2017-05-09 12:28:27,801 : INFO : torch.Size([168, 300])
2017-05-09 12:28:27,802 : INFO : torch.Size([168])
2017-05-09 12:28:27,802 : INFO : torch.Size([168, 168])
2017-05-09 12:28:27,803 : INFO : torch.Size([168])
2017-05-09 12:28:27,804 : INFO : torch.Size([168, 300])
2017-05-09 12:28:27,804 : INFO : torch.Size([168])
2017-05-09 12:28:27,805 : INFO : torch.Size([168, 168])
2017-05-09 12:28:27,805 : INFO : torch.Size([168])
2017-05-09 12:28:27,806 : INFO : torch.Size([3, 168])
2017-05-09 12:28:27,806 : INFO : torch.Size([3])
2017-05-09 12:28:27,807 : INFO : sum
2017-05-09 12:28:27,807 : INFO : 316347
2017-05-09 12:28:27,808 : INFO : ____________
2017-05-09 12:28:46,630 : INFO : LOG_FILE
2017-05-09 12:28:46,630 : INFO : _________________________________start___________________________________
2017-05-09 12:28:46,642 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 12:28:46,645 : INFO : ==> SST vocabulary size : 105
2017-05-09 12:28:48,791 : INFO : _param count_
2017-05-09 12:28:48,794 : INFO : torch.Size([168, 300])
2017-05-09 12:28:48,794 : INFO : torch.Size([168])
2017-05-09 12:28:48,795 : INFO : torch.Size([168, 168])
2017-05-09 12:28:48,796 : INFO : torch.Size([168])
2017-05-09 12:28:48,796 : INFO : torch.Size([168, 168])
2017-05-09 12:28:48,797 : INFO : torch.Size([168])
2017-05-09 12:28:48,797 : INFO : torch.Size([168, 300])
2017-05-09 12:28:48,798 : INFO : torch.Size([168])
2017-05-09 12:28:48,798 : INFO : torch.Size([168, 300])
2017-05-09 12:28:48,799 : INFO : torch.Size([168])
2017-05-09 12:28:48,800 : INFO : torch.Size([168, 168])
2017-05-09 12:28:48,800 : INFO : torch.Size([168])
2017-05-09 12:28:48,801 : INFO : torch.Size([168, 300])
2017-05-09 12:28:48,801 : INFO : torch.Size([168])
2017-05-09 12:28:48,802 : INFO : torch.Size([168, 168])
2017-05-09 12:28:48,802 : INFO : torch.Size([168])
2017-05-09 12:28:48,803 : INFO : torch.Size([3, 168])
2017-05-09 12:28:48,803 : INFO : torch.Size([3])
2017-05-09 12:28:48,804 : INFO : sum
2017-05-09 12:28:48,804 : INFO : 316347
2017-05-09 12:28:48,805 : INFO : ____________
2017-05-09 12:28:50,340 : INFO : (1, 1.1577483415603638)
2017-05-09 12:28:50,345 : INFO : (2, 1.0745458602905273)
2017-05-09 12:28:50,351 : INFO : (3, 1.1600637435913086)
2017-05-09 12:28:50,357 : INFO : (6, 1.132184624671936)
2017-05-09 12:28:50,362 : INFO : (7, 1.1577483415603638)
2017-05-09 12:28:50,368 : INFO : (10, 1.1717875003814697)
2017-05-09 12:28:50,373 : INFO : (11, 2.3070101737976074)
2017-05-09 12:28:50,379 : INFO : (9, 3.467470169067383)
2017-05-09 12:28:50,385 : INFO : (8, 5.735894680023193)
2017-05-09 12:28:50,390 : INFO : (12, 1.1339832544326782)
2017-05-09 12:28:50,396 : INFO : (13, 8.002062797546387)
2017-05-09 12:28:50,401 : INFO : (5, 9.174448013305664)
2017-05-09 12:28:50,408 : INFO : (4, 13.670205116271973)
2017-05-09 12:28:50,413 : INFO : (14, 1.188725233078003)
2017-05-09 12:28:50,419 : INFO : (15, 1.1875367164611816)
2017-05-09 12:28:50,424 : INFO : (17, 1.17649507522583)
2017-05-09 12:28:50,430 : INFO : (18, 1.1859716176986694)
2017-05-09 12:28:50,435 : INFO : (21, 1.1483649015426636)
2017-05-09 12:28:50,441 : INFO : (20, 2.3133182525634766)
2017-05-09 12:28:50,448 : INFO : (19, 4.6813507080078125)
2017-05-09 12:28:50,453 : INFO : (22, 1.1968988180160522)
2017-05-09 12:28:50,459 : INFO : (23, 1.0630097389221191)
2017-05-09 12:28:50,464 : INFO : (24, 1.0733129978179932)
2017-05-09 12:28:50,470 : INFO : (26, 1.1568262577056885)
2017-05-09 12:28:50,475 : INFO : (27, 1.1412229537963867)
2017-05-09 12:28:50,481 : INFO : (29, 1.1548421382904053)
2017-05-09 12:28:50,487 : INFO : (28, 4.592202186584473)
2017-05-09 12:28:50,494 : INFO : (30, 1.16572105884552)
2017-05-09 12:28:50,500 : INFO : (33, 1.1145954132080078)
2017-05-09 12:28:50,505 : INFO : (35, 1.1548421382904053)
2017-05-09 12:28:50,511 : INFO : (34, 3.3807010650634766)
2017-05-09 12:28:50,516 : INFO : (36, 4.50806999206543)
2017-05-09 12:28:50,522 : INFO : (32, 5.662715435028076)
2017-05-09 12:28:50,528 : INFO : (31, 12.58568000793457)
2017-05-09 12:28:50,536 : INFO : (25, 22.970571517944336)
2017-05-09 12:28:50,541 : INFO : (37, 1.2072006464004517)
2017-05-09 12:28:50,550 : INFO : (16, 41.31589126586914)
2017-05-09 12:41:11,713 : INFO : LOG_FILE
2017-05-09 12:41:11,713 : INFO : _________________________________start___________________________________
2017-05-09 12:41:11,726 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 12:41:11,729 : INFO : ==> SST vocabulary size : 105
2017-05-09 12:41:13,964 : INFO : _param count_
2017-05-09 12:41:13,966 : INFO : torch.Size([168, 300])
2017-05-09 12:41:13,967 : INFO : torch.Size([168])
2017-05-09 12:41:13,967 : INFO : torch.Size([168, 168])
2017-05-09 12:41:13,968 : INFO : torch.Size([168])
2017-05-09 12:41:13,968 : INFO : torch.Size([168, 168])
2017-05-09 12:41:13,969 : INFO : torch.Size([168])
2017-05-09 12:41:13,970 : INFO : torch.Size([168, 300])
2017-05-09 12:41:13,970 : INFO : torch.Size([168])
2017-05-09 12:41:13,971 : INFO : torch.Size([168, 300])
2017-05-09 12:41:13,971 : INFO : torch.Size([168])
2017-05-09 12:41:13,972 : INFO : torch.Size([168, 168])
2017-05-09 12:41:13,972 : INFO : torch.Size([168])
2017-05-09 12:41:13,973 : INFO : torch.Size([168, 300])
2017-05-09 12:41:13,973 : INFO : torch.Size([168])
2017-05-09 12:41:13,974 : INFO : torch.Size([168, 168])
2017-05-09 12:41:13,975 : INFO : torch.Size([168])
2017-05-09 12:41:13,975 : INFO : torch.Size([3, 168])
2017-05-09 12:41:13,976 : INFO : torch.Size([3])
2017-05-09 12:41:13,976 : INFO : sum
2017-05-09 12:41:13,977 : INFO : 316347
2017-05-09 12:41:13,978 : INFO : ____________
2017-05-09 12:41:59,836 : INFO : LOG_FILE
2017-05-09 12:41:59,836 : INFO : _________________________________start___________________________________
2017-05-09 12:41:59,849 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 12:41:59,852 : INFO : ==> SST vocabulary size : 105
2017-05-09 12:42:02,028 : INFO : _param count_
2017-05-09 12:42:02,030 : INFO : torch.Size([168, 300])
2017-05-09 12:42:02,031 : INFO : torch.Size([168])
2017-05-09 12:42:02,032 : INFO : torch.Size([168, 168])
2017-05-09 12:42:02,032 : INFO : torch.Size([168])
2017-05-09 12:42:02,033 : INFO : torch.Size([168, 168])
2017-05-09 12:42:02,033 : INFO : torch.Size([168])
2017-05-09 12:42:02,034 : INFO : torch.Size([168, 300])
2017-05-09 12:42:02,034 : INFO : torch.Size([168])
2017-05-09 12:42:02,035 : INFO : torch.Size([168, 300])
2017-05-09 12:42:02,035 : INFO : torch.Size([168])
2017-05-09 12:42:02,036 : INFO : torch.Size([168, 168])
2017-05-09 12:42:02,037 : INFO : torch.Size([168])
2017-05-09 12:42:02,037 : INFO : torch.Size([168, 300])
2017-05-09 12:42:02,038 : INFO : torch.Size([168])
2017-05-09 12:42:02,038 : INFO : torch.Size([168, 168])
2017-05-09 12:42:02,039 : INFO : torch.Size([168])
2017-05-09 12:42:02,039 : INFO : torch.Size([3, 168])
2017-05-09 12:42:02,040 : INFO : torch.Size([3])
2017-05-09 12:42:02,041 : INFO : sum
2017-05-09 12:42:02,041 : INFO : 316347
2017-05-09 12:42:02,042 : INFO : ____________
2017-05-09 12:42:25,392 : INFO : LOG_FILE
2017-05-09 12:42:25,393 : INFO : _________________________________start___________________________________
2017-05-09 12:42:25,406 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 12:42:25,410 : INFO : ==> SST vocabulary size : 105
2017-05-09 12:42:27,588 : INFO : _param count_
2017-05-09 12:42:27,590 : INFO : torch.Size([168, 300])
2017-05-09 12:42:27,591 : INFO : torch.Size([168])
2017-05-09 12:42:27,591 : INFO : torch.Size([168, 168])
2017-05-09 12:42:27,592 : INFO : torch.Size([168])
2017-05-09 12:42:27,592 : INFO : torch.Size([168, 168])
2017-05-09 12:42:27,593 : INFO : torch.Size([168])
2017-05-09 12:42:27,593 : INFO : torch.Size([168, 300])
2017-05-09 12:42:27,594 : INFO : torch.Size([168])
2017-05-09 12:42:27,595 : INFO : torch.Size([168, 300])
2017-05-09 12:42:27,595 : INFO : torch.Size([168])
2017-05-09 12:42:27,596 : INFO : torch.Size([168, 168])
2017-05-09 12:42:27,596 : INFO : torch.Size([168])
2017-05-09 12:42:27,597 : INFO : torch.Size([168, 300])
2017-05-09 12:42:27,597 : INFO : torch.Size([168])
2017-05-09 12:42:27,598 : INFO : torch.Size([168, 168])
2017-05-09 12:42:27,599 : INFO : torch.Size([168])
2017-05-09 12:42:27,599 : INFO : torch.Size([3, 168])
2017-05-09 12:42:27,600 : INFO : torch.Size([3])
2017-05-09 12:42:27,600 : INFO : sum
2017-05-09 12:42:27,601 : INFO : 316347
2017-05-09 12:42:27,601 : INFO : ____________
2017-05-09 12:42:54,078 : INFO : (1, 1.1577483415603638)
2017-05-09 12:43:26,596 : INFO : LOG_FILE
2017-05-09 12:43:26,597 : INFO : _________________________________start___________________________________
2017-05-09 12:43:26,610 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 12:43:26,613 : INFO : ==> SST vocabulary size : 105
2017-05-09 12:43:28,770 : INFO : _param count_
2017-05-09 12:43:28,773 : INFO : torch.Size([168, 300])
2017-05-09 12:43:28,773 : INFO : torch.Size([168])
2017-05-09 12:43:28,774 : INFO : torch.Size([168, 168])
2017-05-09 12:43:28,774 : INFO : torch.Size([168])
2017-05-09 12:43:28,775 : INFO : torch.Size([168, 168])
2017-05-09 12:43:28,776 : INFO : torch.Size([168])
2017-05-09 12:43:28,776 : INFO : torch.Size([168, 300])
2017-05-09 12:43:28,777 : INFO : torch.Size([168])
2017-05-09 12:43:28,777 : INFO : torch.Size([168, 300])
2017-05-09 12:43:28,778 : INFO : torch.Size([168])
2017-05-09 12:43:28,778 : INFO : torch.Size([168, 168])
2017-05-09 12:43:28,779 : INFO : torch.Size([168])
2017-05-09 12:43:28,779 : INFO : torch.Size([168, 300])
2017-05-09 12:43:28,780 : INFO : torch.Size([168])
2017-05-09 12:43:28,781 : INFO : torch.Size([168, 168])
2017-05-09 12:43:28,781 : INFO : torch.Size([168])
2017-05-09 12:43:28,782 : INFO : torch.Size([3, 168])
2017-05-09 12:43:28,782 : INFO : torch.Size([3])
2017-05-09 12:43:28,783 : INFO : sum
2017-05-09 12:43:28,783 : INFO : 316347
2017-05-09 12:43:28,784 : INFO : ____________
2017-05-09 13:28:38,410 : INFO : LOG_FILE
2017-05-09 13:28:38,410 : INFO : _________________________________start___________________________________
2017-05-09 13:28:38,422 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 13:28:38,425 : INFO : ==> SST vocabulary size : 105
2017-05-09 13:28:40,548 : INFO : _param count_
2017-05-09 13:28:40,551 : INFO : torch.Size([168, 300])
2017-05-09 13:28:40,551 : INFO : torch.Size([168])
2017-05-09 13:28:40,552 : INFO : torch.Size([168, 168])
2017-05-09 13:28:40,552 : INFO : torch.Size([168])
2017-05-09 13:28:40,553 : INFO : torch.Size([168, 168])
2017-05-09 13:28:40,553 : INFO : torch.Size([168])
2017-05-09 13:28:40,554 : INFO : torch.Size([168, 300])
2017-05-09 13:28:40,555 : INFO : torch.Size([168])
2017-05-09 13:28:40,555 : INFO : torch.Size([168, 300])
2017-05-09 13:28:40,556 : INFO : torch.Size([168])
2017-05-09 13:28:40,556 : INFO : torch.Size([168, 168])
2017-05-09 13:28:40,557 : INFO : torch.Size([168])
2017-05-09 13:28:40,557 : INFO : torch.Size([168, 300])
2017-05-09 13:28:40,558 : INFO : torch.Size([168])
2017-05-09 13:28:40,558 : INFO : torch.Size([168, 168])
2017-05-09 13:28:40,559 : INFO : torch.Size([168])
2017-05-09 13:28:40,560 : INFO : torch.Size([3, 168])
2017-05-09 13:28:40,560 : INFO : torch.Size([3])
2017-05-09 13:28:40,561 : INFO : sum
2017-05-09 13:28:40,561 : INFO : 316347
2017-05-09 13:28:40,562 : INFO : ____________
2017-05-09 13:29:58,418 : INFO : LOG_FILE
2017-05-09 13:29:58,419 : INFO : _________________________________start___________________________________
2017-05-09 13:29:58,432 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 13:29:58,435 : INFO : ==> SST vocabulary size : 105
2017-05-09 13:30:00,611 : INFO : _param count_
2017-05-09 13:30:00,613 : INFO : torch.Size([168, 300])
2017-05-09 13:30:00,614 : INFO : torch.Size([168])
2017-05-09 13:30:00,615 : INFO : torch.Size([168, 168])
2017-05-09 13:30:00,615 : INFO : torch.Size([168])
2017-05-09 13:30:00,616 : INFO : torch.Size([168, 168])
2017-05-09 13:30:00,616 : INFO : torch.Size([168])
2017-05-09 13:30:00,617 : INFO : torch.Size([168, 300])
2017-05-09 13:30:00,617 : INFO : torch.Size([168])
2017-05-09 13:30:00,618 : INFO : torch.Size([168, 300])
2017-05-09 13:30:00,618 : INFO : torch.Size([168])
2017-05-09 13:30:00,619 : INFO : torch.Size([168, 168])
2017-05-09 13:30:00,619 : INFO : torch.Size([168])
2017-05-09 13:30:00,620 : INFO : torch.Size([168, 300])
2017-05-09 13:30:00,621 : INFO : torch.Size([168])
2017-05-09 13:30:00,621 : INFO : torch.Size([168, 168])
2017-05-09 13:30:00,622 : INFO : torch.Size([168])
2017-05-09 13:30:00,622 : INFO : torch.Size([3, 168])
2017-05-09 13:30:00,623 : INFO : torch.Size([3])
2017-05-09 13:30:00,623 : INFO : sum
2017-05-09 13:30:00,624 : INFO : 316347
2017-05-09 13:30:00,624 : INFO : ____________
2017-05-09 13:32:35,848 : INFO : LOG_FILE
2017-05-09 13:32:35,848 : INFO : _________________________________start___________________________________
2017-05-09 13:32:35,861 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 13:32:35,864 : INFO : ==> SST vocabulary size : 105
2017-05-09 13:32:38,016 : INFO : _param count_
2017-05-09 13:32:38,019 : INFO : torch.Size([168, 300])
2017-05-09 13:32:38,019 : INFO : torch.Size([168])
2017-05-09 13:32:38,020 : INFO : torch.Size([168, 168])
2017-05-09 13:32:38,020 : INFO : torch.Size([168])
2017-05-09 13:32:38,021 : INFO : torch.Size([168, 168])
2017-05-09 13:32:38,021 : INFO : torch.Size([168])
2017-05-09 13:32:38,022 : INFO : torch.Size([168, 300])
2017-05-09 13:32:38,022 : INFO : torch.Size([168])
2017-05-09 13:32:38,023 : INFO : torch.Size([168, 300])
2017-05-09 13:32:38,024 : INFO : torch.Size([168])
2017-05-09 13:32:38,024 : INFO : torch.Size([168, 168])
2017-05-09 13:32:38,025 : INFO : torch.Size([168])
2017-05-09 13:32:38,025 : INFO : torch.Size([168, 300])
2017-05-09 13:32:38,026 : INFO : torch.Size([168])
2017-05-09 13:32:38,026 : INFO : torch.Size([168, 168])
2017-05-09 13:32:38,027 : INFO : torch.Size([168])
2017-05-09 13:32:38,027 : INFO : torch.Size([3, 168])
2017-05-09 13:32:38,028 : INFO : torch.Size([3])
2017-05-09 13:32:38,028 : INFO : sum
2017-05-09 13:32:38,029 : INFO : 316347
2017-05-09 13:32:38,030 : INFO : ____________
2017-05-09 15:53:28,993 : INFO : LOG_FILE
2017-05-09 15:53:28,993 : INFO : _________________________________start___________________________________
2017-05-09 15:53:29,000 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 15:53:29,002 : INFO : ==> SST vocabulary size : 105
2017-05-09 15:53:31,192 : INFO : _param count_
2017-05-09 15:53:31,193 : INFO : torch.Size([168, 300])
2017-05-09 15:53:31,193 : INFO : torch.Size([168])
2017-05-09 15:53:31,193 : INFO : torch.Size([168, 168])
2017-05-09 15:53:31,193 : INFO : torch.Size([168])
2017-05-09 15:53:31,193 : INFO : torch.Size([168, 168])
2017-05-09 15:53:31,193 : INFO : torch.Size([168])
2017-05-09 15:53:31,193 : INFO : torch.Size([168, 300])
2017-05-09 15:53:31,193 : INFO : torch.Size([168])
2017-05-09 15:53:31,194 : INFO : torch.Size([168, 300])
2017-05-09 15:53:31,194 : INFO : torch.Size([168])
2017-05-09 15:53:31,194 : INFO : torch.Size([168, 168])
2017-05-09 15:53:31,194 : INFO : torch.Size([168])
2017-05-09 15:53:31,194 : INFO : torch.Size([168, 300])
2017-05-09 15:53:31,194 : INFO : torch.Size([168])
2017-05-09 15:53:31,194 : INFO : torch.Size([168, 168])
2017-05-09 15:53:31,195 : INFO : torch.Size([168])
2017-05-09 15:53:31,195 : INFO : torch.Size([3, 168])
2017-05-09 15:53:31,195 : INFO : torch.Size([3])
2017-05-09 15:53:31,195 : INFO : sum
2017-05-09 15:53:31,195 : INFO : 316347
2017-05-09 15:53:31,195 : INFO : ____________
2017-05-09 15:53:31,385 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:31,387 : INFO : (2, 1.0745457410812378)
2017-05-09 15:53:31,389 : INFO : (3, 1.1600637435913086)
2017-05-09 15:53:31,391 : INFO : (6, 1.1321847438812256)
2017-05-09 15:53:31,394 : INFO : (7, 1.1577483415603638)
2017-05-09 15:53:31,396 : INFO : (10, 1.1717875003814697)
2017-05-09 15:53:31,400 : INFO : (11, 2.3070101737976074)
2017-05-09 15:53:31,403 : INFO : (9, 3.467470169067383)
2017-05-09 15:53:31,406 : INFO : (8, 5.735894680023193)
2017-05-09 15:53:31,408 : INFO : (12, 1.1339832544326782)
2017-05-09 15:53:31,410 : INFO : (13, 8.002062797546387)
2017-05-09 15:53:31,413 : INFO : (5, 9.174448013305664)
2017-05-09 15:53:31,416 : INFO : (4, 13.670205116271973)
2017-05-09 15:53:31,418 : INFO : (14, 1.188725233078003)
2017-05-09 15:53:31,420 : INFO : (15, 1.1875367164611816)
2017-05-09 15:53:31,423 : INFO : (17, 1.17649507522583)
2017-05-09 15:53:31,425 : INFO : (18, 1.1859716176986694)
2017-05-09 15:53:31,428 : INFO : (21, 1.148364782333374)
2017-05-09 15:53:31,430 : INFO : (20, 2.3133182525634766)
2017-05-09 15:53:31,432 : INFO : (19, 4.6813507080078125)
2017-05-09 15:53:31,435 : INFO : (22, 1.1968988180160522)
2017-05-09 15:53:31,438 : INFO : (23, 1.0630097389221191)
2017-05-09 15:53:31,440 : INFO : (24, 1.0733129978179932)
2017-05-09 15:53:31,442 : INFO : (26, 1.1568262577056885)
2017-05-09 15:53:31,444 : INFO : (27, 1.1412229537963867)
2017-05-09 15:53:31,446 : INFO : (29, 1.1548422574996948)
2017-05-09 15:53:31,450 : INFO : (28, 4.592202186584473)
2017-05-09 15:53:31,453 : INFO : (30, 1.1657209396362305)
2017-05-09 15:53:31,455 : INFO : (33, 1.1145955324172974)
2017-05-09 15:53:31,457 : INFO : (35, 1.1548422574996948)
2017-05-09 15:53:31,460 : INFO : (34, 3.3807010650634766)
2017-05-09 15:53:31,462 : INFO : (36, 4.50806999206543)
2017-05-09 15:53:31,464 : INFO : (32, 5.662715435028076)
2017-05-09 15:53:31,467 : INFO : (31, 12.58568000793457)
2017-05-09 15:53:31,472 : INFO : (25, 22.970571517944336)
2017-05-09 15:53:31,475 : INFO : (37, 1.2072006464004517)
2017-05-09 15:53:31,478 : INFO : (16, 41.31589126586914)
2017-05-09 15:53:31,482 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:31,484 : INFO : (2, 2.256925582885742)
2017-05-09 15:53:31,488 : INFO : (3, 1.188725233078003)
2017-05-09 15:53:31,490 : INFO : (5, 1.1619560718536377)
2017-05-09 15:53:31,492 : INFO : (6, 1.1730492115020752)
2017-05-09 15:53:31,495 : INFO : (7, 1.1717875003814697)
2017-05-09 15:53:31,497 : INFO : (8, 1.1558316946029663)
2017-05-09 15:53:31,499 : INFO : (10, 1.1548422574996948)
2017-05-09 15:53:31,502 : INFO : (9, 4.64690637588501)
2017-05-09 15:53:31,504 : INFO : (11, 1.0916519165039062)
2017-05-09 15:53:31,506 : INFO : (12, 1.1321847438812256)
2017-05-09 15:53:31,509 : INFO : (14, 1.1339832544326782)
2017-05-09 15:53:31,512 : INFO : (13, 11.48015022277832)
2017-05-09 15:53:31,514 : INFO : (15, 1.153801441192627)
2017-05-09 15:53:31,516 : INFO : (16, 1.17649507522583)
2017-05-09 15:53:31,519 : INFO : (17, 1.149524211883545)
2017-05-09 15:53:31,521 : INFO : (18, 1.1548422574996948)
2017-05-09 15:53:31,523 : INFO : (20, 1.1619560718536377)
2017-05-09 15:53:31,525 : INFO : (22, 1.1859716176986694)
2017-05-09 15:53:31,528 : INFO : (23, 2.281428813934326)
2017-05-09 15:53:31,530 : INFO : (24, 1.1686874628067017)
2017-05-09 15:53:31,532 : INFO : (27, 1.1727831363677979)
2017-05-09 15:53:31,534 : INFO : (29, 1.1546502113342285)
2017-05-09 15:53:31,537 : INFO : (30, 1.1597667932510376)
2017-05-09 15:53:31,539 : INFO : (31, 1.185125708580017)
2017-05-09 15:53:31,541 : INFO : (32, 3.502020835876465)
2017-05-09 15:53:31,544 : INFO : (33, 1.1817998886108398)
2017-05-09 15:53:31,546 : INFO : (34, 1.155847430229187)
2017-05-09 15:53:31,549 : INFO : (35, 2.317129611968994)
2017-05-09 15:53:31,552 : INFO : (28, 10.493244171142578)
2017-05-09 15:53:31,554 : INFO : (26, 11.677874565124512)
2017-05-09 15:53:31,556 : INFO : (25, 12.846562385559082)
2017-05-09 15:53:31,559 : INFO : (21, 17.38468360900879)
2017-05-09 15:53:31,562 : INFO : (19, 21.961782455444336)
2017-05-09 15:53:31,564 : INFO : (36, 1.2072006464004517)
2017-05-09 15:53:31,568 : INFO : (4, 40.32240676879883)
2017-05-09 15:53:31,573 : INFO : (1, 0.0)
2017-05-09 15:53:31,575 : INFO : (2, 0.0)
2017-05-09 15:53:31,577 : INFO : (3, 0.0)
2017-05-09 15:53:31,579 : INFO : (5, 0.0)
2017-05-09 15:53:31,581 : INFO : (6, 0.0)
2017-05-09 15:53:31,583 : INFO : (7, 0.0)
2017-05-09 15:53:31,585 : INFO : (8, 0.0)
2017-05-09 15:53:31,587 : INFO : (10, 0.0)
2017-05-09 15:53:31,589 : INFO : (9, 0.0)
2017-05-09 15:53:31,591 : INFO : (11, 0.0)
2017-05-09 15:53:31,593 : INFO : (12, 0.0)
2017-05-09 15:53:31,595 : INFO : (14, 0.0)
2017-05-09 15:53:31,599 : INFO : (13, 0.0)
2017-05-09 15:53:31,601 : INFO : (15, 0.0)
2017-05-09 15:53:31,605 : INFO : (16, 0.0)
2017-05-09 15:53:31,607 : INFO : (17, 0.0)
2017-05-09 15:53:31,609 : INFO : (18, 0.0)
2017-05-09 15:53:31,611 : INFO : (20, 0.0)
2017-05-09 15:53:31,613 : INFO : (22, 0.0)
2017-05-09 15:53:31,615 : INFO : (23, 0.0)
2017-05-09 15:53:31,617 : INFO : (24, 0.0)
2017-05-09 15:53:31,620 : INFO : (27, 0.0)
2017-05-09 15:53:31,622 : INFO : (29, 0.0)
2017-05-09 15:53:31,625 : INFO : (30, 0.0)
2017-05-09 15:53:31,627 : INFO : (31, 0.0)
2017-05-09 15:53:31,629 : INFO : (32, 0.0)
2017-05-09 15:53:31,631 : INFO : (33, 0.0)
2017-05-09 15:53:31,633 : INFO : (34, 0.0)
2017-05-09 15:53:31,635 : INFO : (35, 0.0)
2017-05-09 15:53:31,639 : INFO : (28, 0.0)
2017-05-09 15:53:31,641 : INFO : (26, 0.0)
2017-05-09 15:53:31,644 : INFO : (25, 0.0)
2017-05-09 15:53:31,646 : INFO : (21, 0.0)
2017-05-09 15:53:31,649 : INFO : (19, 0.0)
2017-05-09 15:53:31,651 : INFO : (36, 0.0)
2017-05-09 15:53:31,654 : INFO : (4, 0.0)
2017-05-09 15:53:31,659 : INFO : (1, 0.0)
2017-05-09 15:53:31,661 : INFO : (2, 0.0)
2017-05-09 15:53:31,663 : INFO : (3, 0.0)
2017-05-09 15:53:31,665 : INFO : (6, 0.0)
2017-05-09 15:53:31,668 : INFO : (7, 0.0)
2017-05-09 15:53:31,670 : INFO : (10, 0.0)
2017-05-09 15:53:31,672 : INFO : (11, 0.0)
2017-05-09 15:53:31,674 : INFO : (9, 0.0)
2017-05-09 15:53:31,677 : INFO : (8, 0.0)
2017-05-09 15:53:31,679 : INFO : (12, 0.0)
2017-05-09 15:53:31,681 : INFO : (13, 0.0)
2017-05-09 15:53:31,683 : INFO : (5, 0.0)
2017-05-09 15:53:31,686 : INFO : (4, 0.0)
2017-05-09 15:53:31,688 : INFO : (14, 0.0)
2017-05-09 15:53:31,690 : INFO : (15, 0.0)
2017-05-09 15:53:31,692 : INFO : (17, 0.0)
2017-05-09 15:53:31,694 : INFO : (18, 0.0)
2017-05-09 15:53:31,696 : INFO : (21, 0.0)
2017-05-09 15:53:31,698 : INFO : (20, 0.0)
2017-05-09 15:53:31,701 : INFO : (19, 0.0)
2017-05-09 15:53:31,703 : INFO : (22, 0.0)
2017-05-09 15:53:31,705 : INFO : (23, 0.0)
2017-05-09 15:53:31,707 : INFO : (24, 0.0)
2017-05-09 15:53:31,709 : INFO : (26, 0.0)
2017-05-09 15:53:31,711 : INFO : (27, 0.0)
2017-05-09 15:53:31,713 : INFO : (29, 0.0)
2017-05-09 15:53:31,715 : INFO : (28, 0.0)
2017-05-09 15:53:31,717 : INFO : (30, 0.0)
2017-05-09 15:53:31,719 : INFO : (33, 0.0)
2017-05-09 15:53:31,721 : INFO : (35, 0.0)
2017-05-09 15:53:31,723 : INFO : (34, 0.0)
2017-05-09 15:53:31,725 : INFO : (36, 0.0)
2017-05-09 15:53:31,727 : INFO : (32, 0.0)
2017-05-09 15:53:31,729 : INFO : (31, 0.0)
2017-05-09 15:53:31,733 : INFO : (25, 0.0)
2017-05-09 15:53:31,734 : INFO : (37, 0.0)
2017-05-09 15:53:31,737 : INFO : (16, 0.0)
2017-05-09 15:53:31,742 : INFO : (4, 0.0)
2017-05-09 15:53:31,744 : INFO : (3, 0.0)
2017-05-09 15:53:31,746 : INFO : (2, 0.0)
2017-05-09 15:53:31,748 : INFO : (5, 0.0)
2017-05-09 15:53:31,750 : INFO : (6, 0.0)
2017-05-09 15:53:31,752 : INFO : (7, 0.0)
2017-05-09 15:53:31,754 : INFO : (9, 0.0)
2017-05-09 15:53:31,756 : INFO : (10, 0.0)
2017-05-09 15:53:31,758 : INFO : (11, 0.0)
2017-05-09 15:53:31,759 : INFO : (13, 0.0)
2017-05-09 15:53:31,762 : INFO : (14, 0.0)
2017-05-09 15:53:31,763 : INFO : (15, 0.0)
2017-05-09 15:53:31,765 : INFO : (16, 0.0)
2017-05-09 15:53:31,767 : INFO : (17, 0.0)
2017-05-09 15:53:31,769 : INFO : (18, 0.0)
2017-05-09 15:53:31,771 : INFO : (19, 0.0)
2017-05-09 15:53:31,773 : INFO : (22, 0.0)
2017-05-09 15:53:31,775 : INFO : (21, 0.0)
2017-05-09 15:53:31,778 : INFO : (20, 0.0)
2017-05-09 15:53:31,781 : INFO : (12, 0.0)
2017-05-09 15:53:31,783 : INFO : (23, 0.0)
2017-05-09 15:53:31,786 : INFO : (8, 0.0)
2017-05-09 15:53:31,788 : INFO : (1, 0.0)
2017-05-09 15:53:31,792 : INFO : (1, 0.0)
2017-05-09 15:53:31,794 : INFO : (2, 0.0)
2017-05-09 15:53:31,796 : INFO : (3, 0.0)
2017-05-09 15:53:31,798 : INFO : (5, 0.0)
2017-05-09 15:53:31,800 : INFO : (8, 0.0)
2017-05-09 15:53:31,805 : INFO : (9, 0.0)
2017-05-09 15:53:31,807 : INFO : (7, 0.0)
2017-05-09 15:53:31,809 : INFO : (10, 0.0)
2017-05-09 15:53:31,811 : INFO : (12, 0.0)
2017-05-09 15:53:31,813 : INFO : (11, 0.0)
2017-05-09 15:53:31,815 : INFO : (6, 0.0)
2017-05-09 15:53:31,818 : INFO : (4, 0.0)
2017-05-09 15:53:31,820 : INFO : (13, 0.0)
2017-05-09 15:53:31,822 : INFO : (14, 0.0)
2017-05-09 15:53:31,824 : INFO : (15, 0.0)
2017-05-09 15:53:31,827 : INFO : (16, 0.0)
2017-05-09 15:53:31,829 : INFO : (17, 0.0)
2017-05-09 15:53:31,831 : INFO : (19, 0.0)
2017-05-09 15:53:31,833 : INFO : (20, 0.0)
2017-05-09 15:53:31,835 : INFO : (21, 0.0)
2017-05-09 15:53:31,838 : INFO : (18, 0.0)
2017-05-09 15:53:31,839 : INFO : ==> Train loss   : 1.082739
2017-05-09 15:53:31,839 : INFO : Epoch
2017-05-09 15:53:31,839 : INFO : 0
2017-05-09 15:53:31,839 : INFO : train percentage
2017-05-09 15:53:31,840 : INFO : 0.0
2017-05-09 15:53:31,840 : INFO : Epoch
2017-05-09 15:53:31,840 : INFO : 0
2017-05-09 15:53:31,840 : INFO : dev percentage
2017-05-09 15:53:31,840 : INFO : 0.0
2017-05-09 15:53:31,840 : INFO : Epoch
2017-05-09 15:53:31,840 : INFO : 0
2017-05-09 15:53:31,840 : INFO : test percentage
2017-05-09 15:53:31,841 : INFO : 0.0
2017-05-09 15:53:31,845 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:31,848 : INFO : (2, 1.0745457410812378)
2017-05-09 15:53:31,851 : INFO : (3, 1.1600637435913086)
2017-05-09 15:53:31,853 : INFO : (6, 1.1321847438812256)
2017-05-09 15:53:31,855 : INFO : (7, 1.1577483415603638)
2017-05-09 15:53:31,858 : INFO : (10, 1.1717875003814697)
2017-05-09 15:53:31,860 : INFO : (11, 2.3070101737976074)
2017-05-09 15:53:31,865 : INFO : (9, 3.467470169067383)
2017-05-09 15:53:31,868 : INFO : (8, 5.735894680023193)
2017-05-09 15:53:31,871 : INFO : (12, 1.1339832544326782)
2017-05-09 15:53:31,874 : INFO : (13, 8.002062797546387)
2017-05-09 15:53:31,876 : INFO : (5, 9.174448013305664)
2017-05-09 15:53:31,879 : INFO : (4, 13.670205116271973)
2017-05-09 15:53:31,881 : INFO : (14, 1.188725233078003)
2017-05-09 15:53:31,883 : INFO : (15, 1.1875367164611816)
2017-05-09 15:53:31,885 : INFO : (17, 1.17649507522583)
2017-05-09 15:53:31,888 : INFO : (18, 1.1859716176986694)
2017-05-09 15:53:31,890 : INFO : (21, 1.148364782333374)
2017-05-09 15:53:31,893 : INFO : (20, 2.3133182525634766)
2017-05-09 15:53:31,895 : INFO : (19, 4.6813507080078125)
2017-05-09 15:53:31,897 : INFO : (22, 1.1968988180160522)
2017-05-09 15:53:31,899 : INFO : (23, 1.0630097389221191)
2017-05-09 15:53:31,902 : INFO : (24, 1.0733129978179932)
2017-05-09 15:53:31,904 : INFO : (26, 1.1568262577056885)
2017-05-09 15:53:31,906 : INFO : (27, 1.1412229537963867)
2017-05-09 15:53:31,909 : INFO : (29, 1.1548422574996948)
2017-05-09 15:53:31,911 : INFO : (28, 4.592202186584473)
2017-05-09 15:53:31,913 : INFO : (30, 1.1657209396362305)
2017-05-09 15:53:31,916 : INFO : (33, 1.1145955324172974)
2017-05-09 15:53:31,918 : INFO : (35, 1.1548422574996948)
2017-05-09 15:53:31,920 : INFO : (34, 3.3807010650634766)
2017-05-09 15:53:31,922 : INFO : (36, 4.50806999206543)
2017-05-09 15:53:31,925 : INFO : (32, 5.662715435028076)
2017-05-09 15:53:31,927 : INFO : (31, 12.58568000793457)
2017-05-09 15:53:31,931 : INFO : (25, 22.970571517944336)
2017-05-09 15:53:31,933 : INFO : (37, 1.2072006464004517)
2017-05-09 15:53:31,936 : INFO : (16, 41.31589126586914)
2017-05-09 15:53:31,940 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:31,942 : INFO : (2, 2.256925582885742)
2017-05-09 15:53:31,944 : INFO : (3, 1.188725233078003)
2017-05-09 15:53:31,946 : INFO : (5, 1.1619560718536377)
2017-05-09 15:53:31,949 : INFO : (6, 1.1730492115020752)
2017-05-09 15:53:31,951 : INFO : (7, 1.1717875003814697)
2017-05-09 15:53:31,953 : INFO : (8, 1.1558316946029663)
2017-05-09 15:53:31,956 : INFO : (10, 1.1548422574996948)
2017-05-09 15:53:31,960 : INFO : (9, 4.64690637588501)
2017-05-09 15:53:31,963 : INFO : (11, 1.0916519165039062)
2017-05-09 15:53:31,966 : INFO : (12, 1.1321847438812256)
2017-05-09 15:53:31,969 : INFO : (14, 1.1339832544326782)
2017-05-09 15:53:31,973 : INFO : (13, 11.48015022277832)
2017-05-09 15:53:31,976 : INFO : (15, 1.153801441192627)
2017-05-09 15:53:31,980 : INFO : (16, 1.17649507522583)
2017-05-09 15:53:31,982 : INFO : (17, 1.149524211883545)
2017-05-09 15:53:31,984 : INFO : (18, 1.1548422574996948)
2017-05-09 15:53:31,987 : INFO : (20, 1.1619560718536377)
2017-05-09 15:53:31,989 : INFO : (22, 1.1859716176986694)
2017-05-09 15:53:31,991 : INFO : (23, 2.281428813934326)
2017-05-09 15:53:31,994 : INFO : (24, 1.1686874628067017)
2017-05-09 15:53:31,996 : INFO : (27, 1.1727831363677979)
2017-05-09 15:53:31,998 : INFO : (29, 1.1546502113342285)
2017-05-09 15:53:32,001 : INFO : (30, 1.1597667932510376)
2017-05-09 15:53:32,004 : INFO : (31, 1.185125708580017)
2017-05-09 15:53:32,008 : INFO : (32, 3.502020835876465)
2017-05-09 15:53:32,010 : INFO : (33, 1.1817998886108398)
2017-05-09 15:53:32,012 : INFO : (34, 1.155847430229187)
2017-05-09 15:53:32,015 : INFO : (35, 2.317129611968994)
2017-05-09 15:53:32,018 : INFO : (28, 10.493244171142578)
2017-05-09 15:53:32,020 : INFO : (26, 11.677874565124512)
2017-05-09 15:53:32,023 : INFO : (25, 12.846562385559082)
2017-05-09 15:53:32,026 : INFO : (21, 17.38468360900879)
2017-05-09 15:53:32,029 : INFO : (19, 21.961782455444336)
2017-05-09 15:53:32,031 : INFO : (36, 1.2072006464004517)
2017-05-09 15:53:32,035 : INFO : (4, 40.32240676879883)
2017-05-09 15:53:32,041 : INFO : (1, 0.0)
2017-05-09 15:53:32,043 : INFO : (2, 0.0)
2017-05-09 15:53:32,045 : INFO : (3, 0.0)
2017-05-09 15:53:32,047 : INFO : (5, 0.0)
2017-05-09 15:53:32,049 : INFO : (6, 0.0)
2017-05-09 15:53:32,051 : INFO : (7, 0.0)
2017-05-09 15:53:32,053 : INFO : (8, 0.0)
2017-05-09 15:53:32,055 : INFO : (10, 0.0)
2017-05-09 15:53:32,058 : INFO : (9, 0.0)
2017-05-09 15:53:32,060 : INFO : (11, 0.0)
2017-05-09 15:53:32,062 : INFO : (12, 0.0)
2017-05-09 15:53:32,064 : INFO : (14, 0.0)
2017-05-09 15:53:32,067 : INFO : (13, 0.0)
2017-05-09 15:53:32,070 : INFO : (15, 0.0)
2017-05-09 15:53:32,072 : INFO : (16, 0.0)
2017-05-09 15:53:32,074 : INFO : (17, 0.0)
2017-05-09 15:53:32,076 : INFO : (18, 0.0)
2017-05-09 15:53:32,078 : INFO : (20, 0.0)
2017-05-09 15:53:32,080 : INFO : (22, 0.0)
2017-05-09 15:53:32,083 : INFO : (23, 0.0)
2017-05-09 15:53:32,085 : INFO : (24, 0.0)
2017-05-09 15:53:32,088 : INFO : (27, 0.0)
2017-05-09 15:53:32,090 : INFO : (29, 0.0)
2017-05-09 15:53:32,092 : INFO : (30, 0.0)
2017-05-09 15:53:32,094 : INFO : (31, 0.0)
2017-05-09 15:53:32,096 : INFO : (32, 0.0)
2017-05-09 15:53:32,098 : INFO : (33, 0.0)
2017-05-09 15:53:32,101 : INFO : (34, 0.0)
2017-05-09 15:53:32,103 : INFO : (35, 0.0)
2017-05-09 15:53:32,106 : INFO : (28, 0.0)
2017-05-09 15:53:32,108 : INFO : (26, 0.0)
2017-05-09 15:53:32,110 : INFO : (25, 0.0)
2017-05-09 15:53:32,112 : INFO : (21, 0.0)
2017-05-09 15:53:32,115 : INFO : (19, 0.0)
2017-05-09 15:53:32,117 : INFO : (36, 0.0)
2017-05-09 15:53:32,120 : INFO : (4, 0.0)
2017-05-09 15:53:32,124 : INFO : (1, 0.0)
2017-05-09 15:53:32,126 : INFO : (2, 0.0)
2017-05-09 15:53:32,128 : INFO : (3, 0.0)
2017-05-09 15:53:32,130 : INFO : (6, 0.0)
2017-05-09 15:53:32,132 : INFO : (7, 0.0)
2017-05-09 15:53:32,135 : INFO : (10, 0.0)
2017-05-09 15:53:32,137 : INFO : (11, 0.0)
2017-05-09 15:53:32,139 : INFO : (9, 0.0)
2017-05-09 15:53:32,141 : INFO : (8, 0.0)
2017-05-09 15:53:32,143 : INFO : (12, 0.0)
2017-05-09 15:53:32,145 : INFO : (13, 0.0)
2017-05-09 15:53:32,147 : INFO : (5, 0.0)
2017-05-09 15:53:32,150 : INFO : (4, 0.0)
2017-05-09 15:53:32,152 : INFO : (14, 0.0)
2017-05-09 15:53:32,154 : INFO : (15, 0.0)
2017-05-09 15:53:32,156 : INFO : (17, 0.0)
2017-05-09 15:53:32,158 : INFO : (18, 0.0)
2017-05-09 15:53:32,160 : INFO : (21, 0.0)
2017-05-09 15:53:32,162 : INFO : (20, 0.0)
2017-05-09 15:53:32,164 : INFO : (19, 0.0)
2017-05-09 15:53:32,166 : INFO : (22, 0.0)
2017-05-09 15:53:32,168 : INFO : (23, 0.0)
2017-05-09 15:53:32,170 : INFO : (24, 0.0)
2017-05-09 15:53:32,172 : INFO : (26, 0.0)
2017-05-09 15:53:32,174 : INFO : (27, 0.0)
2017-05-09 15:53:32,176 : INFO : (29, 0.0)
2017-05-09 15:53:32,179 : INFO : (28, 0.0)
2017-05-09 15:53:32,181 : INFO : (30, 0.0)
2017-05-09 15:53:32,183 : INFO : (33, 0.0)
2017-05-09 15:53:32,185 : INFO : (35, 0.0)
2017-05-09 15:53:32,187 : INFO : (34, 0.0)
2017-05-09 15:53:32,189 : INFO : (36, 0.0)
2017-05-09 15:53:32,191 : INFO : (32, 0.0)
2017-05-09 15:53:32,193 : INFO : (31, 0.0)
2017-05-09 15:53:32,196 : INFO : (25, 0.0)
2017-05-09 15:53:32,198 : INFO : (37, 0.0)
2017-05-09 15:53:32,201 : INFO : (16, 0.0)
2017-05-09 15:53:32,210 : INFO : (4, 0.0)
2017-05-09 15:53:32,212 : INFO : (3, 0.0)
2017-05-09 15:53:32,214 : INFO : (2, 0.0)
2017-05-09 15:53:32,216 : INFO : (5, 0.0)
2017-05-09 15:53:32,218 : INFO : (6, 0.0)
2017-05-09 15:53:32,220 : INFO : (7, 0.0)
2017-05-09 15:53:32,222 : INFO : (9, 0.0)
2017-05-09 15:53:32,224 : INFO : (10, 0.0)
2017-05-09 15:53:32,226 : INFO : (11, 0.0)
2017-05-09 15:53:32,229 : INFO : (13, 0.0)
2017-05-09 15:53:32,231 : INFO : (14, 0.0)
2017-05-09 15:53:32,233 : INFO : (15, 0.0)
2017-05-09 15:53:32,235 : INFO : (16, 0.0)
2017-05-09 15:53:32,237 : INFO : (17, 0.0)
2017-05-09 15:53:32,240 : INFO : (18, 0.0)
2017-05-09 15:53:32,243 : INFO : (19, 0.0)
2017-05-09 15:53:32,245 : INFO : (22, 0.0)
2017-05-09 15:53:32,247 : INFO : (21, 0.0)
2017-05-09 15:53:32,250 : INFO : (20, 0.0)
2017-05-09 15:53:32,253 : INFO : (12, 0.0)
2017-05-09 15:53:32,255 : INFO : (23, 0.0)
2017-05-09 15:53:32,258 : INFO : (8, 0.0)
2017-05-09 15:53:32,260 : INFO : (1, 0.0)
2017-05-09 15:53:32,265 : INFO : (1, 0.0)
2017-05-09 15:53:32,267 : INFO : (2, 0.0)
2017-05-09 15:53:32,269 : INFO : (3, 0.0)
2017-05-09 15:53:32,271 : INFO : (5, 0.0)
2017-05-09 15:53:32,273 : INFO : (8, 0.0)
2017-05-09 15:53:32,276 : INFO : (9, 0.0)
2017-05-09 15:53:32,278 : INFO : (7, 0.0)
2017-05-09 15:53:32,280 : INFO : (10, 0.0)
2017-05-09 15:53:32,282 : INFO : (12, 0.0)
2017-05-09 15:53:32,285 : INFO : (11, 0.0)
2017-05-09 15:53:32,287 : INFO : (6, 0.0)
2017-05-09 15:53:32,290 : INFO : (4, 0.0)
2017-05-09 15:53:32,292 : INFO : (13, 0.0)
2017-05-09 15:53:32,294 : INFO : (14, 0.0)
2017-05-09 15:53:32,296 : INFO : (15, 0.0)
2017-05-09 15:53:32,298 : INFO : (16, 0.0)
2017-05-09 15:53:32,300 : INFO : (17, 0.0)
2017-05-09 15:53:32,302 : INFO : (19, 0.0)
2017-05-09 15:53:32,304 : INFO : (20, 0.0)
2017-05-09 15:53:32,306 : INFO : (21, 0.0)
2017-05-09 15:53:32,310 : INFO : (18, 0.0)
2017-05-09 15:53:32,310 : INFO : ==> Train loss   : 1.082739
2017-05-09 15:53:32,311 : INFO : Epoch
2017-05-09 15:53:32,311 : INFO : 1
2017-05-09 15:53:32,311 : INFO : train percentage
2017-05-09 15:53:32,311 : INFO : 0.0
2017-05-09 15:53:32,311 : INFO : Epoch
2017-05-09 15:53:32,311 : INFO : 1
2017-05-09 15:53:32,311 : INFO : dev percentage
2017-05-09 15:53:32,311 : INFO : 0.0
2017-05-09 15:53:32,312 : INFO : Epoch
2017-05-09 15:53:32,312 : INFO : 1
2017-05-09 15:53:32,312 : INFO : test percentage
2017-05-09 15:53:32,312 : INFO : 0.0
2017-05-09 15:53:32,317 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:32,319 : INFO : (2, 1.0745457410812378)
2017-05-09 15:53:32,322 : INFO : (3, 1.1600637435913086)
2017-05-09 15:53:32,324 : INFO : (6, 1.1321847438812256)
2017-05-09 15:53:32,326 : INFO : (7, 1.1577483415603638)
2017-05-09 15:53:32,328 : INFO : (10, 1.1717875003814697)
2017-05-09 15:53:32,331 : INFO : (11, 2.3070101737976074)
2017-05-09 15:53:32,333 : INFO : (9, 3.467470169067383)
2017-05-09 15:53:32,335 : INFO : (8, 5.735894680023193)
2017-05-09 15:53:32,338 : INFO : (12, 1.1339832544326782)
2017-05-09 15:53:32,340 : INFO : (13, 8.002062797546387)
2017-05-09 15:53:32,343 : INFO : (5, 9.174448013305664)
2017-05-09 15:53:32,345 : INFO : (4, 13.670205116271973)
2017-05-09 15:53:32,348 : INFO : (14, 1.188725233078003)
2017-05-09 15:53:32,350 : INFO : (15, 1.1875367164611816)
2017-05-09 15:53:32,353 : INFO : (17, 1.17649507522583)
2017-05-09 15:53:32,355 : INFO : (18, 1.1859716176986694)
2017-05-09 15:53:32,357 : INFO : (21, 1.148364782333374)
2017-05-09 15:53:32,359 : INFO : (20, 2.3133182525634766)
2017-05-09 15:53:32,362 : INFO : (19, 4.6813507080078125)
2017-05-09 15:53:32,364 : INFO : (22, 1.1968988180160522)
2017-05-09 15:53:32,367 : INFO : (23, 1.0630097389221191)
2017-05-09 15:53:32,369 : INFO : (24, 1.0733129978179932)
2017-05-09 15:53:32,371 : INFO : (26, 1.1568262577056885)
2017-05-09 15:53:32,373 : INFO : (27, 1.1412229537963867)
2017-05-09 15:53:32,376 : INFO : (29, 1.1548422574996948)
2017-05-09 15:53:32,378 : INFO : (28, 4.592202186584473)
2017-05-09 15:53:32,380 : INFO : (30, 1.1657209396362305)
2017-05-09 15:53:32,383 : INFO : (33, 1.1145955324172974)
2017-05-09 15:53:32,385 : INFO : (35, 1.1548422574996948)
2017-05-09 15:53:32,388 : INFO : (34, 3.3807010650634766)
2017-05-09 15:53:32,390 : INFO : (36, 4.50806999206543)
2017-05-09 15:53:32,392 : INFO : (32, 5.662715435028076)
2017-05-09 15:53:32,395 : INFO : (31, 12.58568000793457)
2017-05-09 15:53:32,400 : INFO : (25, 22.970571517944336)
2017-05-09 15:53:32,402 : INFO : (37, 1.2072006464004517)
2017-05-09 15:53:32,405 : INFO : (16, 41.31589126586914)
2017-05-09 15:53:32,411 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:32,437 : INFO : (2, 2.256925582885742)
2017-05-09 15:53:32,441 : INFO : (3, 1.188725233078003)
2017-05-09 15:53:32,444 : INFO : (5, 1.1619560718536377)
2017-05-09 15:53:32,446 : INFO : (6, 1.1730492115020752)
2017-05-09 15:53:32,448 : INFO : (7, 1.1717875003814697)
2017-05-09 15:53:32,451 : INFO : (8, 1.1558316946029663)
2017-05-09 15:53:32,453 : INFO : (10, 1.1548422574996948)
2017-05-09 15:53:32,455 : INFO : (9, 4.64690637588501)
2017-05-09 15:53:32,458 : INFO : (11, 1.0916519165039062)
2017-05-09 15:53:32,460 : INFO : (12, 1.1321847438812256)
2017-05-09 15:53:32,462 : INFO : (14, 1.1339832544326782)
2017-05-09 15:53:32,467 : INFO : (13, 11.48015022277832)
2017-05-09 15:53:32,469 : INFO : (15, 1.153801441192627)
2017-05-09 15:53:32,471 : INFO : (16, 1.17649507522583)
2017-05-09 15:53:32,473 : INFO : (17, 1.149524211883545)
2017-05-09 15:53:32,476 : INFO : (18, 1.1548422574996948)
2017-05-09 15:53:32,478 : INFO : (20, 1.1619560718536377)
2017-05-09 15:53:32,481 : INFO : (22, 1.1859716176986694)
2017-05-09 15:53:32,483 : INFO : (23, 2.281428813934326)
2017-05-09 15:53:32,485 : INFO : (24, 1.1686874628067017)
2017-05-09 15:53:32,487 : INFO : (27, 1.1727831363677979)
2017-05-09 15:53:32,490 : INFO : (29, 1.1546502113342285)
2017-05-09 15:53:32,492 : INFO : (30, 1.1597667932510376)
2017-05-09 15:53:32,494 : INFO : (31, 1.185125708580017)
2017-05-09 15:53:32,498 : INFO : (32, 3.502020835876465)
2017-05-09 15:53:32,500 : INFO : (33, 1.1817998886108398)
2017-05-09 15:53:32,502 : INFO : (34, 1.155847430229187)
2017-05-09 15:53:32,505 : INFO : (35, 2.317129611968994)
2017-05-09 15:53:32,509 : INFO : (28, 10.493244171142578)
2017-05-09 15:53:32,511 : INFO : (26, 11.677874565124512)
2017-05-09 15:53:32,513 : INFO : (25, 12.846562385559082)
2017-05-09 15:53:32,516 : INFO : (21, 17.38468360900879)
2017-05-09 15:53:32,519 : INFO : (19, 21.961782455444336)
2017-05-09 15:53:32,521 : INFO : (36, 1.2072006464004517)
2017-05-09 15:53:32,524 : INFO : (4, 40.32240676879883)
2017-05-09 15:53:32,533 : INFO : (1, 0.0)
2017-05-09 15:53:32,535 : INFO : (2, 0.0)
2017-05-09 15:53:32,537 : INFO : (3, 0.0)
2017-05-09 15:53:32,539 : INFO : (5, 0.0)
2017-05-09 15:53:32,541 : INFO : (6, 0.0)
2017-05-09 15:53:32,543 : INFO : (7, 0.0)
2017-05-09 15:53:32,545 : INFO : (8, 0.0)
2017-05-09 15:53:32,547 : INFO : (10, 0.0)
2017-05-09 15:53:32,549 : INFO : (9, 0.0)
2017-05-09 15:53:32,551 : INFO : (11, 0.0)
2017-05-09 15:53:32,553 : INFO : (12, 0.0)
2017-05-09 15:53:32,555 : INFO : (14, 0.0)
2017-05-09 15:53:32,559 : INFO : (13, 0.0)
2017-05-09 15:53:32,561 : INFO : (15, 0.0)
2017-05-09 15:53:32,564 : INFO : (16, 0.0)
2017-05-09 15:53:32,566 : INFO : (17, 0.0)
2017-05-09 15:53:32,568 : INFO : (18, 0.0)
2017-05-09 15:53:32,570 : INFO : (20, 0.0)
2017-05-09 15:53:32,572 : INFO : (22, 0.0)
2017-05-09 15:53:32,574 : INFO : (23, 0.0)
2017-05-09 15:53:32,576 : INFO : (24, 0.0)
2017-05-09 15:53:32,578 : INFO : (27, 0.0)
2017-05-09 15:53:32,580 : INFO : (29, 0.0)
2017-05-09 15:53:32,582 : INFO : (30, 0.0)
2017-05-09 15:53:32,584 : INFO : (31, 0.0)
2017-05-09 15:53:32,586 : INFO : (32, 0.0)
2017-05-09 15:53:32,588 : INFO : (33, 0.0)
2017-05-09 15:53:32,590 : INFO : (34, 0.0)
2017-05-09 15:53:32,592 : INFO : (35, 0.0)
2017-05-09 15:53:32,595 : INFO : (28, 0.0)
2017-05-09 15:53:32,597 : INFO : (26, 0.0)
2017-05-09 15:53:32,600 : INFO : (25, 0.0)
2017-05-09 15:53:32,602 : INFO : (21, 0.0)
2017-05-09 15:53:32,605 : INFO : (19, 0.0)
2017-05-09 15:53:32,607 : INFO : (36, 0.0)
2017-05-09 15:53:32,611 : INFO : (4, 0.0)
2017-05-09 15:53:32,615 : INFO : (1, 0.0)
2017-05-09 15:53:32,617 : INFO : (2, 0.0)
2017-05-09 15:53:32,619 : INFO : (3, 0.0)
2017-05-09 15:53:32,622 : INFO : (6, 0.0)
2017-05-09 15:53:32,624 : INFO : (7, 0.0)
2017-05-09 15:53:32,626 : INFO : (10, 0.0)
2017-05-09 15:53:32,628 : INFO : (11, 0.0)
2017-05-09 15:53:32,630 : INFO : (9, 0.0)
2017-05-09 15:53:32,633 : INFO : (8, 0.0)
2017-05-09 15:53:32,635 : INFO : (12, 0.0)
2017-05-09 15:53:32,637 : INFO : (13, 0.0)
2017-05-09 15:53:32,639 : INFO : (5, 0.0)
2017-05-09 15:53:32,642 : INFO : (4, 0.0)
2017-05-09 15:53:32,644 : INFO : (14, 0.0)
2017-05-09 15:53:32,646 : INFO : (15, 0.0)
2017-05-09 15:53:32,648 : INFO : (17, 0.0)
2017-05-09 15:53:32,650 : INFO : (18, 0.0)
2017-05-09 15:53:32,653 : INFO : (21, 0.0)
2017-05-09 15:53:32,655 : INFO : (20, 0.0)
2017-05-09 15:53:32,657 : INFO : (19, 0.0)
2017-05-09 15:53:32,659 : INFO : (22, 0.0)
2017-05-09 15:53:32,662 : INFO : (23, 0.0)
2017-05-09 15:53:32,664 : INFO : (24, 0.0)
2017-05-09 15:53:32,666 : INFO : (26, 0.0)
2017-05-09 15:53:32,668 : INFO : (27, 0.0)
2017-05-09 15:53:32,670 : INFO : (29, 0.0)
2017-05-09 15:53:32,673 : INFO : (28, 0.0)
2017-05-09 15:53:32,675 : INFO : (30, 0.0)
2017-05-09 15:53:32,678 : INFO : (33, 0.0)
2017-05-09 15:53:32,680 : INFO : (35, 0.0)
2017-05-09 15:53:32,682 : INFO : (34, 0.0)
2017-05-09 15:53:32,684 : INFO : (36, 0.0)
2017-05-09 15:53:32,686 : INFO : (32, 0.0)
2017-05-09 15:53:32,689 : INFO : (31, 0.0)
2017-05-09 15:53:32,692 : INFO : (25, 0.0)
2017-05-09 15:53:32,694 : INFO : (37, 0.0)
2017-05-09 15:53:32,697 : INFO : (16, 0.0)
2017-05-09 15:53:32,702 : INFO : (4, 0.0)
2017-05-09 15:53:32,704 : INFO : (3, 0.0)
2017-05-09 15:53:32,706 : INFO : (2, 0.0)
2017-05-09 15:53:32,708 : INFO : (5, 0.0)
2017-05-09 15:53:32,710 : INFO : (6, 0.0)
2017-05-09 15:53:32,712 : INFO : (7, 0.0)
2017-05-09 15:53:32,715 : INFO : (9, 0.0)
2017-05-09 15:53:32,717 : INFO : (10, 0.0)
2017-05-09 15:53:32,719 : INFO : (11, 0.0)
2017-05-09 15:53:32,721 : INFO : (13, 0.0)
2017-05-09 15:53:32,723 : INFO : (14, 0.0)
2017-05-09 15:53:32,725 : INFO : (15, 0.0)
2017-05-09 15:53:32,727 : INFO : (16, 0.0)
2017-05-09 15:53:32,729 : INFO : (17, 0.0)
2017-05-09 15:53:32,731 : INFO : (18, 0.0)
2017-05-09 15:53:32,733 : INFO : (19, 0.0)
2017-05-09 15:53:32,735 : INFO : (22, 0.0)
2017-05-09 15:53:32,737 : INFO : (21, 0.0)
2017-05-09 15:53:32,740 : INFO : (20, 0.0)
2017-05-09 15:53:32,743 : INFO : (12, 0.0)
2017-05-09 15:53:32,745 : INFO : (23, 0.0)
2017-05-09 15:53:32,748 : INFO : (8, 0.0)
2017-05-09 15:53:32,750 : INFO : (1, 0.0)
2017-05-09 15:53:32,755 : INFO : (1, 0.0)
2017-05-09 15:53:32,757 : INFO : (2, 0.0)
2017-05-09 15:53:32,759 : INFO : (3, 0.0)
2017-05-09 15:53:32,761 : INFO : (5, 0.0)
2017-05-09 15:53:32,763 : INFO : (8, 0.0)
2017-05-09 15:53:32,766 : INFO : (9, 0.0)
2017-05-09 15:53:32,768 : INFO : (7, 0.0)
2017-05-09 15:53:32,770 : INFO : (10, 0.0)
2017-05-09 15:53:32,772 : INFO : (12, 0.0)
2017-05-09 15:53:32,774 : INFO : (11, 0.0)
2017-05-09 15:53:32,776 : INFO : (6, 0.0)
2017-05-09 15:53:32,779 : INFO : (4, 0.0)
2017-05-09 15:53:32,781 : INFO : (13, 0.0)
2017-05-09 15:53:32,783 : INFO : (14, 0.0)
2017-05-09 15:53:32,785 : INFO : (15, 0.0)
2017-05-09 15:53:32,787 : INFO : (16, 0.0)
2017-05-09 15:53:32,789 : INFO : (17, 0.0)
2017-05-09 15:53:32,791 : INFO : (19, 0.0)
2017-05-09 15:53:32,793 : INFO : (20, 0.0)
2017-05-09 15:53:32,795 : INFO : (21, 0.0)
2017-05-09 15:53:32,799 : INFO : (18, 0.0)
2017-05-09 15:53:32,800 : INFO : ==> Train loss   : 1.082739
2017-05-09 15:53:32,800 : INFO : Epoch
2017-05-09 15:53:32,800 : INFO : 2
2017-05-09 15:53:32,800 : INFO : train percentage
2017-05-09 15:53:32,800 : INFO : 0.0
2017-05-09 15:53:32,800 : INFO : Epoch
2017-05-09 15:53:32,800 : INFO : 2
2017-05-09 15:53:32,800 : INFO : dev percentage
2017-05-09 15:53:32,801 : INFO : 0.0
2017-05-09 15:53:32,801 : INFO : Epoch
2017-05-09 15:53:32,801 : INFO : 2
2017-05-09 15:53:32,801 : INFO : test percentage
2017-05-09 15:53:32,801 : INFO : 0.0
2017-05-09 15:53:32,806 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:32,809 : INFO : (2, 1.0745457410812378)
2017-05-09 15:53:32,812 : INFO : (3, 1.1600637435913086)
2017-05-09 15:53:32,815 : INFO : (6, 1.1321847438812256)
2017-05-09 15:53:32,819 : INFO : (7, 1.1577483415603638)
2017-05-09 15:53:32,821 : INFO : (10, 1.1717875003814697)
2017-05-09 15:53:32,825 : INFO : (11, 2.3070101737976074)
2017-05-09 15:53:32,827 : INFO : (9, 3.467470169067383)
2017-05-09 15:53:32,830 : INFO : (8, 5.735894680023193)
2017-05-09 15:53:32,832 : INFO : (12, 1.1339832544326782)
2017-05-09 15:53:32,836 : INFO : (13, 8.002062797546387)
2017-05-09 15:53:32,838 : INFO : (5, 9.174448013305664)
2017-05-09 15:53:32,843 : INFO : (4, 13.670205116271973)
2017-05-09 15:53:32,845 : INFO : (14, 1.188725233078003)
2017-05-09 15:53:32,848 : INFO : (15, 1.1875367164611816)
2017-05-09 15:53:32,850 : INFO : (17, 1.17649507522583)
2017-05-09 15:53:32,853 : INFO : (18, 1.1859716176986694)
2017-05-09 15:53:32,855 : INFO : (21, 1.148364782333374)
2017-05-09 15:53:32,857 : INFO : (20, 2.3133182525634766)
2017-05-09 15:53:32,860 : INFO : (19, 4.6813507080078125)
2017-05-09 15:53:32,863 : INFO : (22, 1.1968988180160522)
2017-05-09 15:53:32,865 : INFO : (23, 1.0630097389221191)
2017-05-09 15:53:32,867 : INFO : (24, 1.0733129978179932)
2017-05-09 15:53:32,870 : INFO : (26, 1.1568262577056885)
2017-05-09 15:53:32,872 : INFO : (27, 1.1412229537963867)
2017-05-09 15:53:32,875 : INFO : (29, 1.1548422574996948)
2017-05-09 15:53:32,877 : INFO : (28, 4.592202186584473)
2017-05-09 15:53:32,881 : INFO : (30, 1.1657209396362305)
2017-05-09 15:53:32,886 : INFO : (33, 1.1145955324172974)
2017-05-09 15:53:32,888 : INFO : (35, 1.1548422574996948)
2017-05-09 15:53:32,890 : INFO : (34, 3.3807010650634766)
2017-05-09 15:53:32,893 : INFO : (36, 4.50806999206543)
2017-05-09 15:53:32,895 : INFO : (32, 5.662715435028076)
2017-05-09 15:53:32,898 : INFO : (31, 12.58568000793457)
2017-05-09 15:53:32,901 : INFO : (25, 22.970571517944336)
2017-05-09 15:53:32,903 : INFO : (37, 1.2072006464004517)
2017-05-09 15:53:32,906 : INFO : (16, 41.31589126586914)
2017-05-09 15:53:32,913 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:32,915 : INFO : (2, 2.256925582885742)
2017-05-09 15:53:32,917 : INFO : (3, 1.188725233078003)
2017-05-09 15:53:32,920 : INFO : (5, 1.1619560718536377)
2017-05-09 15:53:32,922 : INFO : (6, 1.1730492115020752)
2017-05-09 15:53:32,924 : INFO : (7, 1.1717875003814697)
2017-05-09 15:53:32,927 : INFO : (8, 1.1558316946029663)
2017-05-09 15:53:32,929 : INFO : (10, 1.1548422574996948)
2017-05-09 15:53:32,932 : INFO : (9, 4.64690637588501)
2017-05-09 15:53:32,934 : INFO : (11, 1.0916519165039062)
2017-05-09 15:53:32,936 : INFO : (12, 1.1321847438812256)
2017-05-09 15:53:32,939 : INFO : (14, 1.1339832544326782)
2017-05-09 15:53:32,942 : INFO : (13, 11.48015022277832)
2017-05-09 15:53:32,944 : INFO : (15, 1.153801441192627)
2017-05-09 15:53:32,947 : INFO : (16, 1.17649507522583)
2017-05-09 15:53:32,949 : INFO : (17, 1.149524211883545)
2017-05-09 15:53:32,951 : INFO : (18, 1.1548422574996948)
2017-05-09 15:53:32,953 : INFO : (20, 1.1619560718536377)
2017-05-09 15:53:32,956 : INFO : (22, 1.1859716176986694)
2017-05-09 15:53:32,958 : INFO : (23, 2.281428813934326)
2017-05-09 15:53:32,960 : INFO : (24, 1.1686874628067017)
2017-05-09 15:53:32,962 : INFO : (27, 1.1727831363677979)
2017-05-09 15:53:32,965 : INFO : (29, 1.1546502113342285)
2017-05-09 15:53:32,967 : INFO : (30, 1.1597667932510376)
2017-05-09 15:53:32,969 : INFO : (31, 1.185125708580017)
2017-05-09 15:53:32,972 : INFO : (32, 3.502020835876465)
2017-05-09 15:53:32,974 : INFO : (33, 1.1817998886108398)
2017-05-09 15:53:32,976 : INFO : (34, 1.155847430229187)
2017-05-09 15:53:32,980 : INFO : (35, 2.317129611968994)
2017-05-09 15:53:32,983 : INFO : (28, 10.493244171142578)
2017-05-09 15:53:32,985 : INFO : (26, 11.677874565124512)
2017-05-09 15:53:32,988 : INFO : (25, 12.846562385559082)
2017-05-09 15:53:32,990 : INFO : (21, 17.38468360900879)
2017-05-09 15:53:32,993 : INFO : (19, 21.961782455444336)
2017-05-09 15:53:32,996 : INFO : (36, 1.2072006464004517)
2017-05-09 15:53:32,999 : INFO : (4, 40.32240676879883)
2017-05-09 15:53:33,005 : INFO : (1, 0.0)
2017-05-09 15:53:33,007 : INFO : (2, 0.0)
2017-05-09 15:53:33,009 : INFO : (3, 0.0)
2017-05-09 15:53:33,011 : INFO : (5, 0.0)
2017-05-09 15:53:33,013 : INFO : (6, 0.0)
2017-05-09 15:53:33,015 : INFO : (7, 0.0)
2017-05-09 15:53:33,017 : INFO : (8, 0.0)
2017-05-09 15:53:33,019 : INFO : (10, 0.0)
2017-05-09 15:53:33,022 : INFO : (9, 0.0)
2017-05-09 15:53:33,024 : INFO : (11, 0.0)
2017-05-09 15:53:33,026 : INFO : (12, 0.0)
2017-05-09 15:53:33,028 : INFO : (14, 0.0)
2017-05-09 15:53:33,031 : INFO : (13, 0.0)
2017-05-09 15:53:33,033 : INFO : (15, 0.0)
2017-05-09 15:53:33,035 : INFO : (16, 0.0)
2017-05-09 15:53:33,037 : INFO : (17, 0.0)
2017-05-09 15:53:33,039 : INFO : (18, 0.0)
2017-05-09 15:53:33,041 : INFO : (20, 0.0)
2017-05-09 15:53:33,043 : INFO : (22, 0.0)
2017-05-09 15:53:33,045 : INFO : (23, 0.0)
2017-05-09 15:53:33,047 : INFO : (24, 0.0)
2017-05-09 15:53:33,050 : INFO : (27, 0.0)
2017-05-09 15:53:33,053 : INFO : (29, 0.0)
2017-05-09 15:53:33,055 : INFO : (30, 0.0)
2017-05-09 15:53:33,057 : INFO : (31, 0.0)
2017-05-09 15:53:33,059 : INFO : (32, 0.0)
2017-05-09 15:53:33,061 : INFO : (33, 0.0)
2017-05-09 15:53:33,064 : INFO : (34, 0.0)
2017-05-09 15:53:33,067 : INFO : (35, 0.0)
2017-05-09 15:53:33,070 : INFO : (28, 0.0)
2017-05-09 15:53:33,072 : INFO : (26, 0.0)
2017-05-09 15:53:33,074 : INFO : (25, 0.0)
2017-05-09 15:53:33,077 : INFO : (21, 0.0)
2017-05-09 15:53:33,079 : INFO : (19, 0.0)
2017-05-09 15:53:33,081 : INFO : (36, 0.0)
2017-05-09 15:53:33,084 : INFO : (4, 0.0)
2017-05-09 15:53:33,089 : INFO : (1, 0.0)
2017-05-09 15:53:33,091 : INFO : (2, 0.0)
2017-05-09 15:53:33,093 : INFO : (3, 0.0)
2017-05-09 15:53:33,095 : INFO : (6, 0.0)
2017-05-09 15:53:33,097 : INFO : (7, 0.0)
2017-05-09 15:53:33,100 : INFO : (10, 0.0)
2017-05-09 15:53:33,102 : INFO : (11, 0.0)
2017-05-09 15:53:33,104 : INFO : (9, 0.0)
2017-05-09 15:53:33,106 : INFO : (8, 0.0)
2017-05-09 15:53:33,108 : INFO : (12, 0.0)
2017-05-09 15:53:33,111 : INFO : (13, 0.0)
2017-05-09 15:53:33,113 : INFO : (5, 0.0)
2017-05-09 15:53:33,115 : INFO : (4, 0.0)
2017-05-09 15:53:33,117 : INFO : (14, 0.0)
2017-05-09 15:53:33,119 : INFO : (15, 0.0)
2017-05-09 15:53:33,122 : INFO : (17, 0.0)
2017-05-09 15:53:33,124 : INFO : (18, 0.0)
2017-05-09 15:53:33,126 : INFO : (21, 0.0)
2017-05-09 15:53:33,127 : INFO : (20, 0.0)
2017-05-09 15:53:33,131 : INFO : (19, 0.0)
2017-05-09 15:53:33,133 : INFO : (22, 0.0)
2017-05-09 15:53:33,135 : INFO : (23, 0.0)
2017-05-09 15:53:33,137 : INFO : (24, 0.0)
2017-05-09 15:53:33,139 : INFO : (26, 0.0)
2017-05-09 15:53:33,141 : INFO : (27, 0.0)
2017-05-09 15:53:33,143 : INFO : (29, 0.0)
2017-05-09 15:53:33,145 : INFO : (28, 0.0)
2017-05-09 15:53:33,147 : INFO : (30, 0.0)
2017-05-09 15:53:33,150 : INFO : (33, 0.0)
2017-05-09 15:53:33,152 : INFO : (35, 0.0)
2017-05-09 15:53:33,154 : INFO : (34, 0.0)
2017-05-09 15:53:33,156 : INFO : (36, 0.0)
2017-05-09 15:53:33,158 : INFO : (32, 0.0)
2017-05-09 15:53:33,160 : INFO : (31, 0.0)
2017-05-09 15:53:33,163 : INFO : (25, 0.0)
2017-05-09 15:53:33,165 : INFO : (37, 0.0)
2017-05-09 15:53:33,168 : INFO : (16, 0.0)
2017-05-09 15:53:33,173 : INFO : (4, 0.0)
2017-05-09 15:53:33,175 : INFO : (3, 0.0)
2017-05-09 15:53:33,177 : INFO : (2, 0.0)
2017-05-09 15:53:33,179 : INFO : (5, 0.0)
2017-05-09 15:53:33,181 : INFO : (6, 0.0)
2017-05-09 15:53:33,183 : INFO : (7, 0.0)
2017-05-09 15:53:33,185 : INFO : (9, 0.0)
2017-05-09 15:53:33,187 : INFO : (10, 0.0)
2017-05-09 15:53:33,189 : INFO : (11, 0.0)
2017-05-09 15:53:33,191 : INFO : (13, 0.0)
2017-05-09 15:53:33,193 : INFO : (14, 0.0)
2017-05-09 15:53:33,195 : INFO : (15, 0.0)
2017-05-09 15:53:33,197 : INFO : (16, 0.0)
2017-05-09 15:53:33,199 : INFO : (17, 0.0)
2017-05-09 15:53:33,201 : INFO : (18, 0.0)
2017-05-09 15:53:33,203 : INFO : (19, 0.0)
2017-05-09 15:53:33,205 : INFO : (22, 0.0)
2017-05-09 15:53:33,207 : INFO : (21, 0.0)
2017-05-09 15:53:33,210 : INFO : (20, 0.0)
2017-05-09 15:53:33,213 : INFO : (12, 0.0)
2017-05-09 15:53:33,215 : INFO : (23, 0.0)
2017-05-09 15:53:33,218 : INFO : (8, 0.0)
2017-05-09 15:53:33,222 : INFO : (1, 0.0)
2017-05-09 15:53:33,227 : INFO : (1, 0.0)
2017-05-09 15:53:33,229 : INFO : (2, 0.0)
2017-05-09 15:53:33,231 : INFO : (3, 0.0)
2017-05-09 15:53:33,234 : INFO : (5, 0.0)
2017-05-09 15:53:33,236 : INFO : (8, 0.0)
2017-05-09 15:53:33,238 : INFO : (9, 0.0)
2017-05-09 15:53:33,240 : INFO : (7, 0.0)
2017-05-09 15:53:33,242 : INFO : (10, 0.0)
2017-05-09 15:53:33,245 : INFO : (12, 0.0)
2017-05-09 15:53:33,247 : INFO : (11, 0.0)
2017-05-09 15:53:33,250 : INFO : (6, 0.0)
2017-05-09 15:53:33,252 : INFO : (4, 0.0)
2017-05-09 15:53:33,254 : INFO : (13, 0.0)
2017-05-09 15:53:33,256 : INFO : (14, 0.0)
2017-05-09 15:53:33,258 : INFO : (15, 0.0)
2017-05-09 15:53:33,260 : INFO : (16, 0.0)
2017-05-09 15:53:33,263 : INFO : (17, 0.0)
2017-05-09 15:53:33,265 : INFO : (19, 0.0)
2017-05-09 15:53:33,267 : INFO : (20, 0.0)
2017-05-09 15:53:33,269 : INFO : (21, 0.0)
2017-05-09 15:53:33,273 : INFO : (18, 0.0)
2017-05-09 15:53:33,274 : INFO : ==> Train loss   : 1.082739
2017-05-09 15:53:33,274 : INFO : Epoch
2017-05-09 15:53:33,274 : INFO : 3
2017-05-09 15:53:33,274 : INFO : train percentage
2017-05-09 15:53:33,274 : INFO : 0.0
2017-05-09 15:53:33,274 : INFO : Epoch
2017-05-09 15:53:33,274 : INFO : 3
2017-05-09 15:53:33,275 : INFO : dev percentage
2017-05-09 15:53:33,275 : INFO : 0.0
2017-05-09 15:53:33,275 : INFO : Epoch
2017-05-09 15:53:33,275 : INFO : 3
2017-05-09 15:53:33,275 : INFO : test percentage
2017-05-09 15:53:33,275 : INFO : 0.0
2017-05-09 15:53:33,281 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:33,283 : INFO : (2, 1.0745457410812378)
2017-05-09 15:53:33,286 : INFO : (3, 1.1600637435913086)
2017-05-09 15:53:33,288 : INFO : (6, 1.1321847438812256)
2017-05-09 15:53:33,290 : INFO : (7, 1.1577483415603638)
2017-05-09 15:53:33,293 : INFO : (10, 1.1717875003814697)
2017-05-09 15:53:33,295 : INFO : (11, 2.3070101737976074)
2017-05-09 15:53:33,297 : INFO : (9, 3.467470169067383)
2017-05-09 15:53:33,300 : INFO : (8, 5.735894680023193)
2017-05-09 15:53:33,302 : INFO : (12, 1.1339832544326782)
2017-05-09 15:53:33,305 : INFO : (13, 8.002062797546387)
2017-05-09 15:53:33,307 : INFO : (5, 9.174448013305664)
2017-05-09 15:53:33,310 : INFO : (4, 13.670205116271973)
2017-05-09 15:53:33,312 : INFO : (14, 1.188725233078003)
2017-05-09 15:53:33,314 : INFO : (15, 1.1875367164611816)
2017-05-09 15:53:33,317 : INFO : (17, 1.17649507522583)
2017-05-09 15:53:33,319 : INFO : (18, 1.1859716176986694)
2017-05-09 15:53:33,321 : INFO : (21, 1.148364782333374)
2017-05-09 15:53:33,324 : INFO : (20, 2.3133182525634766)
2017-05-09 15:53:33,326 : INFO : (19, 4.6813507080078125)
2017-05-09 15:53:33,328 : INFO : (22, 1.1968988180160522)
2017-05-09 15:53:33,330 : INFO : (23, 1.0630097389221191)
2017-05-09 15:53:33,333 : INFO : (24, 1.0733129978179932)
2017-05-09 15:53:33,335 : INFO : (26, 1.1568262577056885)
2017-05-09 15:53:33,337 : INFO : (27, 1.1412229537963867)
2017-05-09 15:53:33,339 : INFO : (29, 1.1548422574996948)
2017-05-09 15:53:33,342 : INFO : (28, 4.592202186584473)
2017-05-09 15:53:33,345 : INFO : (30, 1.1657209396362305)
2017-05-09 15:53:33,348 : INFO : (33, 1.1145955324172974)
2017-05-09 15:53:33,350 : INFO : (35, 1.1548422574996948)
2017-05-09 15:53:33,352 : INFO : (34, 3.3807010650634766)
2017-05-09 15:53:33,355 : INFO : (36, 4.50806999206543)
2017-05-09 15:53:33,357 : INFO : (32, 5.662715435028076)
2017-05-09 15:53:33,359 : INFO : (31, 12.58568000793457)
2017-05-09 15:53:33,364 : INFO : (25, 22.970571517944336)
2017-05-09 15:53:33,366 : INFO : (37, 1.2072006464004517)
2017-05-09 15:53:33,370 : INFO : (16, 41.31589126586914)
2017-05-09 15:53:33,374 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:33,376 : INFO : (2, 2.256925582885742)
2017-05-09 15:53:33,378 : INFO : (3, 1.188725233078003)
2017-05-09 15:53:33,380 : INFO : (5, 1.1619560718536377)
2017-05-09 15:53:33,383 : INFO : (6, 1.1730492115020752)
2017-05-09 15:53:33,385 : INFO : (7, 1.1717875003814697)
2017-05-09 15:53:33,387 : INFO : (8, 1.1558316946029663)
2017-05-09 15:53:33,389 : INFO : (10, 1.1548422574996948)
2017-05-09 15:53:33,392 : INFO : (9, 4.64690637588501)
2017-05-09 15:53:33,394 : INFO : (11, 1.0916519165039062)
2017-05-09 15:53:33,396 : INFO : (12, 1.1321847438812256)
2017-05-09 15:53:33,399 : INFO : (14, 1.1339832544326782)
2017-05-09 15:53:33,402 : INFO : (13, 11.48015022277832)
2017-05-09 15:53:33,404 : INFO : (15, 1.153801441192627)
2017-05-09 15:53:33,407 : INFO : (16, 1.17649507522583)
2017-05-09 15:53:33,409 : INFO : (17, 1.149524211883545)
2017-05-09 15:53:33,411 : INFO : (18, 1.1548422574996948)
2017-05-09 15:53:33,413 : INFO : (20, 1.1619560718536377)
2017-05-09 15:53:33,416 : INFO : (22, 1.1859716176986694)
2017-05-09 15:53:33,418 : INFO : (23, 2.281428813934326)
2017-05-09 15:53:33,420 : INFO : (24, 1.1686874628067017)
2017-05-09 15:53:33,425 : INFO : (27, 1.1727831363677979)
2017-05-09 15:53:33,427 : INFO : (29, 1.1546502113342285)
2017-05-09 15:53:33,429 : INFO : (30, 1.1597667932510376)
2017-05-09 15:53:33,432 : INFO : (31, 1.185125708580017)
2017-05-09 15:53:33,434 : INFO : (32, 3.502020835876465)
2017-05-09 15:53:33,437 : INFO : (33, 1.1817998886108398)
2017-05-09 15:53:33,439 : INFO : (34, 1.155847430229187)
2017-05-09 15:53:33,441 : INFO : (35, 2.317129611968994)
2017-05-09 15:53:33,445 : INFO : (28, 10.493244171142578)
2017-05-09 15:53:33,448 : INFO : (26, 11.677874565124512)
2017-05-09 15:53:33,450 : INFO : (25, 12.846562385559082)
2017-05-09 15:53:33,453 : INFO : (21, 17.38468360900879)
2017-05-09 15:53:33,457 : INFO : (19, 21.961782455444336)
2017-05-09 15:53:33,459 : INFO : (36, 1.2072006464004517)
2017-05-09 15:53:33,463 : INFO : (4, 40.32240676879883)
2017-05-09 15:53:33,468 : INFO : (1, 0.0)
2017-05-09 15:53:33,471 : INFO : (2, 0.0)
2017-05-09 15:53:33,473 : INFO : (3, 0.0)
2017-05-09 15:53:33,475 : INFO : (5, 0.0)
2017-05-09 15:53:33,477 : INFO : (6, 0.0)
2017-05-09 15:53:33,479 : INFO : (7, 0.0)
2017-05-09 15:53:33,482 : INFO : (8, 0.0)
2017-05-09 15:53:33,484 : INFO : (10, 0.0)
2017-05-09 15:53:33,486 : INFO : (9, 0.0)
2017-05-09 15:53:33,489 : INFO : (11, 0.0)
2017-05-09 15:53:33,491 : INFO : (12, 0.0)
2017-05-09 15:53:33,493 : INFO : (14, 0.0)
2017-05-09 15:53:33,496 : INFO : (13, 0.0)
2017-05-09 15:53:33,498 : INFO : (15, 0.0)
2017-05-09 15:53:33,500 : INFO : (16, 0.0)
2017-05-09 15:53:33,503 : INFO : (17, 0.0)
2017-05-09 15:53:33,505 : INFO : (18, 0.0)
2017-05-09 15:53:33,507 : INFO : (20, 0.0)
2017-05-09 15:53:33,509 : INFO : (22, 0.0)
2017-05-09 15:53:33,511 : INFO : (23, 0.0)
2017-05-09 15:53:33,513 : INFO : (24, 0.0)
2017-05-09 15:53:33,515 : INFO : (27, 0.0)
2017-05-09 15:53:33,517 : INFO : (29, 0.0)
2017-05-09 15:53:33,519 : INFO : (30, 0.0)
2017-05-09 15:53:33,522 : INFO : (31, 0.0)
2017-05-09 15:53:33,524 : INFO : (32, 0.0)
2017-05-09 15:53:33,526 : INFO : (33, 0.0)
2017-05-09 15:53:33,528 : INFO : (34, 0.0)
2017-05-09 15:53:33,530 : INFO : (35, 0.0)
2017-05-09 15:53:33,533 : INFO : (28, 0.0)
2017-05-09 15:53:33,535 : INFO : (26, 0.0)
2017-05-09 15:53:33,537 : INFO : (25, 0.0)
2017-05-09 15:53:33,539 : INFO : (21, 0.0)
2017-05-09 15:53:33,542 : INFO : (19, 0.0)
2017-05-09 15:53:33,544 : INFO : (36, 0.0)
2017-05-09 15:53:33,547 : INFO : (4, 0.0)
2017-05-09 15:53:33,551 : INFO : (1, 0.0)
2017-05-09 15:53:33,553 : INFO : (2, 0.0)
2017-05-09 15:53:33,555 : INFO : (3, 0.0)
2017-05-09 15:53:33,558 : INFO : (6, 0.0)
2017-05-09 15:53:33,560 : INFO : (7, 0.0)
2017-05-09 15:53:33,563 : INFO : (10, 0.0)
2017-05-09 15:53:33,565 : INFO : (11, 0.0)
2017-05-09 15:53:33,567 : INFO : (9, 0.0)
2017-05-09 15:53:33,569 : INFO : (8, 0.0)
2017-05-09 15:53:33,571 : INFO : (12, 0.0)
2017-05-09 15:53:33,574 : INFO : (13, 0.0)
2017-05-09 15:53:33,576 : INFO : (5, 0.0)
2017-05-09 15:53:33,578 : INFO : (4, 0.0)
2017-05-09 15:53:33,580 : INFO : (14, 0.0)
2017-05-09 15:53:33,582 : INFO : (15, 0.0)
2017-05-09 15:53:33,584 : INFO : (17, 0.0)
2017-05-09 15:53:33,586 : INFO : (18, 0.0)
2017-05-09 15:53:33,588 : INFO : (21, 0.0)
2017-05-09 15:53:33,590 : INFO : (20, 0.0)
2017-05-09 15:53:33,592 : INFO : (19, 0.0)
2017-05-09 15:53:33,594 : INFO : (22, 0.0)
2017-05-09 15:53:33,596 : INFO : (23, 0.0)
2017-05-09 15:53:33,598 : INFO : (24, 0.0)
2017-05-09 15:53:33,600 : INFO : (26, 0.0)
2017-05-09 15:53:33,602 : INFO : (27, 0.0)
2017-05-09 15:53:33,604 : INFO : (29, 0.0)
2017-05-09 15:53:33,607 : INFO : (28, 0.0)
2017-05-09 15:53:33,609 : INFO : (30, 0.0)
2017-05-09 15:53:33,611 : INFO : (33, 0.0)
2017-05-09 15:53:33,613 : INFO : (35, 0.0)
2017-05-09 15:53:33,615 : INFO : (34, 0.0)
2017-05-09 15:53:33,617 : INFO : (36, 0.0)
2017-05-09 15:53:33,619 : INFO : (32, 0.0)
2017-05-09 15:53:33,621 : INFO : (31, 0.0)
2017-05-09 15:53:33,624 : INFO : (25, 0.0)
2017-05-09 15:53:33,627 : INFO : (37, 0.0)
2017-05-09 15:53:33,632 : INFO : (16, 0.0)
2017-05-09 15:53:33,636 : INFO : (4, 0.0)
2017-05-09 15:53:33,638 : INFO : (3, 0.0)
2017-05-09 15:53:33,640 : INFO : (2, 0.0)
2017-05-09 15:53:33,642 : INFO : (5, 0.0)
2017-05-09 15:53:33,645 : INFO : (6, 0.0)
2017-05-09 15:53:33,646 : INFO : (7, 0.0)
2017-05-09 15:53:33,649 : INFO : (9, 0.0)
2017-05-09 15:53:33,651 : INFO : (10, 0.0)
2017-05-09 15:53:33,653 : INFO : (11, 0.0)
2017-05-09 15:53:33,655 : INFO : (13, 0.0)
2017-05-09 15:53:33,657 : INFO : (14, 0.0)
2017-05-09 15:53:33,659 : INFO : (15, 0.0)
2017-05-09 15:53:33,661 : INFO : (16, 0.0)
2017-05-09 15:53:33,665 : INFO : (17, 0.0)
2017-05-09 15:53:33,667 : INFO : (18, 0.0)
2017-05-09 15:53:33,669 : INFO : (19, 0.0)
2017-05-09 15:53:33,671 : INFO : (22, 0.0)
2017-05-09 15:53:33,673 : INFO : (21, 0.0)
2017-05-09 15:53:33,676 : INFO : (20, 0.0)
2017-05-09 15:53:33,679 : INFO : (12, 0.0)
2017-05-09 15:53:33,681 : INFO : (23, 0.0)
2017-05-09 15:53:33,685 : INFO : (8, 0.0)
2017-05-09 15:53:33,687 : INFO : (1, 0.0)
2017-05-09 15:53:33,692 : INFO : (1, 0.0)
2017-05-09 15:53:33,694 : INFO : (2, 0.0)
2017-05-09 15:53:33,696 : INFO : (3, 0.0)
2017-05-09 15:53:33,698 : INFO : (5, 0.0)
2017-05-09 15:53:33,701 : INFO : (8, 0.0)
2017-05-09 15:53:33,703 : INFO : (9, 0.0)
2017-05-09 15:53:33,705 : INFO : (7, 0.0)
2017-05-09 15:53:33,707 : INFO : (10, 0.0)
2017-05-09 15:53:33,709 : INFO : (12, 0.0)
2017-05-09 15:53:33,711 : INFO : (11, 0.0)
2017-05-09 15:53:33,713 : INFO : (6, 0.0)
2017-05-09 15:53:33,716 : INFO : (4, 0.0)
2017-05-09 15:53:33,718 : INFO : (13, 0.0)
2017-05-09 15:53:33,720 : INFO : (14, 0.0)
2017-05-09 15:53:33,722 : INFO : (15, 0.0)
2017-05-09 15:53:33,724 : INFO : (16, 0.0)
2017-05-09 15:53:33,726 : INFO : (17, 0.0)
2017-05-09 15:53:33,728 : INFO : (19, 0.0)
2017-05-09 15:53:33,730 : INFO : (20, 0.0)
2017-05-09 15:53:33,732 : INFO : (21, 0.0)
2017-05-09 15:53:33,736 : INFO : (18, 0.0)
2017-05-09 15:53:33,736 : INFO : ==> Train loss   : 1.082739
2017-05-09 15:53:33,736 : INFO : Epoch
2017-05-09 15:53:33,737 : INFO : 4
2017-05-09 15:53:33,737 : INFO : train percentage
2017-05-09 15:53:33,737 : INFO : 0.0
2017-05-09 15:53:33,737 : INFO : Epoch
2017-05-09 15:53:33,737 : INFO : 4
2017-05-09 15:53:33,737 : INFO : dev percentage
2017-05-09 15:53:33,737 : INFO : 0.0
2017-05-09 15:53:33,737 : INFO : Epoch
2017-05-09 15:53:33,738 : INFO : 4
2017-05-09 15:53:33,738 : INFO : test percentage
2017-05-09 15:53:33,738 : INFO : 0.0
2017-05-09 15:53:33,742 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:33,748 : INFO : (2, 1.0745457410812378)
2017-05-09 15:53:33,751 : INFO : (3, 1.1600637435913086)
2017-05-09 15:53:33,753 : INFO : (6, 1.1321847438812256)
2017-05-09 15:53:33,756 : INFO : (7, 1.1577483415603638)
2017-05-09 15:53:33,758 : INFO : (10, 1.1717875003814697)
2017-05-09 15:53:33,760 : INFO : (11, 2.3070101737976074)
2017-05-09 15:53:33,762 : INFO : (9, 3.467470169067383)
2017-05-09 15:53:33,765 : INFO : (8, 5.735894680023193)
2017-05-09 15:53:33,767 : INFO : (12, 1.1339832544326782)
2017-05-09 15:53:33,769 : INFO : (13, 8.002062797546387)
2017-05-09 15:53:33,772 : INFO : (5, 9.174448013305664)
2017-05-09 15:53:33,774 : INFO : (4, 13.670205116271973)
2017-05-09 15:53:33,777 : INFO : (14, 1.188725233078003)
2017-05-09 15:53:33,779 : INFO : (15, 1.1875367164611816)
2017-05-09 15:53:33,781 : INFO : (17, 1.17649507522583)
2017-05-09 15:53:33,783 : INFO : (18, 1.1859716176986694)
2017-05-09 15:53:33,786 : INFO : (21, 1.148364782333374)
2017-05-09 15:53:33,788 : INFO : (20, 2.3133182525634766)
2017-05-09 15:53:33,790 : INFO : (19, 4.6813507080078125)
2017-05-09 15:53:33,792 : INFO : (22, 1.1968988180160522)
2017-05-09 15:53:33,794 : INFO : (23, 1.0630097389221191)
2017-05-09 15:53:33,797 : INFO : (24, 1.0733129978179932)
2017-05-09 15:53:33,799 : INFO : (26, 1.1568262577056885)
2017-05-09 15:53:33,801 : INFO : (27, 1.1412229537963867)
2017-05-09 15:53:33,803 : INFO : (29, 1.1548422574996948)
2017-05-09 15:53:33,806 : INFO : (28, 4.592202186584473)
2017-05-09 15:53:33,808 : INFO : (30, 1.1657209396362305)
2017-05-09 15:53:33,810 : INFO : (33, 1.1145955324172974)
2017-05-09 15:53:33,812 : INFO : (35, 1.1548422574996948)
2017-05-09 15:53:33,815 : INFO : (34, 3.3807010650634766)
2017-05-09 15:53:33,817 : INFO : (36, 4.50806999206543)
2017-05-09 15:53:33,819 : INFO : (32, 5.662715435028076)
2017-05-09 15:53:33,822 : INFO : (31, 12.58568000793457)
2017-05-09 15:53:33,825 : INFO : (25, 22.970571517944336)
2017-05-09 15:53:33,827 : INFO : (37, 1.2072006464004517)
2017-05-09 15:53:33,831 : INFO : (16, 41.31589126586914)
2017-05-09 15:53:33,835 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:33,837 : INFO : (2, 2.256925582885742)
2017-05-09 15:53:33,839 : INFO : (3, 1.188725233078003)
2017-05-09 15:53:33,842 : INFO : (5, 1.1619560718536377)
2017-05-09 15:53:33,844 : INFO : (6, 1.1730492115020752)
2017-05-09 15:53:33,847 : INFO : (7, 1.1717875003814697)
2017-05-09 15:53:33,849 : INFO : (8, 1.1558316946029663)
2017-05-09 15:53:33,851 : INFO : (10, 1.1548422574996948)
2017-05-09 15:53:33,854 : INFO : (9, 4.64690637588501)
2017-05-09 15:53:33,856 : INFO : (11, 1.0916519165039062)
2017-05-09 15:53:33,858 : INFO : (12, 1.1321847438812256)
2017-05-09 15:53:33,861 : INFO : (14, 1.1339832544326782)
2017-05-09 15:53:33,865 : INFO : (13, 11.48015022277832)
2017-05-09 15:53:33,867 : INFO : (15, 1.153801441192627)
2017-05-09 15:53:33,869 : INFO : (16, 1.17649507522583)
2017-05-09 15:53:33,871 : INFO : (17, 1.149524211883545)
2017-05-09 15:53:33,874 : INFO : (18, 1.1548422574996948)
2017-05-09 15:53:33,876 : INFO : (20, 1.1619560718536377)
2017-05-09 15:53:33,879 : INFO : (22, 1.1859716176986694)
2017-05-09 15:53:33,881 : INFO : (23, 2.281428813934326)
2017-05-09 15:53:33,883 : INFO : (24, 1.1686874628067017)
2017-05-09 15:53:33,888 : INFO : (27, 1.1727831363677979)
2017-05-09 15:53:33,890 : INFO : (29, 1.1546502113342285)
2017-05-09 15:53:33,892 : INFO : (30, 1.1597667932510376)
2017-05-09 15:53:33,894 : INFO : (31, 1.185125708580017)
2017-05-09 15:53:33,897 : INFO : (32, 3.502020835876465)
2017-05-09 15:53:33,899 : INFO : (33, 1.1817998886108398)
2017-05-09 15:53:33,902 : INFO : (34, 1.155847430229187)
2017-05-09 15:53:33,904 : INFO : (35, 2.317129611968994)
2017-05-09 15:53:33,907 : INFO : (28, 10.493244171142578)
2017-05-09 15:53:33,909 : INFO : (26, 11.677874565124512)
2017-05-09 15:53:33,912 : INFO : (25, 12.846562385559082)
2017-05-09 15:53:33,914 : INFO : (21, 17.38468360900879)
2017-05-09 15:53:33,917 : INFO : (19, 21.961782455444336)
2017-05-09 15:53:33,921 : INFO : (36, 1.2072006464004517)
2017-05-09 15:53:33,924 : INFO : (4, 40.32240676879883)
2017-05-09 15:53:33,931 : INFO : (1, 0.0)
2017-05-09 15:53:33,933 : INFO : (2, 0.0)
2017-05-09 15:53:33,935 : INFO : (3, 0.0)
2017-05-09 15:53:33,937 : INFO : (5, 0.0)
2017-05-09 15:53:33,939 : INFO : (6, 0.0)
2017-05-09 15:53:33,941 : INFO : (7, 0.0)
2017-05-09 15:53:33,943 : INFO : (8, 0.0)
2017-05-09 15:53:33,945 : INFO : (10, 0.0)
2017-05-09 15:53:33,948 : INFO : (9, 0.0)
2017-05-09 15:53:33,950 : INFO : (11, 0.0)
2017-05-09 15:53:33,952 : INFO : (12, 0.0)
2017-05-09 15:53:33,954 : INFO : (14, 0.0)
2017-05-09 15:53:33,957 : INFO : (13, 0.0)
2017-05-09 15:53:33,959 : INFO : (15, 0.0)
2017-05-09 15:53:33,961 : INFO : (16, 0.0)
2017-05-09 15:53:33,963 : INFO : (17, 0.0)
2017-05-09 15:53:33,965 : INFO : (18, 0.0)
2017-05-09 15:53:33,967 : INFO : (20, 0.0)
2017-05-09 15:53:33,969 : INFO : (22, 0.0)
2017-05-09 15:53:33,971 : INFO : (23, 0.0)
2017-05-09 15:53:33,974 : INFO : (24, 0.0)
2017-05-09 15:53:33,976 : INFO : (27, 0.0)
2017-05-09 15:53:33,978 : INFO : (29, 0.0)
2017-05-09 15:53:33,980 : INFO : (30, 0.0)
2017-05-09 15:53:33,982 : INFO : (31, 0.0)
2017-05-09 15:53:33,984 : INFO : (32, 0.0)
2017-05-09 15:53:33,986 : INFO : (33, 0.0)
2017-05-09 15:53:33,988 : INFO : (34, 0.0)
2017-05-09 15:53:33,990 : INFO : (35, 0.0)
2017-05-09 15:53:33,993 : INFO : (28, 0.0)
2017-05-09 15:53:33,995 : INFO : (26, 0.0)
2017-05-09 15:53:33,997 : INFO : (25, 0.0)
2017-05-09 15:53:34,000 : INFO : (21, 0.0)
2017-05-09 15:53:34,002 : INFO : (19, 0.0)
2017-05-09 15:53:34,004 : INFO : (36, 0.0)
2017-05-09 15:53:34,007 : INFO : (4, 0.0)
2017-05-09 15:53:34,011 : INFO : (1, 0.0)
2017-05-09 15:53:34,013 : INFO : (2, 0.0)
2017-05-09 15:53:34,015 : INFO : (3, 0.0)
2017-05-09 15:53:34,017 : INFO : (6, 0.0)
2017-05-09 15:53:34,020 : INFO : (7, 0.0)
2017-05-09 15:53:34,022 : INFO : (10, 0.0)
2017-05-09 15:53:34,024 : INFO : (11, 0.0)
2017-05-09 15:53:34,026 : INFO : (9, 0.0)
2017-05-09 15:53:34,028 : INFO : (8, 0.0)
2017-05-09 15:53:34,030 : INFO : (12, 0.0)
2017-05-09 15:53:34,035 : INFO : (13, 0.0)
2017-05-09 15:53:34,037 : INFO : (5, 0.0)
2017-05-09 15:53:34,040 : INFO : (4, 0.0)
2017-05-09 15:53:34,042 : INFO : (14, 0.0)
2017-05-09 15:53:34,044 : INFO : (15, 0.0)
2017-05-09 15:53:34,046 : INFO : (17, 0.0)
2017-05-09 15:53:34,048 : INFO : (18, 0.0)
2017-05-09 15:53:34,051 : INFO : (21, 0.0)
2017-05-09 15:53:34,053 : INFO : (20, 0.0)
2017-05-09 15:53:34,056 : INFO : (19, 0.0)
2017-05-09 15:53:34,058 : INFO : (22, 0.0)
2017-05-09 15:53:34,060 : INFO : (23, 0.0)
2017-05-09 15:53:34,062 : INFO : (24, 0.0)
2017-05-09 15:53:34,064 : INFO : (26, 0.0)
2017-05-09 15:53:34,066 : INFO : (27, 0.0)
2017-05-09 15:53:34,069 : INFO : (29, 0.0)
2017-05-09 15:53:34,072 : INFO : (28, 0.0)
2017-05-09 15:53:34,074 : INFO : (30, 0.0)
2017-05-09 15:53:34,076 : INFO : (33, 0.0)
2017-05-09 15:53:34,078 : INFO : (35, 0.0)
2017-05-09 15:53:34,081 : INFO : (34, 0.0)
2017-05-09 15:53:34,083 : INFO : (36, 0.0)
2017-05-09 15:53:34,085 : INFO : (32, 0.0)
2017-05-09 15:53:34,087 : INFO : (31, 0.0)
2017-05-09 15:53:34,090 : INFO : (25, 0.0)
2017-05-09 15:53:34,092 : INFO : (37, 0.0)
2017-05-09 15:53:34,095 : INFO : (16, 0.0)
2017-05-09 15:53:34,100 : INFO : (4, 0.0)
2017-05-09 15:53:34,103 : INFO : (3, 0.0)
2017-05-09 15:53:34,105 : INFO : (2, 0.0)
2017-05-09 15:53:34,107 : INFO : (5, 0.0)
2017-05-09 15:53:34,109 : INFO : (6, 0.0)
2017-05-09 15:53:34,111 : INFO : (7, 0.0)
2017-05-09 15:53:34,113 : INFO : (9, 0.0)
2017-05-09 15:53:34,115 : INFO : (10, 0.0)
2017-05-09 15:53:34,117 : INFO : (11, 0.0)
2017-05-09 15:53:34,119 : INFO : (13, 0.0)
2017-05-09 15:53:34,121 : INFO : (14, 0.0)
2017-05-09 15:53:34,123 : INFO : (15, 0.0)
2017-05-09 15:53:34,125 : INFO : (16, 0.0)
2017-05-09 15:53:34,127 : INFO : (17, 0.0)
2017-05-09 15:53:34,129 : INFO : (18, 0.0)
2017-05-09 15:53:34,131 : INFO : (19, 0.0)
2017-05-09 15:53:34,133 : INFO : (22, 0.0)
2017-05-09 15:53:34,135 : INFO : (21, 0.0)
2017-05-09 15:53:34,138 : INFO : (20, 0.0)
2017-05-09 15:53:34,141 : INFO : (12, 0.0)
2017-05-09 15:53:34,143 : INFO : (23, 0.0)
2017-05-09 15:53:34,146 : INFO : (8, 0.0)
2017-05-09 15:53:34,148 : INFO : (1, 0.0)
2017-05-09 15:53:34,152 : INFO : (1, 0.0)
2017-05-09 15:53:34,154 : INFO : (2, 0.0)
2017-05-09 15:53:34,156 : INFO : (3, 0.0)
2017-05-09 15:53:34,158 : INFO : (5, 0.0)
2017-05-09 15:53:34,160 : INFO : (8, 0.0)
2017-05-09 15:53:34,162 : INFO : (9, 0.0)
2017-05-09 15:53:34,164 : INFO : (7, 0.0)
2017-05-09 15:53:34,166 : INFO : (10, 0.0)
2017-05-09 15:53:34,168 : INFO : (12, 0.0)
2017-05-09 15:53:34,170 : INFO : (11, 0.0)
2017-05-09 15:53:34,173 : INFO : (6, 0.0)
2017-05-09 15:53:34,175 : INFO : (4, 0.0)
2017-05-09 15:53:34,177 : INFO : (13, 0.0)
2017-05-09 15:53:34,179 : INFO : (14, 0.0)
2017-05-09 15:53:34,181 : INFO : (15, 0.0)
2017-05-09 15:53:34,183 : INFO : (16, 0.0)
2017-05-09 15:53:34,185 : INFO : (17, 0.0)
2017-05-09 15:53:34,187 : INFO : (19, 0.0)
2017-05-09 15:53:34,189 : INFO : (20, 0.0)
2017-05-09 15:53:34,191 : INFO : (21, 0.0)
2017-05-09 15:53:34,195 : INFO : (18, 0.0)
2017-05-09 15:53:34,196 : INFO : ==> Train loss   : 1.082739
2017-05-09 15:53:34,196 : INFO : Epoch
2017-05-09 15:53:34,196 : INFO : 5
2017-05-09 15:53:34,196 : INFO : train percentage
2017-05-09 15:53:34,196 : INFO : 0.0
2017-05-09 15:53:34,196 : INFO : Epoch
2017-05-09 15:53:34,196 : INFO : 5
2017-05-09 15:53:34,196 : INFO : dev percentage
2017-05-09 15:53:34,197 : INFO : 0.0
2017-05-09 15:53:34,197 : INFO : Epoch
2017-05-09 15:53:34,197 : INFO : 5
2017-05-09 15:53:34,197 : INFO : test percentage
2017-05-09 15:53:34,197 : INFO : 0.0
2017-05-09 15:53:34,202 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:34,204 : INFO : (2, 1.0745457410812378)
2017-05-09 15:53:34,206 : INFO : (3, 1.1600637435913086)
2017-05-09 15:53:34,209 : INFO : (6, 1.1321847438812256)
2017-05-09 15:53:34,211 : INFO : (7, 1.1577483415603638)
2017-05-09 15:53:34,213 : INFO : (10, 1.1717875003814697)
2017-05-09 15:53:34,216 : INFO : (11, 2.3070101737976074)
2017-05-09 15:53:34,218 : INFO : (9, 3.467470169067383)
2017-05-09 15:53:34,220 : INFO : (8, 5.735894680023193)
2017-05-09 15:53:34,223 : INFO : (12, 1.1339832544326782)
2017-05-09 15:53:34,225 : INFO : (13, 8.002062797546387)
2017-05-09 15:53:34,227 : INFO : (5, 9.174448013305664)
2017-05-09 15:53:34,230 : INFO : (4, 13.670205116271973)
2017-05-09 15:53:34,234 : INFO : (14, 1.188725233078003)
2017-05-09 15:53:34,237 : INFO : (15, 1.1875367164611816)
2017-05-09 15:53:34,239 : INFO : (17, 1.17649507522583)
2017-05-09 15:53:34,241 : INFO : (18, 1.1859716176986694)
2017-05-09 15:53:34,244 : INFO : (21, 1.148364782333374)
2017-05-09 15:53:34,246 : INFO : (20, 2.3133182525634766)
2017-05-09 15:53:34,248 : INFO : (19, 4.6813507080078125)
2017-05-09 15:53:34,251 : INFO : (22, 1.1968988180160522)
2017-05-09 15:53:34,253 : INFO : (23, 1.0630097389221191)
2017-05-09 15:53:34,260 : INFO : (24, 1.0733129978179932)
2017-05-09 15:53:34,263 : INFO : (26, 1.1568262577056885)
2017-05-09 15:53:34,265 : INFO : (27, 1.1412229537963867)
2017-05-09 15:53:34,268 : INFO : (29, 1.1548422574996948)
2017-05-09 15:53:34,271 : INFO : (28, 4.592202186584473)
2017-05-09 15:53:34,275 : INFO : (30, 1.1657209396362305)
2017-05-09 15:53:34,277 : INFO : (33, 1.1145955324172974)
2017-05-09 15:53:34,280 : INFO : (35, 1.1548422574996948)
2017-05-09 15:53:34,282 : INFO : (34, 3.3807010650634766)
2017-05-09 15:53:34,285 : INFO : (36, 4.50806999206543)
2017-05-09 15:53:34,287 : INFO : (32, 5.662715435028076)
2017-05-09 15:53:34,291 : INFO : (31, 12.58568000793457)
2017-05-09 15:53:34,295 : INFO : (25, 22.970571517944336)
2017-05-09 15:53:34,297 : INFO : (37, 1.2072006464004517)
2017-05-09 15:53:34,300 : INFO : (16, 41.31589126586914)
2017-05-09 15:53:34,305 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:34,307 : INFO : (2, 2.256925582885742)
2017-05-09 15:53:34,311 : INFO : (3, 1.188725233078003)
2017-05-09 15:53:34,314 : INFO : (5, 1.1619560718536377)
2017-05-09 15:53:34,317 : INFO : (6, 1.1730492115020752)
2017-05-09 15:53:34,320 : INFO : (7, 1.1717875003814697)
2017-05-09 15:53:34,322 : INFO : (8, 1.1558316946029663)
2017-05-09 15:53:34,325 : INFO : (10, 1.1548422574996948)
2017-05-09 15:53:34,327 : INFO : (9, 4.64690637588501)
2017-05-09 15:53:34,330 : INFO : (11, 1.0916519165039062)
2017-05-09 15:53:34,332 : INFO : (12, 1.1321847438812256)
2017-05-09 15:53:34,334 : INFO : (14, 1.1339832544326782)
2017-05-09 15:53:34,338 : INFO : (13, 11.48015022277832)
2017-05-09 15:53:34,340 : INFO : (15, 1.153801441192627)
2017-05-09 15:53:34,343 : INFO : (16, 1.17649507522583)
2017-05-09 15:53:34,345 : INFO : (17, 1.149524211883545)
2017-05-09 15:53:34,347 : INFO : (18, 1.1548422574996948)
2017-05-09 15:53:34,350 : INFO : (20, 1.1619560718536377)
2017-05-09 15:53:34,352 : INFO : (22, 1.1859716176986694)
2017-05-09 15:53:34,354 : INFO : (23, 2.281428813934326)
2017-05-09 15:53:34,356 : INFO : (24, 1.1686874628067017)
2017-05-09 15:53:34,359 : INFO : (27, 1.1727831363677979)
2017-05-09 15:53:34,361 : INFO : (29, 1.1546502113342285)
2017-05-09 15:53:34,363 : INFO : (30, 1.1597667932510376)
2017-05-09 15:53:34,365 : INFO : (31, 1.185125708580017)
2017-05-09 15:53:34,368 : INFO : (32, 3.502020835876465)
2017-05-09 15:53:34,370 : INFO : (33, 1.1817998886108398)
2017-05-09 15:53:34,372 : INFO : (34, 1.155847430229187)
2017-05-09 15:53:34,375 : INFO : (35, 2.317129611968994)
2017-05-09 15:53:34,378 : INFO : (28, 10.493244171142578)
2017-05-09 15:53:34,380 : INFO : (26, 11.677874565124512)
2017-05-09 15:53:34,382 : INFO : (25, 12.846562385559082)
2017-05-09 15:53:34,385 : INFO : (21, 17.38468360900879)
2017-05-09 15:53:34,388 : INFO : (19, 21.961782455444336)
2017-05-09 15:53:34,390 : INFO : (36, 1.2072006464004517)
2017-05-09 15:53:34,396 : INFO : (4, 40.32240676879883)
2017-05-09 15:53:34,402 : INFO : (1, 0.0)
2017-05-09 15:53:34,404 : INFO : (2, 0.0)
2017-05-09 15:53:34,406 : INFO : (3, 0.0)
2017-05-09 15:53:34,409 : INFO : (5, 0.0)
2017-05-09 15:53:34,411 : INFO : (6, 0.0)
2017-05-09 15:53:34,413 : INFO : (7, 0.0)
2017-05-09 15:53:34,415 : INFO : (8, 0.0)
2017-05-09 15:53:34,417 : INFO : (10, 0.0)
2017-05-09 15:53:34,420 : INFO : (9, 0.0)
2017-05-09 15:53:34,422 : INFO : (11, 0.0)
2017-05-09 15:53:34,424 : INFO : (12, 0.0)
2017-05-09 15:53:34,426 : INFO : (14, 0.0)
2017-05-09 15:53:34,429 : INFO : (13, 0.0)
2017-05-09 15:53:34,431 : INFO : (15, 0.0)
2017-05-09 15:53:34,434 : INFO : (16, 0.0)
2017-05-09 15:53:34,436 : INFO : (17, 0.0)
2017-05-09 15:53:34,438 : INFO : (18, 0.0)
2017-05-09 15:53:34,441 : INFO : (20, 0.0)
2017-05-09 15:53:34,443 : INFO : (22, 0.0)
2017-05-09 15:53:34,445 : INFO : (23, 0.0)
2017-05-09 15:53:34,448 : INFO : (24, 0.0)
2017-05-09 15:53:34,450 : INFO : (27, 0.0)
2017-05-09 15:53:34,452 : INFO : (29, 0.0)
2017-05-09 15:53:34,454 : INFO : (30, 0.0)
2017-05-09 15:53:34,456 : INFO : (31, 0.0)
2017-05-09 15:53:34,459 : INFO : (32, 0.0)
2017-05-09 15:53:34,462 : INFO : (33, 0.0)
2017-05-09 15:53:34,464 : INFO : (34, 0.0)
2017-05-09 15:53:34,466 : INFO : (35, 0.0)
2017-05-09 15:53:34,469 : INFO : (28, 0.0)
2017-05-09 15:53:34,471 : INFO : (26, 0.0)
2017-05-09 15:53:34,474 : INFO : (25, 0.0)
2017-05-09 15:53:34,476 : INFO : (21, 0.0)
2017-05-09 15:53:34,479 : INFO : (19, 0.0)
2017-05-09 15:53:34,481 : INFO : (36, 0.0)
2017-05-09 15:53:34,485 : INFO : (4, 0.0)
2017-05-09 15:53:34,489 : INFO : (1, 0.0)
2017-05-09 15:53:34,491 : INFO : (2, 0.0)
2017-05-09 15:53:34,493 : INFO : (3, 0.0)
2017-05-09 15:53:34,496 : INFO : (6, 0.0)
2017-05-09 15:53:34,499 : INFO : (7, 0.0)
2017-05-09 15:53:34,501 : INFO : (10, 0.0)
2017-05-09 15:53:34,503 : INFO : (11, 0.0)
2017-05-09 15:53:34,505 : INFO : (9, 0.0)
2017-05-09 15:53:34,508 : INFO : (8, 0.0)
2017-05-09 15:53:34,510 : INFO : (12, 0.0)
2017-05-09 15:53:34,513 : INFO : (13, 0.0)
2017-05-09 15:53:34,515 : INFO : (5, 0.0)
2017-05-09 15:53:34,517 : INFO : (4, 0.0)
2017-05-09 15:53:34,519 : INFO : (14, 0.0)
2017-05-09 15:53:34,521 : INFO : (15, 0.0)
2017-05-09 15:53:34,523 : INFO : (17, 0.0)
2017-05-09 15:53:34,526 : INFO : (18, 0.0)
2017-05-09 15:53:34,528 : INFO : (21, 0.0)
2017-05-09 15:53:34,530 : INFO : (20, 0.0)
2017-05-09 15:53:34,533 : INFO : (19, 0.0)
2017-05-09 15:53:34,535 : INFO : (22, 0.0)
2017-05-09 15:53:34,537 : INFO : (23, 0.0)
2017-05-09 15:53:34,539 : INFO : (24, 0.0)
2017-05-09 15:53:34,541 : INFO : (26, 0.0)
2017-05-09 15:53:34,543 : INFO : (27, 0.0)
2017-05-09 15:53:34,546 : INFO : (29, 0.0)
2017-05-09 15:53:34,548 : INFO : (28, 0.0)
2017-05-09 15:53:34,550 : INFO : (30, 0.0)
2017-05-09 15:53:34,552 : INFO : (33, 0.0)
2017-05-09 15:53:34,554 : INFO : (35, 0.0)
2017-05-09 15:53:34,557 : INFO : (34, 0.0)
2017-05-09 15:53:34,559 : INFO : (36, 0.0)
2017-05-09 15:53:34,561 : INFO : (32, 0.0)
2017-05-09 15:53:34,563 : INFO : (31, 0.0)
2017-05-09 15:53:34,566 : INFO : (25, 0.0)
2017-05-09 15:53:34,568 : INFO : (37, 0.0)
2017-05-09 15:53:34,571 : INFO : (16, 0.0)
2017-05-09 15:53:34,577 : INFO : (4, 0.0)
2017-05-09 15:53:34,579 : INFO : (3, 0.0)
2017-05-09 15:53:34,581 : INFO : (2, 0.0)
2017-05-09 15:53:34,583 : INFO : (5, 0.0)
2017-05-09 15:53:34,585 : INFO : (6, 0.0)
2017-05-09 15:53:34,587 : INFO : (7, 0.0)
2017-05-09 15:53:34,590 : INFO : (9, 0.0)
2017-05-09 15:53:34,592 : INFO : (10, 0.0)
2017-05-09 15:53:34,594 : INFO : (11, 0.0)
2017-05-09 15:53:34,596 : INFO : (13, 0.0)
2017-05-09 15:53:34,598 : INFO : (14, 0.0)
2017-05-09 15:53:34,600 : INFO : (15, 0.0)
2017-05-09 15:53:34,602 : INFO : (16, 0.0)
2017-05-09 15:53:34,604 : INFO : (17, 0.0)
2017-05-09 15:53:34,606 : INFO : (18, 0.0)
2017-05-09 15:53:34,608 : INFO : (19, 0.0)
2017-05-09 15:53:34,610 : INFO : (22, 0.0)
2017-05-09 15:53:34,612 : INFO : (21, 0.0)
2017-05-09 15:53:34,615 : INFO : (20, 0.0)
2017-05-09 15:53:34,618 : INFO : (12, 0.0)
2017-05-09 15:53:34,620 : INFO : (23, 0.0)
2017-05-09 15:53:34,623 : INFO : (8, 0.0)
2017-05-09 15:53:34,625 : INFO : (1, 0.0)
2017-05-09 15:53:34,630 : INFO : (1, 0.0)
2017-05-09 15:53:34,632 : INFO : (2, 0.0)
2017-05-09 15:53:34,634 : INFO : (3, 0.0)
2017-05-09 15:53:34,636 : INFO : (5, 0.0)
2017-05-09 15:53:34,638 : INFO : (8, 0.0)
2017-05-09 15:53:34,640 : INFO : (9, 0.0)
2017-05-09 15:53:34,643 : INFO : (7, 0.0)
2017-05-09 15:53:34,647 : INFO : (10, 0.0)
2017-05-09 15:53:34,649 : INFO : (12, 0.0)
2017-05-09 15:53:34,651 : INFO : (11, 0.0)
2017-05-09 15:53:34,654 : INFO : (6, 0.0)
2017-05-09 15:53:34,657 : INFO : (4, 0.0)
2017-05-09 15:53:34,659 : INFO : (13, 0.0)
2017-05-09 15:53:34,661 : INFO : (14, 0.0)
2017-05-09 15:53:34,663 : INFO : (15, 0.0)
2017-05-09 15:53:34,665 : INFO : (16, 0.0)
2017-05-09 15:53:34,667 : INFO : (17, 0.0)
2017-05-09 15:53:34,670 : INFO : (19, 0.0)
2017-05-09 15:53:34,672 : INFO : (20, 0.0)
2017-05-09 15:53:34,674 : INFO : (21, 0.0)
2017-05-09 15:53:34,677 : INFO : (18, 0.0)
2017-05-09 15:53:34,679 : INFO : ==> Train loss   : 1.082739
2017-05-09 15:53:34,679 : INFO : Epoch
2017-05-09 15:53:34,679 : INFO : 6
2017-05-09 15:53:34,679 : INFO : train percentage
2017-05-09 15:53:34,679 : INFO : 0.0
2017-05-09 15:53:34,679 : INFO : Epoch
2017-05-09 15:53:34,679 : INFO : 6
2017-05-09 15:53:34,680 : INFO : dev percentage
2017-05-09 15:53:34,680 : INFO : 0.0
2017-05-09 15:53:34,680 : INFO : Epoch
2017-05-09 15:53:34,680 : INFO : 6
2017-05-09 15:53:34,680 : INFO : test percentage
2017-05-09 15:53:34,680 : INFO : 0.0
2017-05-09 15:53:34,685 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:34,688 : INFO : (2, 1.0745457410812378)
2017-05-09 15:53:34,690 : INFO : (3, 1.1600637435913086)
2017-05-09 15:53:34,693 : INFO : (6, 1.1321847438812256)
2017-05-09 15:53:34,695 : INFO : (7, 1.1577483415603638)
2017-05-09 15:53:34,698 : INFO : (10, 1.1717875003814697)
2017-05-09 15:53:34,700 : INFO : (11, 2.3070101737976074)
2017-05-09 15:53:34,703 : INFO : (9, 3.467470169067383)
2017-05-09 15:53:34,706 : INFO : (8, 5.735894680023193)
2017-05-09 15:53:34,708 : INFO : (12, 1.1339832544326782)
2017-05-09 15:53:34,711 : INFO : (13, 8.002062797546387)
2017-05-09 15:53:34,713 : INFO : (5, 9.174448013305664)
2017-05-09 15:53:34,716 : INFO : (4, 13.670205116271973)
2017-05-09 15:53:34,718 : INFO : (14, 1.188725233078003)
2017-05-09 15:53:34,721 : INFO : (15, 1.1875367164611816)
2017-05-09 15:53:34,723 : INFO : (17, 1.17649507522583)
2017-05-09 15:53:34,726 : INFO : (18, 1.1859716176986694)
2017-05-09 15:53:34,728 : INFO : (21, 1.148364782333374)
2017-05-09 15:53:34,730 : INFO : (20, 2.3133182525634766)
2017-05-09 15:53:34,733 : INFO : (19, 4.6813507080078125)
2017-05-09 15:53:34,735 : INFO : (22, 1.1968988180160522)
2017-05-09 15:53:34,738 : INFO : (23, 1.0630097389221191)
2017-05-09 15:53:34,740 : INFO : (24, 1.0733129978179932)
2017-05-09 15:53:34,742 : INFO : (26, 1.1568262577056885)
2017-05-09 15:53:34,745 : INFO : (27, 1.1412229537963867)
2017-05-09 15:53:34,747 : INFO : (29, 1.1548422574996948)
2017-05-09 15:53:34,750 : INFO : (28, 4.592202186584473)
2017-05-09 15:53:34,752 : INFO : (30, 1.1657209396362305)
2017-05-09 15:53:34,754 : INFO : (33, 1.1145955324172974)
2017-05-09 15:53:34,757 : INFO : (35, 1.1548422574996948)
2017-05-09 15:53:34,759 : INFO : (34, 3.3807010650634766)
2017-05-09 15:53:34,762 : INFO : (36, 4.50806999206543)
2017-05-09 15:53:34,764 : INFO : (32, 5.662715435028076)
2017-05-09 15:53:34,767 : INFO : (31, 12.58568000793457)
2017-05-09 15:53:34,770 : INFO : (25, 22.970571517944336)
2017-05-09 15:53:34,772 : INFO : (37, 1.2072006464004517)
2017-05-09 15:53:34,775 : INFO : (16, 41.31589126586914)
2017-05-09 15:53:34,780 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:34,782 : INFO : (2, 2.256925582885742)
2017-05-09 15:53:34,784 : INFO : (3, 1.188725233078003)
2017-05-09 15:53:34,786 : INFO : (5, 1.1619560718536377)
2017-05-09 15:53:34,790 : INFO : (6, 1.1730492115020752)
2017-05-09 15:53:34,793 : INFO : (7, 1.1717875003814697)
2017-05-09 15:53:34,795 : INFO : (8, 1.1558316946029663)
2017-05-09 15:53:34,797 : INFO : (10, 1.1548422574996948)
2017-05-09 15:53:34,800 : INFO : (9, 4.64690637588501)
2017-05-09 15:53:34,803 : INFO : (11, 1.0916519165039062)
2017-05-09 15:53:34,805 : INFO : (12, 1.1321847438812256)
2017-05-09 15:53:34,807 : INFO : (14, 1.1339832544326782)
2017-05-09 15:53:34,810 : INFO : (13, 11.48015022277832)
2017-05-09 15:53:34,813 : INFO : (15, 1.153801441192627)
2017-05-09 15:53:34,816 : INFO : (16, 1.17649507522583)
2017-05-09 15:53:34,818 : INFO : (17, 1.149524211883545)
2017-05-09 15:53:34,820 : INFO : (18, 1.1548422574996948)
2017-05-09 15:53:34,823 : INFO : (20, 1.1619560718536377)
2017-05-09 15:53:34,825 : INFO : (22, 1.1859716176986694)
2017-05-09 15:53:34,827 : INFO : (23, 2.281428813934326)
2017-05-09 15:53:34,830 : INFO : (24, 1.1686874628067017)
2017-05-09 15:53:34,832 : INFO : (27, 1.1727831363677979)
2017-05-09 15:53:34,834 : INFO : (29, 1.1546502113342285)
2017-05-09 15:53:34,837 : INFO : (30, 1.1597667932510376)
2017-05-09 15:53:34,839 : INFO : (31, 1.185125708580017)
2017-05-09 15:53:34,841 : INFO : (32, 3.502020835876465)
2017-05-09 15:53:34,844 : INFO : (33, 1.1817998886108398)
2017-05-09 15:53:34,846 : INFO : (34, 1.155847430229187)
2017-05-09 15:53:34,849 : INFO : (35, 2.317129611968994)
2017-05-09 15:53:34,853 : INFO : (28, 10.493244171142578)
2017-05-09 15:53:34,855 : INFO : (26, 11.677874565124512)
2017-05-09 15:53:34,857 : INFO : (25, 12.846562385559082)
2017-05-09 15:53:34,861 : INFO : (21, 17.38468360900879)
2017-05-09 15:53:34,864 : INFO : (19, 21.961782455444336)
2017-05-09 15:53:34,866 : INFO : (36, 1.2072006464004517)
2017-05-09 15:53:34,870 : INFO : (4, 40.32240676879883)
2017-05-09 15:53:34,880 : INFO : (1, 0.0)
2017-05-09 15:53:34,882 : INFO : (2, 0.0)
2017-05-09 15:53:34,884 : INFO : (3, 0.0)
2017-05-09 15:53:34,886 : INFO : (5, 0.0)
2017-05-09 15:53:34,888 : INFO : (6, 0.0)
2017-05-09 15:53:34,890 : INFO : (7, 0.0)
2017-05-09 15:53:34,892 : INFO : (8, 0.0)
2017-05-09 15:53:34,894 : INFO : (10, 0.0)
2017-05-09 15:53:34,898 : INFO : (9, 0.0)
2017-05-09 15:53:34,900 : INFO : (11, 0.0)
2017-05-09 15:53:34,902 : INFO : (12, 0.0)
2017-05-09 15:53:34,904 : INFO : (14, 0.0)
2017-05-09 15:53:34,907 : INFO : (13, 0.0)
2017-05-09 15:53:34,909 : INFO : (15, 0.0)
2017-05-09 15:53:34,911 : INFO : (16, 0.0)
2017-05-09 15:53:34,913 : INFO : (17, 0.0)
2017-05-09 15:53:34,915 : INFO : (18, 0.0)
2017-05-09 15:53:34,918 : INFO : (20, 0.0)
2017-05-09 15:53:34,920 : INFO : (22, 0.0)
2017-05-09 15:53:34,922 : INFO : (23, 0.0)
2017-05-09 15:53:34,924 : INFO : (24, 0.0)
2017-05-09 15:53:34,926 : INFO : (27, 0.0)
2017-05-09 15:53:34,928 : INFO : (29, 0.0)
2017-05-09 15:53:34,931 : INFO : (30, 0.0)
2017-05-09 15:53:34,933 : INFO : (31, 0.0)
2017-05-09 15:53:34,935 : INFO : (32, 0.0)
2017-05-09 15:53:34,937 : INFO : (33, 0.0)
2017-05-09 15:53:34,939 : INFO : (34, 0.0)
2017-05-09 15:53:34,941 : INFO : (35, 0.0)
2017-05-09 15:53:34,944 : INFO : (28, 0.0)
2017-05-09 15:53:34,946 : INFO : (26, 0.0)
2017-05-09 15:53:34,949 : INFO : (25, 0.0)
2017-05-09 15:53:34,951 : INFO : (21, 0.0)
2017-05-09 15:53:34,954 : INFO : (19, 0.0)
2017-05-09 15:53:34,956 : INFO : (36, 0.0)
2017-05-09 15:53:34,959 : INFO : (4, 0.0)
2017-05-09 15:53:34,963 : INFO : (1, 0.0)
2017-05-09 15:53:34,965 : INFO : (2, 0.0)
2017-05-09 15:53:34,967 : INFO : (3, 0.0)
2017-05-09 15:53:34,969 : INFO : (6, 0.0)
2017-05-09 15:53:34,971 : INFO : (7, 0.0)
2017-05-09 15:53:34,974 : INFO : (10, 0.0)
2017-05-09 15:53:34,976 : INFO : (11, 0.0)
2017-05-09 15:53:34,978 : INFO : (9, 0.0)
2017-05-09 15:53:34,980 : INFO : (8, 0.0)
2017-05-09 15:53:34,983 : INFO : (12, 0.0)
2017-05-09 15:53:34,985 : INFO : (13, 0.0)
2017-05-09 15:53:34,987 : INFO : (5, 0.0)
2017-05-09 15:53:34,990 : INFO : (4, 0.0)
2017-05-09 15:53:34,992 : INFO : (14, 0.0)
2017-05-09 15:53:34,994 : INFO : (15, 0.0)
2017-05-09 15:53:34,996 : INFO : (17, 0.0)
2017-05-09 15:53:34,998 : INFO : (18, 0.0)
2017-05-09 15:53:35,000 : INFO : (21, 0.0)
2017-05-09 15:53:35,002 : INFO : (20, 0.0)
2017-05-09 15:53:35,004 : INFO : (19, 0.0)
2017-05-09 15:53:35,006 : INFO : (22, 0.0)
2017-05-09 15:53:35,008 : INFO : (23, 0.0)
2017-05-09 15:53:35,010 : INFO : (24, 0.0)
2017-05-09 15:53:35,012 : INFO : (26, 0.0)
2017-05-09 15:53:35,014 : INFO : (27, 0.0)
2017-05-09 15:53:35,016 : INFO : (29, 0.0)
2017-05-09 15:53:35,019 : INFO : (28, 0.0)
2017-05-09 15:53:35,021 : INFO : (30, 0.0)
2017-05-09 15:53:35,023 : INFO : (33, 0.0)
2017-05-09 15:53:35,025 : INFO : (35, 0.0)
2017-05-09 15:53:35,027 : INFO : (34, 0.0)
2017-05-09 15:53:35,030 : INFO : (36, 0.0)
2017-05-09 15:53:35,032 : INFO : (32, 0.0)
2017-05-09 15:53:35,034 : INFO : (31, 0.0)
2017-05-09 15:53:35,037 : INFO : (25, 0.0)
2017-05-09 15:53:35,039 : INFO : (37, 0.0)
2017-05-09 15:53:35,042 : INFO : (16, 0.0)
2017-05-09 15:53:35,047 : INFO : (4, 0.0)
2017-05-09 15:53:35,049 : INFO : (3, 0.0)
2017-05-09 15:53:35,051 : INFO : (2, 0.0)
2017-05-09 15:53:35,055 : INFO : (5, 0.0)
2017-05-09 15:53:35,057 : INFO : (6, 0.0)
2017-05-09 15:53:35,059 : INFO : (7, 0.0)
2017-05-09 15:53:35,061 : INFO : (9, 0.0)
2017-05-09 15:53:35,063 : INFO : (10, 0.0)
2017-05-09 15:53:35,065 : INFO : (11, 0.0)
2017-05-09 15:53:35,067 : INFO : (13, 0.0)
2017-05-09 15:53:35,069 : INFO : (14, 0.0)
2017-05-09 15:53:35,071 : INFO : (15, 0.0)
2017-05-09 15:53:35,073 : INFO : (16, 0.0)
2017-05-09 15:53:35,075 : INFO : (17, 0.0)
2017-05-09 15:53:35,077 : INFO : (18, 0.0)
2017-05-09 15:53:35,079 : INFO : (19, 0.0)
2017-05-09 15:53:35,081 : INFO : (22, 0.0)
2017-05-09 15:53:35,083 : INFO : (21, 0.0)
2017-05-09 15:53:35,087 : INFO : (20, 0.0)
2017-05-09 15:53:35,090 : INFO : (12, 0.0)
2017-05-09 15:53:35,092 : INFO : (23, 0.0)
2017-05-09 15:53:35,095 : INFO : (8, 0.0)
2017-05-09 15:53:35,098 : INFO : (1, 0.0)
2017-05-09 15:53:35,102 : INFO : (1, 0.0)
2017-05-09 15:53:35,104 : INFO : (2, 0.0)
2017-05-09 15:53:35,106 : INFO : (3, 0.0)
2017-05-09 15:53:35,109 : INFO : (5, 0.0)
2017-05-09 15:53:35,111 : INFO : (8, 0.0)
2017-05-09 15:53:35,113 : INFO : (9, 0.0)
2017-05-09 15:53:35,115 : INFO : (7, 0.0)
2017-05-09 15:53:35,117 : INFO : (10, 0.0)
2017-05-09 15:53:35,119 : INFO : (12, 0.0)
2017-05-09 15:53:35,121 : INFO : (11, 0.0)
2017-05-09 15:53:35,124 : INFO : (6, 0.0)
2017-05-09 15:53:35,126 : INFO : (4, 0.0)
2017-05-09 15:53:35,128 : INFO : (13, 0.0)
2017-05-09 15:53:35,131 : INFO : (14, 0.0)
2017-05-09 15:53:35,133 : INFO : (15, 0.0)
2017-05-09 15:53:35,135 : INFO : (16, 0.0)
2017-05-09 15:53:35,137 : INFO : (17, 0.0)
2017-05-09 15:53:35,139 : INFO : (19, 0.0)
2017-05-09 15:53:35,141 : INFO : (20, 0.0)
2017-05-09 15:53:35,143 : INFO : (21, 0.0)
2017-05-09 15:53:35,146 : INFO : (18, 0.0)
2017-05-09 15:53:35,147 : INFO : ==> Train loss   : 1.082739
2017-05-09 15:53:35,147 : INFO : Epoch
2017-05-09 15:53:35,147 : INFO : 7
2017-05-09 15:53:35,147 : INFO : train percentage
2017-05-09 15:53:35,147 : INFO : 0.0
2017-05-09 15:53:35,148 : INFO : Epoch
2017-05-09 15:53:35,148 : INFO : 7
2017-05-09 15:53:35,148 : INFO : dev percentage
2017-05-09 15:53:35,148 : INFO : 0.0
2017-05-09 15:53:35,148 : INFO : Epoch
2017-05-09 15:53:35,148 : INFO : 7
2017-05-09 15:53:35,148 : INFO : test percentage
2017-05-09 15:53:35,148 : INFO : 0.0
2017-05-09 15:53:35,153 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:35,156 : INFO : (2, 1.0745457410812378)
2017-05-09 15:53:35,158 : INFO : (3, 1.1600637435913086)
2017-05-09 15:53:35,160 : INFO : (6, 1.1321847438812256)
2017-05-09 15:53:35,162 : INFO : (7, 1.1577483415603638)
2017-05-09 15:53:35,165 : INFO : (10, 1.1717875003814697)
2017-05-09 15:53:35,167 : INFO : (11, 2.3070101737976074)
2017-05-09 15:53:35,169 : INFO : (9, 3.467470169067383)
2017-05-09 15:53:35,172 : INFO : (8, 5.735894680023193)
2017-05-09 15:53:35,174 : INFO : (12, 1.1339832544326782)
2017-05-09 15:53:35,176 : INFO : (13, 8.002062797546387)
2017-05-09 15:53:35,179 : INFO : (5, 9.174448013305664)
2017-05-09 15:53:35,182 : INFO : (4, 13.670205116271973)
2017-05-09 15:53:35,184 : INFO : (14, 1.188725233078003)
2017-05-09 15:53:35,186 : INFO : (15, 1.1875367164611816)
2017-05-09 15:53:35,188 : INFO : (17, 1.17649507522583)
2017-05-09 15:53:35,190 : INFO : (18, 1.1859716176986694)
2017-05-09 15:53:35,194 : INFO : (21, 1.148364782333374)
2017-05-09 15:53:35,197 : INFO : (20, 2.3133182525634766)
2017-05-09 15:53:35,200 : INFO : (19, 4.6813507080078125)
2017-05-09 15:53:35,202 : INFO : (22, 1.1968988180160522)
2017-05-09 15:53:35,204 : INFO : (23, 1.0630097389221191)
2017-05-09 15:53:35,207 : INFO : (24, 1.0733129978179932)
2017-05-09 15:53:35,209 : INFO : (26, 1.1568262577056885)
2017-05-09 15:53:35,211 : INFO : (27, 1.1412229537963867)
2017-05-09 15:53:35,213 : INFO : (29, 1.1548422574996948)
2017-05-09 15:53:35,216 : INFO : (28, 4.592202186584473)
2017-05-09 15:53:35,218 : INFO : (30, 1.1657209396362305)
2017-05-09 15:53:35,221 : INFO : (33, 1.1145955324172974)
2017-05-09 15:53:35,223 : INFO : (35, 1.1548422574996948)
2017-05-09 15:53:35,225 : INFO : (34, 3.3807010650634766)
2017-05-09 15:53:35,229 : INFO : (36, 4.50806999206543)
2017-05-09 15:53:35,231 : INFO : (32, 5.662715435028076)
2017-05-09 15:53:35,236 : INFO : (31, 12.58568000793457)
2017-05-09 15:53:35,240 : INFO : (25, 22.970571517944336)
2017-05-09 15:53:35,242 : INFO : (37, 1.2072006464004517)
2017-05-09 15:53:35,246 : INFO : (16, 41.31589126586914)
2017-05-09 15:53:35,250 : INFO : (1, 1.1577483415603638)
2017-05-09 15:53:35,252 : INFO : (2, 2.256925582885742)
2017-05-09 15:53:35,255 : INFO : (3, 1.188725233078003)
2017-05-09 15:53:35,258 : INFO : (5, 1.1619560718536377)
2017-05-09 15:53:35,261 : INFO : (6, 1.1730492115020752)
2017-05-09 15:53:35,264 : INFO : (7, 1.1717875003814697)
2017-05-09 15:53:35,266 : INFO : (8, 1.1558316946029663)
2017-05-09 15:53:35,268 : INFO : (10, 1.1548422574996948)
2017-05-09 15:53:35,271 : INFO : (9, 4.64690637588501)
2017-05-09 15:53:35,273 : INFO : (11, 1.0916519165039062)
2017-05-09 15:53:35,275 : INFO : (12, 1.1321847438812256)
2017-05-09 15:53:35,278 : INFO : (14, 1.1339832544326782)
2017-05-09 15:53:35,281 : INFO : (13, 11.48015022277832)
2017-05-09 15:53:35,284 : INFO : (15, 1.153801441192627)
2017-05-09 15:53:35,286 : INFO : (16, 1.17649507522583)
2017-05-09 15:53:35,288 : INFO : (17, 1.149524211883545)
2017-05-09 15:53:35,291 : INFO : (18, 1.1548422574996948)
2017-05-09 15:53:35,293 : INFO : (20, 1.1619560718536377)
2017-05-09 15:53:35,297 : INFO : (22, 1.1859716176986694)
2017-05-09 15:53:35,299 : INFO : (23, 2.281428813934326)
2017-05-09 15:53:35,301 : INFO : (24, 1.1686874628067017)
2017-05-09 15:53:35,304 : INFO : (27, 1.1727831363677979)
2017-05-09 15:53:35,306 : INFO : (29, 1.1546502113342285)
2017-05-09 15:56:53,782 : INFO : LOG_FILE
2017-05-09 15:56:53,782 : INFO : _________________________________start___________________________________
2017-05-09 15:56:53,789 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 15:56:53,819 : INFO : ==> SST vocabulary size : 21701
2017-05-09 15:56:59,527 : INFO : _param count_
2017-05-09 15:56:59,527 : INFO : torch.Size([168, 300])
2017-05-09 15:56:59,527 : INFO : torch.Size([168])
2017-05-09 15:56:59,528 : INFO : torch.Size([168, 168])
2017-05-09 15:56:59,528 : INFO : torch.Size([168])
2017-05-09 15:56:59,528 : INFO : torch.Size([168, 168])
2017-05-09 15:56:59,528 : INFO : torch.Size([168])
2017-05-09 15:56:59,528 : INFO : torch.Size([168, 300])
2017-05-09 15:56:59,528 : INFO : torch.Size([168])
2017-05-09 15:56:59,528 : INFO : torch.Size([168, 300])
2017-05-09 15:56:59,529 : INFO : torch.Size([168])
2017-05-09 15:56:59,529 : INFO : torch.Size([168, 168])
2017-05-09 15:56:59,529 : INFO : torch.Size([168])
2017-05-09 15:56:59,529 : INFO : torch.Size([168, 300])
2017-05-09 15:56:59,529 : INFO : torch.Size([168])
2017-05-09 15:56:59,529 : INFO : torch.Size([168, 168])
2017-05-09 15:56:59,529 : INFO : torch.Size([168])
2017-05-09 15:56:59,529 : INFO : torch.Size([3, 168])
2017-05-09 15:56:59,530 : INFO : torch.Size([3])
2017-05-09 15:56:59,530 : INFO : sum
2017-05-09 15:56:59,530 : INFO : 316347
2017-05-09 15:56:59,530 : INFO : ____________
2017-05-09 15:57:12,541 : INFO : LOG_FILE
2017-05-09 15:57:12,541 : INFO : _________________________________start___________________________________
2017-05-09 15:57:12,548 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 15:57:12,578 : INFO : ==> SST vocabulary size : 21701
2017-05-09 15:57:18,186 : INFO : _param count_
2017-05-09 15:57:18,187 : INFO : torch.Size([168, 300])
2017-05-09 15:57:18,187 : INFO : torch.Size([168])
2017-05-09 15:57:18,187 : INFO : torch.Size([168, 168])
2017-05-09 15:57:18,187 : INFO : torch.Size([168])
2017-05-09 15:57:18,187 : INFO : torch.Size([168, 168])
2017-05-09 15:57:18,187 : INFO : torch.Size([168])
2017-05-09 15:57:18,187 : INFO : torch.Size([168, 300])
2017-05-09 15:57:18,188 : INFO : torch.Size([168])
2017-05-09 15:57:18,188 : INFO : torch.Size([168, 300])
2017-05-09 15:57:18,188 : INFO : torch.Size([168])
2017-05-09 15:57:18,188 : INFO : torch.Size([168, 168])
2017-05-09 15:57:18,188 : INFO : torch.Size([168])
2017-05-09 15:57:18,188 : INFO : torch.Size([168, 300])
2017-05-09 15:57:18,188 : INFO : torch.Size([168])
2017-05-09 15:57:18,189 : INFO : torch.Size([168, 168])
2017-05-09 15:57:18,189 : INFO : torch.Size([168])
2017-05-09 15:57:18,189 : INFO : torch.Size([3, 168])
2017-05-09 15:57:18,189 : INFO : torch.Size([3])
2017-05-09 15:57:18,189 : INFO : sum
2017-05-09 15:57:18,189 : INFO : 316347
2017-05-09 15:57:18,189 : INFO : ____________
2017-05-09 15:58:06,079 : INFO : LOG_FILE
2017-05-09 15:58:06,080 : INFO : _________________________________start___________________________________
2017-05-09 15:58:06,086 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 15:58:06,116 : INFO : ==> SST vocabulary size : 21701
2017-05-09 15:58:11,721 : INFO : _param count_
2017-05-09 15:58:11,721 : INFO : torch.Size([168, 300])
2017-05-09 15:58:11,721 : INFO : torch.Size([168])
2017-05-09 15:58:11,722 : INFO : torch.Size([168, 168])
2017-05-09 15:58:11,722 : INFO : torch.Size([168])
2017-05-09 15:58:11,722 : INFO : torch.Size([168, 168])
2017-05-09 15:58:11,722 : INFO : torch.Size([168])
2017-05-09 15:58:11,722 : INFO : torch.Size([168, 300])
2017-05-09 15:58:11,722 : INFO : torch.Size([168])
2017-05-09 15:58:11,722 : INFO : torch.Size([168, 300])
2017-05-09 15:58:11,723 : INFO : torch.Size([168])
2017-05-09 15:58:11,723 : INFO : torch.Size([168, 168])
2017-05-09 15:58:11,723 : INFO : torch.Size([168])
2017-05-09 15:58:11,723 : INFO : torch.Size([168, 300])
2017-05-09 15:58:11,723 : INFO : torch.Size([168])
2017-05-09 15:58:11,723 : INFO : torch.Size([168, 168])
2017-05-09 15:58:11,723 : INFO : torch.Size([168])
2017-05-09 15:58:11,723 : INFO : torch.Size([3, 168])
2017-05-09 15:58:11,724 : INFO : torch.Size([3])
2017-05-09 15:58:11,724 : INFO : sum
2017-05-09 15:58:11,724 : INFO : 316347
2017-05-09 15:58:11,724 : INFO : ____________
2017-05-09 15:59:04,757 : INFO : LOG_FILE
2017-05-09 15:59:04,757 : INFO : _________________________________start___________________________________
2017-05-09 15:59:04,764 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 15:59:04,792 : INFO : ==> SST vocabulary size : 21701
2017-05-09 15:59:10,401 : INFO : _param count_
2017-05-09 15:59:10,401 : INFO : torch.Size([168, 300])
2017-05-09 15:59:10,401 : INFO : torch.Size([168])
2017-05-09 15:59:10,401 : INFO : torch.Size([168, 168])
2017-05-09 15:59:10,402 : INFO : torch.Size([168])
2017-05-09 15:59:10,402 : INFO : torch.Size([168, 168])
2017-05-09 15:59:10,402 : INFO : torch.Size([168])
2017-05-09 15:59:10,402 : INFO : torch.Size([168, 300])
2017-05-09 15:59:10,402 : INFO : torch.Size([168])
2017-05-09 15:59:10,402 : INFO : torch.Size([168, 300])
2017-05-09 15:59:10,402 : INFO : torch.Size([168])
2017-05-09 15:59:10,403 : INFO : torch.Size([168, 168])
2017-05-09 15:59:10,403 : INFO : torch.Size([168])
2017-05-09 15:59:10,403 : INFO : torch.Size([168, 300])
2017-05-09 15:59:10,403 : INFO : torch.Size([168])
2017-05-09 15:59:10,403 : INFO : torch.Size([168, 168])
2017-05-09 15:59:10,403 : INFO : torch.Size([168])
2017-05-09 15:59:10,403 : INFO : torch.Size([3, 168])
2017-05-09 15:59:10,404 : INFO : torch.Size([3])
2017-05-09 15:59:10,404 : INFO : sum
2017-05-09 15:59:10,404 : INFO : 316347
2017-05-09 15:59:10,404 : INFO : ____________
2017-05-09 16:01:11,263 : INFO : LOG_FILE
2017-05-09 16:01:11,263 : INFO : _________________________________start___________________________________
2017-05-09 16:01:11,271 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 16:01:11,299 : INFO : ==> SST vocabulary size : 21701
2017-05-09 16:01:16,925 : INFO : _param count_
2017-05-09 16:01:16,926 : INFO : torch.Size([168, 300])
2017-05-09 16:01:16,926 : INFO : torch.Size([168])
2017-05-09 16:01:16,926 : INFO : torch.Size([168, 168])
2017-05-09 16:01:16,926 : INFO : torch.Size([168])
2017-05-09 16:01:16,926 : INFO : torch.Size([168, 168])
2017-05-09 16:01:16,926 : INFO : torch.Size([168])
2017-05-09 16:01:16,927 : INFO : torch.Size([168, 300])
2017-05-09 16:01:16,927 : INFO : torch.Size([168])
2017-05-09 16:01:16,927 : INFO : torch.Size([168, 300])
2017-05-09 16:01:16,927 : INFO : torch.Size([168])
2017-05-09 16:01:16,927 : INFO : torch.Size([168, 168])
2017-05-09 16:01:16,927 : INFO : torch.Size([168])
2017-05-09 16:01:16,927 : INFO : torch.Size([168, 300])
2017-05-09 16:01:16,927 : INFO : torch.Size([168])
2017-05-09 16:01:16,927 : INFO : torch.Size([168, 168])
2017-05-09 16:01:16,928 : INFO : torch.Size([168])
2017-05-09 16:01:16,928 : INFO : torch.Size([3, 168])
2017-05-09 16:01:16,928 : INFO : torch.Size([3])
2017-05-09 16:01:16,928 : INFO : sum
2017-05-09 16:01:16,928 : INFO : 316347
2017-05-09 16:01:16,928 : INFO : ____________
2017-05-09 16:03:21,855 : INFO : LOG_FILE
2017-05-09 16:03:21,856 : INFO : _________________________________start___________________________________
2017-05-09 16:03:21,863 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 16:03:21,891 : INFO : ==> SST vocabulary size : 21701
2017-05-09 16:03:34,479 : INFO : _param count_
2017-05-09 16:03:34,479 : INFO : torch.Size([168, 300])
2017-05-09 16:03:34,480 : INFO : torch.Size([168])
2017-05-09 16:03:34,480 : INFO : torch.Size([168, 168])
2017-05-09 16:03:34,480 : INFO : torch.Size([168])
2017-05-09 16:03:34,480 : INFO : torch.Size([168, 168])
2017-05-09 16:03:34,480 : INFO : torch.Size([168])
2017-05-09 16:03:34,480 : INFO : torch.Size([168, 300])
2017-05-09 16:03:34,480 : INFO : torch.Size([168])
2017-05-09 16:03:34,481 : INFO : torch.Size([168, 300])
2017-05-09 16:03:34,481 : INFO : torch.Size([168])
2017-05-09 16:03:34,481 : INFO : torch.Size([168, 168])
2017-05-09 16:03:34,481 : INFO : torch.Size([168])
2017-05-09 16:03:34,481 : INFO : torch.Size([168, 300])
2017-05-09 16:03:34,481 : INFO : torch.Size([168])
2017-05-09 16:03:34,481 : INFO : torch.Size([168, 168])
2017-05-09 16:03:34,481 : INFO : torch.Size([168])
2017-05-09 16:03:34,482 : INFO : torch.Size([3, 168])
2017-05-09 16:03:34,482 : INFO : torch.Size([3])
2017-05-09 16:03:34,482 : INFO : sum
2017-05-09 16:03:34,482 : INFO : 316347
2017-05-09 16:03:34,482 : INFO : ____________
2017-05-09 16:03:34,482 : INFO : ==> File found, loading to memory
2017-05-09 16:03:41,345 : INFO : ==> GLOVE vocabulary size: 2196016
2017-05-09 16:03:41,586 : INFO : done creating emb, quit
2017-05-09 16:03:41,586 : INFO : quit program
2017-05-09 16:03:44,519 : INFO : LOG_FILE
2017-05-09 16:03:44,519 : INFO : _________________________________start___________________________________
2017-05-09 16:03:44,527 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 16:03:44,554 : INFO : ==> SST vocabulary size : 21701
2017-05-09 16:03:50,156 : INFO : _param count_
2017-05-09 16:03:50,156 : INFO : torch.Size([168, 300])
2017-05-09 16:03:50,157 : INFO : torch.Size([168])
2017-05-09 16:03:50,157 : INFO : torch.Size([168, 168])
2017-05-09 16:03:50,157 : INFO : torch.Size([168])
2017-05-09 16:03:50,157 : INFO : torch.Size([168, 168])
2017-05-09 16:03:50,157 : INFO : torch.Size([168])
2017-05-09 16:03:50,157 : INFO : torch.Size([168, 300])
2017-05-09 16:03:50,158 : INFO : torch.Size([168])
2017-05-09 16:03:50,158 : INFO : torch.Size([168, 300])
2017-05-09 16:03:50,158 : INFO : torch.Size([168])
2017-05-09 16:03:50,158 : INFO : torch.Size([168, 168])
2017-05-09 16:03:50,158 : INFO : torch.Size([168])
2017-05-09 16:03:50,158 : INFO : torch.Size([168, 300])
2017-05-09 16:03:50,158 : INFO : torch.Size([168])
2017-05-09 16:03:50,158 : INFO : torch.Size([168, 168])
2017-05-09 16:03:50,159 : INFO : torch.Size([168])
2017-05-09 16:03:50,159 : INFO : torch.Size([3, 168])
2017-05-09 16:03:50,159 : INFO : torch.Size([3])
2017-05-09 16:03:50,159 : INFO : sum
2017-05-09 16:03:50,159 : INFO : 316347
2017-05-09 16:03:50,159 : INFO : ____________
2017-05-09 16:12:34,526 : INFO : LOG_FILE
2017-05-09 16:12:34,527 : INFO : _________________________________start___________________________________
2017-05-09 16:12:34,533 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 16:12:34,561 : INFO : ==> SST vocabulary size : 21701
2017-05-09 16:12:40,379 : INFO : _param count_
2017-05-09 16:12:40,380 : INFO : torch.Size([168, 300])
2017-05-09 16:12:40,380 : INFO : torch.Size([168])
2017-05-09 16:12:40,380 : INFO : torch.Size([168, 168])
2017-05-09 16:12:40,380 : INFO : torch.Size([168])
2017-05-09 16:12:40,380 : INFO : torch.Size([168, 168])
2017-05-09 16:12:40,380 : INFO : torch.Size([168])
2017-05-09 16:12:40,380 : INFO : torch.Size([168, 300])
2017-05-09 16:12:40,381 : INFO : torch.Size([168])
2017-05-09 16:12:40,381 : INFO : torch.Size([168, 300])
2017-05-09 16:12:40,381 : INFO : torch.Size([168])
2017-05-09 16:12:40,381 : INFO : torch.Size([168, 168])
2017-05-09 16:12:40,381 : INFO : torch.Size([168])
2017-05-09 16:12:40,381 : INFO : torch.Size([168, 300])
2017-05-09 16:12:40,381 : INFO : torch.Size([168])
2017-05-09 16:12:40,382 : INFO : torch.Size([168, 168])
2017-05-09 16:12:40,382 : INFO : torch.Size([168])
2017-05-09 16:12:40,382 : INFO : torch.Size([3, 168])
2017-05-09 16:12:40,382 : INFO : torch.Size([3])
2017-05-09 16:12:40,382 : INFO : sum
2017-05-09 16:12:40,382 : INFO : 316347
2017-05-09 16:12:40,382 : INFO : ____________
2017-05-09 16:12:48,613 : INFO : LOG_FILE
2017-05-09 16:12:48,614 : INFO : _________________________________start___________________________________
2017-05-09 16:12:48,620 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 16:12:48,649 : INFO : ==> SST vocabulary size : 21701
2017-05-09 16:12:54,238 : INFO : _param count_
2017-05-09 16:12:54,238 : INFO : torch.Size([168, 300])
2017-05-09 16:12:54,238 : INFO : torch.Size([168])
2017-05-09 16:12:54,238 : INFO : torch.Size([168, 168])
2017-05-09 16:12:54,238 : INFO : torch.Size([168])
2017-05-09 16:12:54,238 : INFO : torch.Size([168, 168])
2017-05-09 16:12:54,239 : INFO : torch.Size([168])
2017-05-09 16:12:54,239 : INFO : torch.Size([168, 300])
2017-05-09 16:12:54,239 : INFO : torch.Size([168])
2017-05-09 16:12:54,239 : INFO : torch.Size([168, 300])
2017-05-09 16:12:54,239 : INFO : torch.Size([168])
2017-05-09 16:12:54,239 : INFO : torch.Size([168, 168])
2017-05-09 16:12:54,239 : INFO : torch.Size([168])
2017-05-09 16:12:54,240 : INFO : torch.Size([168, 300])
2017-05-09 16:12:54,240 : INFO : torch.Size([168])
2017-05-09 16:12:54,240 : INFO : torch.Size([168, 168])
2017-05-09 16:12:54,240 : INFO : torch.Size([168])
2017-05-09 16:12:54,240 : INFO : torch.Size([3, 168])
2017-05-09 16:12:54,240 : INFO : torch.Size([3])
2017-05-09 16:12:54,240 : INFO : sum
2017-05-09 16:12:54,240 : INFO : 316347
2017-05-09 16:12:54,241 : INFO : ____________
2017-05-09 16:13:23,098 : INFO : LOG_FILE
2017-05-09 16:13:23,098 : INFO : _________________________________start___________________________________
2017-05-09 16:13:23,104 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 16:13:23,131 : INFO : ==> SST vocabulary size : 21701
2017-05-09 16:13:28,709 : INFO : _param count_
2017-05-09 16:13:28,710 : INFO : torch.Size([168, 300])
2017-05-09 16:13:28,711 : INFO : torch.Size([168])
2017-05-09 16:13:28,711 : INFO : torch.Size([168, 168])
2017-05-09 16:13:28,711 : INFO : torch.Size([168])
2017-05-09 16:13:28,712 : INFO : torch.Size([168, 168])
2017-05-09 16:13:28,712 : INFO : torch.Size([168])
2017-05-09 16:13:28,712 : INFO : torch.Size([168, 300])
2017-05-09 16:13:28,712 : INFO : torch.Size([168])
2017-05-09 16:13:28,712 : INFO : torch.Size([168, 300])
2017-05-09 16:13:28,712 : INFO : torch.Size([168])
2017-05-09 16:13:28,712 : INFO : torch.Size([168, 168])
2017-05-09 16:13:28,713 : INFO : torch.Size([168])
2017-05-09 16:13:28,713 : INFO : torch.Size([168, 300])
2017-05-09 16:13:28,713 : INFO : torch.Size([168])
2017-05-09 16:13:28,713 : INFO : torch.Size([168, 168])
2017-05-09 16:13:28,713 : INFO : torch.Size([168])
2017-05-09 16:13:28,713 : INFO : torch.Size([3, 168])
2017-05-09 16:13:28,713 : INFO : torch.Size([3])
2017-05-09 16:13:28,714 : INFO : sum
2017-05-09 16:13:28,714 : INFO : 316347
2017-05-09 16:13:28,714 : INFO : ____________
2017-05-09 16:14:08,105 : INFO : LOG_FILE
2017-05-09 16:14:08,105 : INFO : _________________________________start___________________________________
2017-05-09 16:14:08,111 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 16:14:08,141 : INFO : ==> SST vocabulary size : 21701
2017-05-09 16:14:13,790 : INFO : _param count_
2017-05-09 16:14:13,790 : INFO : torch.Size([168, 300])
2017-05-09 16:14:13,791 : INFO : torch.Size([168])
2017-05-09 16:14:13,791 : INFO : torch.Size([168, 168])
2017-05-09 16:14:13,791 : INFO : torch.Size([168])
2017-05-09 16:14:13,791 : INFO : torch.Size([168, 168])
2017-05-09 16:14:13,791 : INFO : torch.Size([168])
2017-05-09 16:14:13,791 : INFO : torch.Size([168, 300])
2017-05-09 16:14:13,791 : INFO : torch.Size([168])
2017-05-09 16:14:13,792 : INFO : torch.Size([168, 300])
2017-05-09 16:14:13,792 : INFO : torch.Size([168])
2017-05-09 16:14:13,792 : INFO : torch.Size([168, 168])
2017-05-09 16:14:13,792 : INFO : torch.Size([168])
2017-05-09 16:14:13,792 : INFO : torch.Size([168, 300])
2017-05-09 16:14:13,792 : INFO : torch.Size([168])
2017-05-09 16:14:13,792 : INFO : torch.Size([168, 168])
2017-05-09 16:14:13,792 : INFO : torch.Size([168])
2017-05-09 16:14:13,793 : INFO : torch.Size([3, 168])
2017-05-09 16:14:13,793 : INFO : torch.Size([3])
2017-05-09 16:14:13,793 : INFO : sum
2017-05-09 16:14:13,793 : INFO : 316347
2017-05-09 16:14:13,793 : INFO : ____________
2017-05-09 16:15:43,954 : INFO : LOG_FILE
2017-05-09 16:15:43,955 : INFO : _________________________________start___________________________________
2017-05-09 16:15:43,968 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 16:15:44,201 : INFO : ==> SST vocabulary size : 21701
2017-05-09 16:15:50,284 : INFO : _param count_
2017-05-09 16:15:50,286 : INFO : torch.Size([168, 300])
2017-05-09 16:15:50,287 : INFO : torch.Size([168])
2017-05-09 16:15:50,287 : INFO : torch.Size([168, 168])
2017-05-09 16:15:50,288 : INFO : torch.Size([168])
2017-05-09 16:15:50,289 : INFO : torch.Size([168, 168])
2017-05-09 16:15:50,289 : INFO : torch.Size([168])
2017-05-09 16:15:50,290 : INFO : torch.Size([168, 300])
2017-05-09 16:15:50,290 : INFO : torch.Size([168])
2017-05-09 16:15:50,291 : INFO : torch.Size([168, 300])
2017-05-09 16:15:50,291 : INFO : torch.Size([168])
2017-05-09 16:15:50,292 : INFO : torch.Size([168, 168])
2017-05-09 16:15:50,293 : INFO : torch.Size([168])
2017-05-09 16:15:50,293 : INFO : torch.Size([168, 300])
2017-05-09 16:15:50,294 : INFO : torch.Size([168])
2017-05-09 16:15:50,294 : INFO : torch.Size([168, 168])
2017-05-09 16:15:50,295 : INFO : torch.Size([168])
2017-05-09 16:15:50,296 : INFO : torch.Size([3, 168])
2017-05-09 16:15:50,296 : INFO : torch.Size([3])
2017-05-09 16:15:50,297 : INFO : sum
2017-05-09 16:15:50,297 : INFO : 316347
2017-05-09 16:15:50,298 : INFO : ____________
2017-05-09 16:21:16,057 : INFO : LOG_FILE
2017-05-09 16:21:16,058 : INFO : _________________________________start___________________________________
2017-05-09 16:21:16,064 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 16:21:16,095 : INFO : ==> SST vocabulary size : 21701
2017-05-09 16:21:21,933 : INFO : _param count_
2017-05-09 16:21:21,933 : INFO : torch.Size([168, 300])
2017-05-09 16:21:21,933 : INFO : torch.Size([168])
2017-05-09 16:21:21,933 : INFO : torch.Size([168, 168])
2017-05-09 16:21:21,934 : INFO : torch.Size([168])
2017-05-09 16:21:21,934 : INFO : torch.Size([168, 168])
2017-05-09 16:21:21,934 : INFO : torch.Size([168])
2017-05-09 16:21:21,934 : INFO : torch.Size([168, 300])
2017-05-09 16:21:21,934 : INFO : torch.Size([168])
2017-05-09 16:21:21,934 : INFO : torch.Size([168, 300])
2017-05-09 16:21:21,934 : INFO : torch.Size([168])
2017-05-09 16:21:21,934 : INFO : torch.Size([168, 168])
2017-05-09 16:21:21,935 : INFO : torch.Size([168])
2017-05-09 16:21:21,935 : INFO : torch.Size([168, 300])
2017-05-09 16:21:21,935 : INFO : torch.Size([168])
2017-05-09 16:21:21,935 : INFO : torch.Size([168, 168])
2017-05-09 16:21:21,935 : INFO : torch.Size([168])
2017-05-09 16:21:21,935 : INFO : torch.Size([3, 168])
2017-05-09 16:21:21,936 : INFO : torch.Size([3])
2017-05-09 16:21:21,936 : INFO : sum
2017-05-09 16:21:21,936 : INFO : 316347
2017-05-09 16:21:21,936 : INFO : ____________
2017-05-09 16:27:52,997 : INFO : LOG_FILE
2017-05-09 16:27:52,998 : INFO : _________________________________start___________________________________
2017-05-09 16:27:53,004 : INFO : Namespace(batchsize=25, cuda=True, data='data/sst/', emblr=0.1, epochs=200, fine_grain=False, glove='../treelstm.pytorch/data/glove/', input_dim=300, lower=True, lr=0.05, mem_dim=168, num_classes=3, optim='adagrad', reg=0.0001, seed=123, wd=0)
2017-05-09 16:27:53,033 : INFO : ==> SST vocabulary size : 21701
2017-05-09 16:27:58,702 : INFO : _param count_
2017-05-09 16:27:58,703 : INFO : torch.Size([168, 300])
2017-05-09 16:27:58,703 : INFO : torch.Size([168])
2017-05-09 16:27:58,703 : INFO : torch.Size([168, 168])
2017-05-09 16:27:58,703 : INFO : torch.Size([168])
2017-05-09 16:27:58,703 : INFO : torch.Size([168, 168])
2017-05-09 16:27:58,704 : INFO : torch.Size([168])
2017-05-09 16:27:58,704 : INFO : torch.Size([168, 300])
2017-05-09 16:27:58,704 : INFO : torch.Size([168])
2017-05-09 16:27:58,704 : INFO : torch.Size([168, 300])
2017-05-09 16:27:58,704 : INFO : torch.Size([168])
2017-05-09 16:27:58,704 : INFO : torch.Size([168, 168])
2017-05-09 16:27:58,704 : INFO : torch.Size([168])
2017-05-09 16:27:58,705 : INFO : torch.Size([168, 300])
2017-05-09 16:27:58,705 : INFO : torch.Size([168])
2017-05-09 16:27:58,705 : INFO : torch.Size([168, 168])
2017-05-09 16:27:58,705 : INFO : torch.Size([168])
2017-05-09 16:27:58,705 : INFO : torch.Size([3, 168])
2017-05-09 16:27:58,705 : INFO : torch.Size([3])
2017-05-09 16:27:58,705 : INFO : sum
2017-05-09 16:27:58,706 : INFO : 316347
2017-05-09 16:27:58,706 : INFO : ____________
2017-05-09 16:39:31,759 : INFO : ==> Train loss   : 0.411713
2017-05-09 16:39:31,759 : INFO : Epoch
2017-05-09 16:39:31,761 : INFO : 0
2017-05-09 16:39:31,761 : INFO : train percentage
2017-05-09 16:39:31,761 : INFO : 0.823988439306
2017-05-09 16:39:31,761 : INFO : Epoch
2017-05-09 16:39:31,761 : INFO : 0
2017-05-09 16:39:31,761 : INFO : dev percentage
2017-05-09 16:39:31,762 : INFO : 0.811926605505
2017-05-09 16:39:31,762 : INFO : Epoch
2017-05-09 16:39:31,762 : INFO : 0
2017-05-09 16:39:31,762 : INFO : test percentage
2017-05-09 16:39:31,762 : INFO : 0.829214717188
2017-05-09 16:46:22,012 : INFO : ==> Train loss   : 0.246654
2017-05-09 16:46:22,012 : INFO : Epoch
2017-05-09 16:46:22,012 : INFO : 0
2017-05-09 16:46:22,012 : INFO : train percentage
2017-05-09 16:46:22,012 : INFO : 0.917774566474
2017-05-09 16:46:22,012 : INFO : Epoch
2017-05-09 16:46:22,012 : INFO : 0
2017-05-09 16:46:22,013 : INFO : dev percentage
2017-05-09 16:46:22,013 : INFO : 0.857798165138
2017-05-09 16:46:22,013 : INFO : Epoch
2017-05-09 16:46:22,013 : INFO : 0
2017-05-09 16:46:22,013 : INFO : test percentage
2017-05-09 16:46:22,013 : INFO : 0.846238330588
2017-05-09 16:58:43,981 : INFO : ==> Train loss   : 0.364067
2017-05-09 16:58:43,981 : INFO : Epoch
2017-05-09 16:58:43,981 : INFO : 1
2017-05-09 16:58:43,981 : INFO : train percentage
2017-05-09 16:58:43,981 : INFO : 0.84725433526
2017-05-09 16:58:43,981 : INFO : Epoch
2017-05-09 16:58:43,981 : INFO : 1
2017-05-09 16:58:43,982 : INFO : dev percentage
2017-05-09 16:58:43,982 : INFO : 0.824541284404
2017-05-09 16:58:43,982 : INFO : Epoch
2017-05-09 16:58:43,982 : INFO : 1
2017-05-09 16:58:43,982 : INFO : test percentage
2017-05-09 16:58:43,982 : INFO : 0.833607907743
2017-05-09 17:05:45,105 : INFO : ==> Train loss   : 0.132171
2017-05-09 17:05:45,105 : INFO : Epoch
2017-05-09 17:05:45,105 : INFO : 1
2017-05-09 17:05:45,105 : INFO : train percentage
2017-05-09 17:05:45,105 : INFO : 0.958815028902
2017-05-09 17:05:45,105 : INFO : Epoch
2017-05-09 17:05:45,106 : INFO : 1
2017-05-09 17:05:45,106 : INFO : dev percentage
2017-05-09 17:05:45,106 : INFO : 0.837155963303
2017-05-09 17:05:45,106 : INFO : Epoch
2017-05-09 17:05:45,106 : INFO : 1
2017-05-09 17:05:45,106 : INFO : test percentage
2017-05-09 17:05:45,106 : INFO : 0.838001098298
